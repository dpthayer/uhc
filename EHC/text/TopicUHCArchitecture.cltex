%%[abstract
In this paper we describe the architecture of the Utrecht Haskell Compiler (UHC).
UHC is a new Haskell compiler, that supports most (but not all) Haskell98 features, plus some experimental extensions.
It targets multiple backends, including a bytecode interpreter backend and a full program analysis backend, both via C. 
The implementation is rigourously organized as stepwise transformations through some explicit intermediate languages.
The treewalks of all transformations are expressed as an algebra, with the aid of an Attribute Grammar based preprocessor.
The compiler is just one materialization of a framework that supports experimentation with language variants, thanks to an aspect-oriented internal organisation. 
%%]


%%[introduction

\newcommand{\todo}[1]{{\em Todo: [#1]}}
%\newcommand{\todo}[1]{}


This is yet another Haskell compiler, like the
\cref{www04ghc,marlow98new-ghc-run,peytonjones96hs-transf,peytonjones02hs-inline}{Glasgow Haskell Compiler (GHC)}.
\todo{add real introduction}


The main structure of the compiler is shown in figure~\ref{fig-uhcarch-pipeline}.
Haskell source text is translated to an executable program by stepwise transformation.
Some of the transformations translate the program to a lower level language,
many others are transformations within one language,
establishing some invariant or performing some optimization.

Here is a short characterization of the intermediate languages.
In section~\ref{sec-uhcarch-lang} we give a more detailed description.
\begin{itemize}
\item Haskell (HS): a general-purpose, higher-order, polymorphically typed, lazy functional language.
\item Essential Haskell (EH): a higher-order, polymorphically typed, lazy functional language close to lambda-calculus, without syntactic sugar.
\item Core: an untyped, lazy functional language close to lambda-calculus
            (similar to, but not the same as the Core language used in GHC).
\item Grin: `Graph reduction intermediate notation', 
            the instruction set of a virtual machine of a small functional language with strict semantics, 
            with features that enable implementation of lazyness \cite{boquist99phd-optim-lazy}.
\item Silly: `Simple imperative little language', an abstraction of features found in every imperative language
            (if-statements, assignments, explicit memory allocation) augmented with primitives for manipulating a stack,
            easily translatable to e.g.\ C (not all features of C are provided, only those that are needed for our purpose).
\item BC: A bytecode language for a low-level, stack-based graph reduction machine.
\end{itemize}
The compiler targets different backends, based on a choice of the user.
In all cases, the compiler starts compiling on a per module basis,
desugaring the Haskell source text to Essential Haskell, typechecking it and translating it to Core.
Then there is a choice from three modes of operation:
\begin{itemize}
\item In {\em full program analysis mode},
      the Core modules of the program and required libraries are concatenated
      and processed further as a whole.
      At the Grin level, elaborate inter-module optimization takes place.
      Ultimately, all functions are translated to low level C,
      which can be compiled by a standard compiler.
      As alternative backends, we are experimenting with other target languages,
      among which are the Common Intermediate Language (CIL) from the Common language infrastructure used by .NET \cite{iso-cil},
      and the Low-level virtual machine (LLVM) compiler infrastructure \cite{llvm-cgo04}.
\item In {\em bytecode interpreter mode},
      the Core modules are translated to Grin separately.
      Each Grin module is translated into instructions for a custom bytecode machine.
      The bytecode is emitted in the form of C arrays,
      which are interpreted by a handwritten bytecode interpreter in C.
\item In {\em Java mode},
      the Core modules are translated to bytecode for the Java virtual machine (JVM).
      Each function is translated to a separate class with an |eval| function, and
      each closure is represented by an object combining a function with its parameters.
      Together with a driver function in Java which steers the interpretation,
      these can be stored in a Java archive (jar) and be interpreted by a standard Java interpreter.
\end{itemize}


% dijkstra07ehcstruct


\begin{figure}[tbfh]
\FigScaledPDF{0.43}{uhc-pipeline}
\caption{Intermediate languages and transformations in the UHC pipeline, in each of the three operation modes:
full program alalysis (left), bytecode interpreter (middle), and Java (right).}
\label{fig-uhcarch-pipeline}
\end{figure}



%%]






%%[body


\section{Techniques and Tools}

\subsection{Tree-oriented programming}


Using higher order functions on lists, like |map|, |filter| and |foldr|,
is a good way to abstract from common patterns in
functional programs.

The idea that underlies the definition of |foldr|, i.e.\ to capture the pattern
of an inductive definition by having a function parameter for each constructor of
the data structure, can also be used for other data types, and even for
multiple mutually recursive data types.
A function that can be expressed in this way was called a {\em catamorphism}
by Bird, and the collective extra parameters to |foldr|-like functions 
an {\em algebra} \cite{bird84circ-traverse,birdmoor96algebra}. 
Thus, |((+),0)| is an algebra for lists, and |((++),[])| is another.
In fact, every algebra defines a {\em semantics} of the data structure.
When applying |foldr|-like functions to the algebra consisting of the original constructor functions,
such as |((:),[])| for lists, we have the identity function.
Such an algebra is said to define the ``initial'' semantics.
Outside circles of functional programmers and category theorists, an
algebra is simply known as a ``tree walk''.

In compiler construction, algebras are very useful in defining
a semantics of a syntactic structure or, bluntly said, to define tree walks over the parse tree.
The fact that this is not widely done, is due to the following problems:

\begin{enumerate}
\item Unlike lists, for which |foldr| is standard, in a compiler we deal with
      custom data structures for abstract syntax of a language, 
      which each need a custom |fold|
      function. Morover, whenever we change the abstract syntax,
      we need to change the |fold| function and every algebra.
\item Generated code can be described as a semantics of the language, but often
      we need additional semantices: listings, messages,
      and internal structures (symbol tables etc.).
      This can be done by having the semantic functions in algebras return
      tuples, but this makes the program hard to maintain.
\item Data structures for abstract syntax tend to have many alternatives,
      so algebras end up to be clumsy tuples containing dozens of functions.
\item In practice, information not only flows bottom-up in the parse tree,
      but also top-down. E.g., symbol tables with global definitions need
      to be distributed to the leafs of the parse tree to be able to evaluate them.
      This can be done by using higher-order domains
      for the algebras, but the resulting code becomes even harder to understand.
\item A major portion of the algebra is involved with moving information around.
      The essense of a semantics usually forms only a small part of the algebra
      and is obscured by lots of boilerplate.
\end{enumerate}
Some seek the solution to these problems in the use of monads: the reader monad
to pass information down into the tree, the writer monad to move information
upwards, and the state monad and its derivatives to accumulate information during
the tree walk.\cite{jones99thih}.

Despite the attractiveness of staying inside Haskell we think this approach is
doomed to fail when the algrabras to be decribed are getting more and more
complicated.

%Many compiler writers thus end up writing ad hoc recursive functions
%instead of defining the semantics by a algebra,
%or even resort to non-functional techniques.
%Others succeed in giving a concise definition of a semantics,
%often using proof rules of some kind, but thereby loose the executability.
%For the implementation they still need conventional techniques,
%and the issue arises whether the program soundly implements
%the specified semantics.

To save the nice idea of using an algebra for defining a semantics,
we use a preprocessor for Haskell \cite{swierstra99comb-lang-Short} that overcomes the abovementioned problems.
It is not a separate language; we can still use Haskell for writing
auxiliary functions, and use all abstraction techniques and libraries available.
The preprocessor just allows a few additional constructs, which can be translated
into a custom |fold| function and algebras, or an equivalent more efficient implementation.


We describe the main features of the preprocessor here, and explain why they overcome
the five problems mentioned above.
The abstract syntax of the language is defined in a |DATA| declaration,
which is like a Haskell |Data| declaration with named fields,
however without the braces and commas.
Constructor function names need not to be unique between types.
As an example, consider a fragment of a typical imperative language:
\begin{code}
DATA Stat
  =  Assign  dest   :: String  ^^  ^^  ^^  ^^  src   :: Expr
  |  While   cond   :: Expr    ^^  ^^  ^^  ^^  body  :: Stat
  |  Group   elems  :: [Stat]
DATA Expr
  =  Const   num   :: Int
  |  Var     name  :: String
  |  Add     left  :: Expr     ^^  ^^  ^^  ^^  right  :: Expr
  |  Call    name  :: String   ^^  ^^  ^^  ^^  args   :: [Expr]
\end{code}

The preprocessor generates corresponding |Data| declarations
(making the constructors unique by prepending the type name, like |Expr_Const|),
and generates a custom |fold| function. This overcomes problem 1.

For any desired value we wish to compute over a tree, we can declare a ``synthesized attribute''.
Possibly more than one data type can have the same attribute.
For example, we can declare that both statements and expressions need to 
synthesize bytecode as well as listings, and that expressions
can be evaluated to integer values:
\begin{code}
ATTR Expr Stat  SYN bytecode  :: [Instr]  ^^ ^^ ^^ SYN listing   :: String
ATTR Expr       SYN value     :: Int
\end{code}
The preprocessor generates semantic functions that return
tuples of synthesized attributes, but we can simply refer to attributes by name.
This overcomes problem 2.
Moreover, if at a later stage we add extra attributes, we do not have to refactor a lot of code.

The value of each attribute needs to be defined for 
every constructor of every data type which has the attribute.
Such definitions
are known as ``semantic rules'', and start with keyword |SEM|.
\begin{code}
SEM Expr  | Const  lhs.value = @num
          | Add    lhs.value = @left.value + @right.value
\end{code}
This states that the synthesized (left hand side) |value| attribute
of a |Const|ant expression is just the contents of the |num| field,
and that of an |Add|-expression can be computed
by adding the |value| attributes of its subtrees.
The |@|-symbol in this context should be read as ``attribute'',
not to be confused with Haskell ``as-patterns''.
At the left of the |=|-symbol, the attribute to be defined is mentioned;
at the right, the defining Haskell expression is given.
The preprocessor collects and orders all definitions in a single algebra,
replacing attribute references by suitable selections from the results 
of the tree walk on the children. 
This overcomes problem 3.

To be able to pass information downward during a tree walk,
we can define ``inherited'' attributes
(the terminology goes back to Knuth \cite{knuth68ag}).
As an example, it can serve to pass down an environment,
i.e.\ a lookup table that associates variables to values,
which is needed to evaluate expressions:
\begin{code}
TYPE Env = [(String,Int)]
ATTR Expr INH env::Env
SEM Expr | Var  lhs.value =  fromJust $ 
                             lookup @lhs.env @name
\end{code}
The preprocessor translates inherited attributes into
extra parameters for the semantic functions in the algebra.
This overcomes problem 4.

In many situations, |SEM| rules only specify that attributes
a tree node inherites 
should be passed unchanged to its children, as in a |Reader| monad.
To scrap the boilerplate expressing this, 
the preprocessor has
a convention that, 
unless stated otherwise, attributes with the same name
are automatically copied.
A similar automated copying is done for synthesized attributes
passed up the tree, as in a |Writer| monad.
When more than one child offers a synthesized attribute with the required name,
we can specify to |USE| an operator to combine several candidates:
\begin{code}
ATTR Expr Stat SYN listing USE (++) []
\end{code}
which specifies that by default, the synthesized
attribute |listing| is the concatenation of the |listing|s of
all children that produce a sub-listing, or the empty list if no child produces one.
This overcomes problem 5.



\subsection{Rule-oriented programming}

With the AG language we can describe the part of a compiler related to tree walks concisely and efficiently.
However, this does not give us any means of looking at such an implementation in a more formal setting.
We use the domain specific language |Ruler| for describing the AG part related to the type system.
From a Ruler description we both generate the corresponding AG implementation and a LaTeX rendering for use
in text about the formal aspects.

Although the use of Ruler currently is in flux because we are working on a newer version and therefore
are only partially using Ruler for type system descriptions,
we demonstrate some of its capabilities because it is our intent to tackle the difficulties involved with type system implementations
by generating as much as possible automatically from higher level descriptions.

The idea of Ruler is to generate from a single source both generate a \emph{LaTeX} rendering for human use
in technical writing:

\[
%%@Poster.langSeriesSem2Rule
\]

and its corresponding AG implementation:

%%@Poster.langSeriesImpl2

In \thispaper\ we neither further discuss the meaning or intention of 
\cref{dijkstra05phd}{the above fragments}
nor explain \cref{dijkstra06ruler}{Ruler} in depth.
However, to sketch the underlying ideas we show the Ruler code required for the above output;
we need to define the scheme (or type) of a judgement and populate these with actual rules.

%{
%include ruler.fmt

A scheme defines a LaTeX output template (\texttt{judgeuse tex}) with \texttt{holes} to be filled in by rules and a parsing template (\texttt{judgespec}).

%%[[wrap=code
scheme expr =
    holes  [ node e: Expr, inh valGam: ValGam, inh knTy: Ty
           , thread tyVarMp: VarMp, syn ty: Ty ]
    judgeuse tex  valGam ; tyVarMp.inh ; knTy
                  :-.."e" e : ty ~> tyVarMp.syn
    judgespec  valGam ; tyVarMp.inh ; knTy
               :- e : ty ~> tyVarMp.syn
%%]]

The rule for application is then specified by specifying premise judgements (\texttt{judge} above the dash) and a conclusion (blow the dash) using
the parsing template defined for scheme \texttt{expr}.
%%[[wrap=code
rule e.app =
      judge tvarvFresh
      judge expr =  tyVarMp.inh ; tyVarMp ; (tvarv -> knTy)
                    :- eFun : (ty.a -> ty) ~> tyVarMp.fun
      judge expr =  tyVarMp.fun ; valGam ; ty.a
                    :- eArg : ty.a ~> tyVarMp.arg
      -
      judge expr =  tyVarMp.inh ; valGam ; knTy
                    :- (eFun ^^ eArg)
                    : (tyVarMp.arg ty) ~> tyVarMp.arg
%%]]

For this example no further annotations are required to automatically produce AG code, except for the freshness of a type variable.
The judgement \texttt{tvarvFresh} encapsulates this by providing the means to insert some handwritten AG code.

In summary, the basic idea of Ruler thus is to provide a description resembling the original type rule as much as possible,
and then helping the system with annotations to allow the generation of an implementation and a LaTeX rendering.
Obviously the above examples does not contain all the annotations required to achieve this.

%}


\subsection{Aspect-oriented programming}

UHC's source code is organized into small fragments, each belonging to a particular \emph{variant} and \emph{aspect}.
A variant represents a step in a sequence of languages, where each step adds some language features, starting with simply typed lambda calculus and ending with UHC.
Each step builds on top of the previous one.
Independent of a variant each step adds features in terms of aspects.
For example, the type system and code generation are defined as different aspects.
UHC's build system allows for selectively building a compiler for a variant and a set of aspects.

Source code fragments assigned to a variant and asepcts are stored in \emph{chunked} text files.
A tool called \emph{Shuffle} then generates the actual source code when parameterized with the desired variant and aspects.
Shuffle is language neutral, so all varieties of implementation languages can be stored in chunked format.
For example, the following chunk defines a Haskell wrapper for variant @2@ for the construction of a type variable:

%%[[wrap=tt
%%%[(2 hmtyinfer | hmtyast).mkTyVar
%%@EHTy.2.mkTyVar
%%%]
%%]]

The notation \verb@%%[(2 hmtyinfer | hmtyast).mkTyVar@ begins a chunk for variant @2@ with name |mkTyVar| for aspect @hmtyinfer@ (Hindley-Milner type inference)
or @hmtyast@ (Hindley-Milner type abstract syntax),
ended by \verb@%%]@.
Processing by Shuffle then gives:

%%@EHTy.2.mkTyVar wrap=code

The subsequent variant @3@ requires a more elaborate encoding of a type variable (we do not discuss this further).
The wrapper must be redefined,
which we achieve by explicitly overriding \verb@2.mkTyVar@ by a chunk for \verb@3.mkTyVar@:

%%[[wrap=tt
%%%[(3 hmtyinfer | hmtyast).mkTyVar -2.mkTyVar
%%@EHTy.3.mkTyVar
%%%]
%%]]

Although the type signature can be factored out, we refrain from doing so for small definitions.

Chunked sources are organised on a per file basis.
Each chunked file for source code for UHC is processed by Shuffle to yield a corresponding file for further processing,
depending on the language used.
For chunked Haskell a single module is generated, for chunked AG the file may be combined with other AG files by the AG compiler.

The AG compiler itself also supports a notion of aspects,
different from Shuffle's idea of aspects in that it allows definitions for attributes and abstract syntax
to be defined independent of file and position in a file.
Attribute definitions and attribute equations thus can be grouped according to the programmers sense of what should be together;
the AG compiler combines all these definitions and generates corresponding Haskell code.

Finally, chunked files may be combined by Shuffle by means of explicit reference to the name of a chunk.
This also gives a form of literate programming \cref{knuth92litprog}{literate programming tools}
where text is generated by explicitly combining smaller text chunks.
For example,
the above code for \verb@2.mkTyVar@ and \verb@3.mkTyVar@ are extracted from the chunked source code of UHC and
combined with the text for this explanation by Shuffle.

\section{Languages}\label{sec-uhcarch-lang}

The compiler translates a Haskell program to executable code
by applying many small transformations.
In the process, the program is represented using five different
data structures, or languages.
Some transformations map one of these languages to the next,
some are transformations within one language.
Together, the five languages span the spectrum between
a full feature, lazy functional language, 
and a limited, low level imperative language.



\subsection{The Haskell Language}

The Haskell language (HS) closely follows Haskell's concrete syntax.
It consists of numerous datatypes, some of which have many constructors.
A |Module| consists of a name, exports, and declarations.
Declarations can be varied: function bindings, pattern bindings, type signatures, 
data types, new types, type synonyms, class, instance\dots
Function bindings involve a right hand side which is either an expression or a list of guarded expressions.
An expression, in turn, has no less than 29 alternatives.
All in all, the description of the context-free grammar consists of about 1000 lines of code.

We maintain sufficient information in the abstract syntax tree 
to reconstruct the original input; this includes parentheses, source code position, etc. (but no comments and whitespace).

When processing HS we deal with the following tasks:

\begin{itemize}
\item \textbf{Name resolution:}
 Checking for properly introduced names and renaming all identifiers to the equivalent fully qualified name.
\item \textbf{Operator infixity and priority:}
 Expressions are parsed without taking into account the fixity and priority of operators.
 Expressions are rewritten to remedy this.
\item \textbf{Name dependency:}
 Definitions are reordered into different let bindings such that all identifier uses come after their definition.
 Mutually recursive definitions are put into one let binding.
\item \textbf{Definition gathering:}
 Multiple definitions for the same identifier are merged into one.
\item \textbf{Desugaring:}
 List comprehension, |do|-notation, etc. are desugared.
\end{itemize}

In the remainder of this languages section we use the following running example program to show how
the various intermediate languages are used:

%%[[wrap=code
module M where

len :: [a] -> Int
len [] = 0
len (x:xs) = 1 + len xs

main = putStr (show (len (replicate 4 'x')))
%%]]

\subsection{The Essential Haskell Language}

HS processing generates Essential Haskell (EH).
The EH equivalent of the running looks like the following program.
Some details have been omitted and replaced by dots.

%%[[wrap=code
redOn^ let blackOn  M.len :: [a] -> Int
                    M.len
                      =  redOn ^ \x1 ->  redOn ^case x1 of blackOn
                             UHC.Prelude.[]
                               -> UHC.Prelude.redOn ^fromInteger blackOn 0
                             (UHC.Prelude.redOn^ : x xs blackOn)
                               -> ...
redOn ^ in blackOn
let  M.main = redOn UHC.Prelude.blackOn ^putStr ...
 in
let  redOn ^ main :: blackOn UHC.Prelude.redOn ^IO ... blackOn
     main = UHC.Prelude.redOn ^ehcRunMain blackOn M.main
 in
main
%%]]

In constrast to the HS language, the Essential Haskell language (EH)
brings back the language to its essence, removing as much syntactic sugar as is possible.
An EH module consists of a single expression only, which is the
body of the |main| function, with local let-bindings for the other top-level values.

Processing EH deals with the following tasks:

\begin{itemize}
\item \textbf{Type system:}
 Type analysis is done, types are erased when Core is generated.
\item \textbf{Evaluation:}
 Enforcing evaluation is made explicit by means of a |let !| Core construct.
\item \textbf{Recursion:}
 Recursiveness is made explicit by means of a |let rec| Core construct.
\item \textbf{Type classes:}
 All evidence for type class predicates are transformed to explicit dictionary parameters.
\item \textbf{Patterns:}
 Patterns are transformed to their more basic equivalent, inspecting one constructor at a time, etc. .
\end{itemize}


\subsection{The Core Language}

EH processing basically generates |lambda|-calculus, or Core.
The Core equivalent of the running looks like the following program.

%{
%format under = "\_"
%format excl = "!"
%%[[wrap=code
module ^^ M =
let redOn^ rec blackOn
  { ^ M ^ .len =
      \ ^ M ^ .x1 ^ under ^ 1 ->
        redOn ^ let ^ redOn excl ^ blackOn
          { ^ under ^ 2 = ^ M ^ .x1 ^ under ^ 1} in
        case ^^ under ^ 2 of
         {  redOn C ^ : ^ blackOn { ..., ... } -> ...
         ;  redOn C ^ [ ^ ] ^ blackOn {  } ->
             let
               { ^ under ^ 3 =
                   redOn ( ^ UHC ^ .Prelude ^ .packedStringToInteger) blackOn
                     redOn (#String"0")} blackOn in
             let
               { ^ under ^ 4 =
                   ( ^ UHC ^ .Prelude ^ .fromInteger)
                     (redOn ^ UHC ^ .Prelude ^ .under ^ d1 ^ under ^ Num : DICT blackOn)
                     ( ^ under ^ 3)} in
             ^ under ^ 4
         }
in ...
%%]]
%}


A Core module, apart from its name,
consists of nothing more than an expression,
which can be thought of as the body of |main|:
%%[[wrap=code
DATA  CModule
   =  Mod  nm:Name   expr:CExpr
%%]]
An expression resembles an expression in lambda calculus.
We have constants, variables, and lambda abstractions and applications of one argument:
%%[[wrap=code
DATA  CExpr
   =  Int     int:Int
   |  Char    char:Char
   |  String  str:String
   |  Var     name:Name
   |  Tup     tag:Tag
   |  Lam     arg:Name    body:CExpr
   |  App     func:CExpr  arg:Cexpr
%%]]
Alternative |Tup| encodes a constructor, to be used with |App| to construct actual data alternatives or tuples.
The |Tag| of a |Tup| encodes the |Int| tag, arity, and other information.

Furthermore, there is case distinction and local binding:
%%[[wrap=code
   |  Case    expr:CExpr   alts:[CAlt]    dflt:CExpr
   |  Let     categ:Categ  binds:[CBind]  body:CExpr
%%]]
The |categ| of a |Let| describes whether the binding is recursive, strict, or plain.
These two constructs use the auxiliary notions of alternative and binding:
%%[[wrap=code
DATA  CAlt
   =  Alt     pat:CPat   expr:CExpr
DATA  CBind   
   =  Bind    name:Name  expr:CExpr
   |  FFI     name:Name  imp:String   ty:Ty
%%]]

A pattern introduces bindings, either directly or as a field of a constructor:

%%[[wrap=code
DATA CPat
  =  Var         name:Name
  |  Con         name:Name   tag:Tag   binds:[CPatBind]
  |  BoolExpr    name:Name   cexpr:CExpr
DATA CPatBind
  =  Bind        offset:Int  pat:CPat
%%]]

The actual Core is more complex because of the following:
\begin{itemize}
\item
Experiments with extensible records;
we omit this part as extensible records are currently not supported in UHC.
\item
Type classes involves implies codegeneration because only after context reduction dictionary values and coercions are known.
Core has syntax for expressing holes to be filled in later;
this is a mechanism similar to type variables representing yet unknown types.
\item
An annotation mechanism is used to propagate information about dictionary values.
This mechanism is somewhat ad-hoc and ee expect it to be changed when more analyses are done in earlier stages of the compiler.
\end{itemize}





\subsection{The Grin Language}

%{
%format doll = "\$"
%format under = "\_"
%format excl = "!"
%format slash = "/"
%%[[wrap=code
module ^^ M
{ rec
    {  M.len ^^ M.x1 ^ under ^ 1 =
       {  redOn ^ eval ^^ M.x1 ^ under ^ 1 ; \ ^ under ^ 2 -> ^ blackOn
          case ^^ under ^ 2 of
             { C ^ : ^ ->
                 {  ... } 
             ; C ^ [ ^ ] ^ ->
                 {  redOn ^ store  blackOn (redOn ^ C ^ blackOn ^ UHC.Prelude.PackedString "0") ; \ ^ under ^ 6 ->
                    store          (redOn ^ F ^ blackOn ^ slash ^ UHC.Prelude.packedStringToInteger ^^ under ^ 6) ; \ ^ under ^ 3 ->
                    store          (redOn ^ P ^ slash ^ 0 ^ blackOn ^ slash ^ UHC.Prelude.fromInteger
                                      UHC.Prelude.under ^ d1 ^ under ^ Num) ; \ ^ under ^ 5 ->
                    store          (redOn ^ A ^ blackOn ^^ under ^ 5 ^^ under ^ 3) ; \ ^ under ^ 4 ->
                    redOn ^ eval ^^ under ^ 4 blackOn}
             } } } 
%%]]
%}

A Grin module consists of its name,
global variabels with their initializations, and
bindings of function names with parameters to their bodies.
%%[[wrap=code
DATA  GrModule
   =  Mod   nm:Name  globals:[GrGlobal]  binds:[GrBind]
DATA  GrGlobal
   =  Glob  nm:Name  val:GrVal
DATA  GrBind
   =  Bind  nm:Name  args:[Name]  body:GrExpr
%%]]
Values manipulated in the Grin language are varied:
we have nodes (think: heap records) consisting of a tag and a list of fields,
standalone tags, basic ints and strings, pointers to nodes, and `empty'.
Some of these are directly representable in the languages (nodes, tags, ints and strings)
%%[[wrap=code
DATA  GrVal
   =  LitInt  int:Int
   |  LitStr  str:String
   |  Tag     tag:GrTag
   |  Node    tag:GrTag   flds:[GrVal]
%%]]
Pointers to nodes are also values, but they have no direct denotation.
On the other hand, variables ranging over values are not a value themselves,
bur for syntactical convenience we do add
the notion of a `variable' to the |GrVal| data type:
%%[[wrap=code
   |  Var     name:Name
%%]]
The tag of a node describes its role.
It can be a constructor of a datatype (|Con|),
a function of which the call is deferred because of lazy evaluation (|Fun|),
a function that is partially parameterized but still needs more arguments (|PApp|), or
a deferred application |apply| of an unknown function (appearing as the first argument of the node) to a list arguments (|App|).
%%[[wrap=code
DATA  GrTag
   =  Con   name:Name
   |  Fun   name:Name
   |  PApp  needs:Int  name:Name
   |  App   applyfn:Name
%%]]
The body of a function denotes the calculation of a value,
which is represented in a program by an `expression'.
Expressions can be combined in a monadic style.
Thus we have |Unit| for describing a computation immediately returning a value,
and |Seq| for binding a computation to a variable (or rather a lambda pattern), to be used subsequently in another computation:
%%[[wrap=code
DATA  GrExpr
   =  Unit   val:GrVal
   |  Seq    expr:GrExpr  pat:GrPatLam  body:GrExpr
%%]]
There are some primitive computations (that is, constants in the monad)
one for storing a node value (returning a pointer value), and two
for fetching a node previously stored, and for fetching one field thereof:
%%[[wrap=code
   |  Store       val:GrVal
   |  FetchNode   name:Name
   |  FetchField  name:Name  offset:Int
%%]]
Other primitive computations call Grin and foreign functions, respectively.
The name mentioned is that of a known function (i.e., there are no function variables) and the argument list should fully saturate it:
%%[[wrap=code
   |  Call        name:Name    args:[GrVal]
   |  FFI         name:String  args:[GrVal]
%%]]
Two special primitive computations are provided for evaluating node that may contain a |Fun| tag,
and for applying a node that must contain a |PApp| tag (a partially parameterized function) to further arguments:
%%[[wrap=code
   |  Eval        name:Name
   |  App         name:Name    args:[GrVal]
%%]]
Next, there is a computation for selecting a matching alternative, given the name of the variabele containing a node pointer:
%%[[wrap=code
   |  Case        val:GrVal    alts:[GrAlt]
%%]]
Finally, we need a primitive computation to express the need of `updating' a variable after it is evaluated.
Boquist proposed an |Update| expression for the purpose which has a side effect only and an `empty' result value \cite{boquist99phd-optim-lazy}.
We observed that the need for updates is always next to either a |FetchNode| or a |Unit|, and found it more practical
and more efficient to introduce two update primitives:
%%[[wrap=code
   |  FetchUpdate  src:Name  dst:Name
   |  UpdateUnit   name:Name  val:GrVal
%%]]
Auxiliary data structures are that for describing a single alternative in a |Case| expression:
%%[[wrap=code
DATA  GrAlt
   |  Alt   pat:GrPatAlt   expr:GrExpr
%%]
And for two kinds of patterns, occuring in a |Seq| expression and in an |Alt| alternative, respectively.
A simplified version of these is the following, but we will need some more constructores for patterns later.
%%[[wrap=code
DATA  GrPatLam
   =  Var   name:Name
DATA  GrPatAlt
   =  Node  tag:GrTag   args:[Name]
%%]




\subsection{The Silly Language}

\todo{Description of Silly}



\section{Transformations}

\subsection{Core Transformations}

Three major gaps have to be bridged in the transformation from 
Core to Grin.
Firstly, where Core has a lazy semantics, in Grin deferring of
function calls and their later evaluation is explicitly encoded.
Secondly, in Core we can have local function definitions,
whereas in Grin all function definitions are at top level.
Grin does have a mechanism for local, explicitly sequenced variable bindings.
Thirdly, wheras Core functions always have one argument,
in Grin functions can have multiple parameters, but they
take them all at the same time. 
Therefore a mechanism for partial parametrization is necessary.
The endresult is lambda lifted Core, that is the floating of lambda-expressions to the top level
and passing of non-global variables explicitly as parameters.

The Core transformations listed below also perform some trivial cleanup and optimisations,
because we avoid burdening the Core generation from EH with such aspects.

\begin{enumerate}
\item {\em EtaReduction}
    Perform |eta|-reduction, that is replace expressions like |\x y -> f x y| with |f|.
    Such expressions are introduced by coercions which (after context reduction) turn out not to coerce anything at all.
\item {\em RenameUnique}
    Renames variables such that all variables are globally unique.
\item {\em LetUnrec}
    Replace mutually recursive bindings
%%[[wrap=code
let rec {v1 = .. ; v2 = ..} in ..
%%]]
    which actually are not mutually recursive by plain bindings
%%[[wrap=code
let v1 = .. in let v2 = .. in ..
%%]]
    Such bindings are introduced because some bindings are potentially mutually recursive, in particular groups of dictionaries.
\item {\em InlineLetAlias}
    Inlines let bindings for variables and constants.
\item {\em ElimTrivApp}
    Eliminates application of the |id| function.
\item {\em ConstProp}
    Performs addition of int constants at compile time.
\item {\em FullLazy}
    Complex expressions like
%%[[wrap=code
f (g a) (h b)
%%]]
    are broken up into a sequence of bindings and simpler expressions
%%[[wrap=code
let v1 = g a in let v2 = h b in f v1 v2
%%]]
    which only have variable references as their subexpressions.
\item {\em LamGlobalAsArg}
    Pass global variables of let-bound lambda-expressions as explicit parameters,
    as a preparation for lambda-lifting.
\item {\em CAFGlobalAsArg}
    Similar for let-bound constant applicative forms (CAFs).
\item {\em FloatToGlobal}
    Performs 'lambda lifting': move bindings of lambda-expressions and CAFs to the global level.
\item {\em LiftDictFields}
    Makes sure that all dictionary fields exist as a top-level binding.
\item {\em FindNullaries}
    Finds nullary (parameterless) functions and duplicates them;
    the two copies are differently annotated,
    such that one of the two will end up as an updateable global variable.
\end{enumerate}
After the transformations, translation to Grin is performed,
where the following issues are adressed:
\begin{itemize}
\item for |Let|-expressions:
      global expressions are collected and made into Grin function bindings;
      local non-recursive expressions are sequenced by Grin |Seq|-expressions;
      for local recursive let-bindings a |Seq|uence is created
      which starts out to bind a new variable to a `black hole' node, then processes the body, and finally generates a |FetchUpdate|-expression for the introduced variable.
\item for |Case|-expressions:
      an explicit |Eval|-expression for the scrutinee is generated, in |Seq|uence with
      a Grin |Case|-expression.
\item for |App|-expressions:
      it is determined what it is that is applied:
      \begin{itemize}
      \item if it is a constructor, then a node with |Con| tag is returned;
      \item if it is a lambda of known arity which has exactly the right number of arguments, then
            either a |Call|-expression is generated (in strict contexts)
            or a node with |Fun| tag is stored with a |Store|-expression (in lazy contexts);
      \item if it is a lambda of known arity that is undersaturated (has not enough arguments), then
            a node with |PApp| tag is returned (in strict contexts) or |Store|d (in lazy contexts)
      \item if it is a lambda of known arity that is oversaturated (has too many arguments), then
            (in stict contexts) first a |Call|-expression to the function is generated that applies the function
            to some of the arguments, and the result is bound to a variable that is sub|Seq|uently |App|lied
            to the remaining arguments; or
            (in non-strict contexts) a node with |Fun| tag is |Store|d, and bound to a variable
            that is used in another node which has an |App| tag.
      \item if it is a variable that represents a function of unknown arity, then
            (in strict contexts) the variable is explicitly |Eval|uated, and its result used in an |App|expression to the arguments; or
            (in non-strict contexts) as a last resort, both function variable and arguments are stored in a node with |App| tag.
      \end{itemize}
\item for global bindings:
      lambda abstractions are `peeled off' the body, to become the arguments of a Grin function binding.
\item for foreign function bindings:
      functions with |IO| reslut type are treated specially.
\end{itemize}

We have now reached the point in the compilation pipeline where we perform our full-program analysis.
The Core module of the program under compilation is merged with the Core modules of all used libraries.
The resulting big Core module is then translated to Grin.



\subsection{Grin Transformations}

In the Grin world, we take the opportunity to perform many optimizing transformations.
Other transformations are designed to move from graph manipulation concepts
(complete nodes that can be `fetched', `evaluated' and pattern matched for)
to a lower level where single word values are moved and inspected in
the imperative target language.

We first list all transformations in the order they are performed,
and then discuss some issues that are tackled with the combined effort
of multiple transformations.

\begin{enumerate}
\item {\em DropUnreachableBindings}
    Drops all functions not reachable from |main|,
    either through direct calls, 
    or through nodes that store a deferred or partially parameterized function.
    The transformation performs a provisional numbering of all functions, and creates a graph of dependencies.
    A standard reachablility algorithm determines which functions are reachable from |main|;
    the others are dropped.
\item {\em MergeInstance}
    Introduces an explicit dictionary for each instance declaration,
    by merging the default definitions of functions taken from class declarations.
    This is possible because we have the full program available now (see discussion below).
\item {\em MemberSelect}
    Looks for the selection of a function from a dictionary and its subsequent
    application to parameters. Replaces that by a direct call.
\item {\em DropUnreachableBindings}
    Drops the now obsolete implicit constructions of dictionaries.
\item {\em Cleanup}
    Replaces some node tags by equivalent ones:
    |PApp 0|, a partial application needing 0 more parameters, is changed into |Fun|, a simple deferred function;
    deferred applications of constructor functions are changed to immediate application of the constructor function.
\item {\em SimpleNullary}
    Optimises nullary functions that immediately return a value or call another function,
    by inlining them in nodes that encode their deferred application.
\item {\em ConstInt}
    Replaces deferred applications of |integer2int| to constant integers by a constant int.
    This situation occurs for every numeric literal in the source program,
    because of the way literals are overloaded in Haskell.
\item {\em BuildAppBindings}
    Introduces bindings for |apply| functions with as many parameters as are needed in the program.
\item {\em GlobalConstants}
    Introduces global variables for each constant foud in the program,
    instead of allocation the constants locally.
\item {\em Inline}
    Inlines functions that are used only once at their call site.
\item {\em SingleCase}
    Replaces case expressions that have a single alternative by the body of that alternative.
\item {\em EvalStored}
    Do not do |Eval| on pointers that bind the result of a previous |Store|.
    Instead, do a |Call| if the stored node is a deferred call (with a |Fun| tag), 
    or do a |Unit| of the stored node for other nodes.
\item {\em ApplyUnited}
    Do not perform |Apply| on variables that bind the result of a previous |Unit| of a node with a |PApp| tag.
    Instead, do a |Call| of the function if it is now saturated, or build a new |PApp| node if it is undersaturated.
\item {\em SpecConst}
    Specialize functions that are called with a constant argument.
    The transformation is useful for creating a specialized `increment' function instead of |plus 1|,
    but its main merit lies in making specialized versions of overloaded functions, 
    that is functions that take a dictionary argument.
    If the dictionary is a constant, specialization exposes new opporunities for the {\em MemberSelect} transformation,
    which is why {\em SpecConst} is iterated in conjunction with {\em EvalStored}, {\em ApplyUnited} and {\em MemberSelect}.
\item {\em DropUnreachableBindings}
    Drops unspecialized functions that may have become obsolete.
\item {\em NumberIdents}
    Attaches an unique number to each variable and function name.
\item {\em HeapPointsTo}
    Does a `heap points to analysis' (HPT), which is an abstract interpretation of the program
    in order to determine the possible tags of the nodes that each variable can refer to.
\item {\em InlineEA}
    Replaces all occurences of |Eval| and |Apply| to equivalent constructs.
    Each |Eval x| is replaced by |FetchNode x|, followed by a |Case| distinction
    on all possible tag values of the node referred to by |x|,
    which was revealed by the HPT analysis.
    If the number of cases is prohibitively large, we resort to a |Call| to a generic |evaluate| function,
    that is generated for the purpose and that distinguishes all possible node tags.
    Each |App f x| construct, that is used to apply an unknown function |f| to argument |x|, is replaced
    by a |Case| distinction on all possible |PApp| tag values of the node referred to by |f|.
\item {\em ImpossibleCase}
    Removes alternatives from |Case| constructs that, according to the HPT analysis, can never occur.
\item {\em LateInline}
    Inlines functions that are used only once at their call site.
    New opportunities for this transformation are present because the {\em InlineEA} transformation introduces new |Call| constructs.
\item {\em SingleCase}
    Replaces case expressions that have a single alternative by the body of that alternative.
    New opportunities for this transformation are present because the {\em InlineEA} transformation introduces new |Case| constructs.
\item {\em DropUnusedExpr}
    Removes bindings to variables if the variable is never used,
    but only when the expression has no side effect.
    Therefore, an analysis is done to determine which expressions may have side effects.
    |Update| and |FFI| expressions are assumed to have side effect, 
    and |Case| and |Seq| expressions if one of their children has them.
    The tricky one is |Call|, which has a side effect if its body does.
    This is circular definition of `has a side effect' if the function is recursive.
    Thus we take a 2-pass approach: a `coarse' approximation that assumes that every |Call| has a side effect, 
    and a `fine' approximation that takes into account the coarse approximation for the body.
    Variables that are never used but which are retained because of the possible side effects of their bodies are replaced by wildcards.
\item {\em MergeCase}
    Merges two adjacent |Case| constructs into a single one in some situations.
\item {\em LowerGrin}
    Translates to a lower level version of Grin, in which variables never represent a node.
    Instead, variables are introduced for the separate fields, of which the number became known through HPT analysis.
    Also, after this transformation |Case| constructs scrutinise on tags rather than full nodes.
\item {\em CopyPropagation}
    Shortcuts repeated copying of variables.
\item {\em SplitFetch}
    Translates to an even lower level version of Grin, in which the node referred to by a pointer is not fetched as a whole,
    but field by field. That is, the |FetchNode| expression is replaced by a series of |FetchField| expressions.
    The first of these fetches the tag, the others are specialized in the alternatives of the |Case| expression
    that always follows a |FetchNode| expression, such that no more fields are fetched than required by this particular tag.
\item {\em DropUnusedExpr}
    Removes variable bindings introduced by {\em LowerGrin} if they happen not to be used.    
\item {\em CopyPropagation}
    Again shortcuts repeated copying of variables.
\end{enumerate}    


\paragraph{Simplification}
The Grin language features constructs for manipulating heap nodes,
including ones that encode deferred function calls, that are explicitly
triggered by an |Eval| expression.
As part of the simplification, this high level construct should be decomposed in smaller steps.
Two strategies can be taken to implement evaluation:
\begin{itemize}
\item {\em tagged}:  nodes are tagged by small numbers,
                     evaluation is performed by calling a special |evaluate| function that scrutinizes the tag,
                     and for each possible |Fun| tag calls the corresponding function and updates the thunk;
\item {\em tagless}: nodes are tagged by pointers to code that does the call and update operations,
                     thus evaluation is tantamount to just jumping to the code pointed to by the tag.
\end{itemize}
The tagged approach has overhead in calling |evaluate|,
but the tagless approach has the disadvantage that the indirect jump involved may stall the lookahead buffer of pipelined processors.
Boquist proposed to inline the |evaluate| function at every occurence of |Eval|,
where for every instance the |Case| expression involved only contains those cases which can actually occur.
It is this approach that we take in UHC.

This way, they high level concept of |Eval| is replaced by lower level concepts of |FetchNode|, |Case|, |Call| and |Update|.
In turn, each |FetchNode| expression is replaced by a series of |FetchField| expressions in a later transformation,
and the |Case| that scrutinzes a node is replaced by one that scrutinizes the tag only.

    
\paragraph{Abstract interpretation}
The desire to inline a specialized version of |evaluate| at every |Eval| instance
brings the need for an analysis that for each pointer variable determines the possible tags of the node.
An abstract interpretation of the program, known as `heap points to (HPT) analysis' tries to approximate this knowledge.
As a preparation, the program is scanned to collect constraints on variables.
Some constaints immediately provides the information needed (e.g., the variable that binds the result of a |Store| expression
is obviously a pointer to a node with the tag of the node that was stored),
but other constraints are indirect (e.g., the variable that binds the result of a |Call| expression
will have the same value as the called function returns).
The analysis is essentially a full-program analysis, as actual parameters of functions
impose constraints on the parameters.

The constraint set is solved in a fixpoint iteration, which processes the indirect constraints
based on information gathered thus far. In order to have fast access to the mapping that records
the abstract value for each variable, we uniquely number all variables, and use mutable arrays to store the mapping.

Special attention deserves the processing of the constraint that expresses that |x| binds the 
result of |Eval p|.
If |p| is already known to point to nodes with a |Con| tag (i.e., values) then this is also a possible value for |x|.
If |p| is known to point to nodes with a |Fun f| tag (i.e., deferred functions), then the possible results for |f| are also possible values for |x|.
And if |p| is known to point to nodes with an |App apply| tag (i.e., generic applications of unknown functions by |apply|),
then the possible results for |apply| are also possible values for |x|.


\paragraph{HPT performance}

The HPT analysis must at least find all possible tags for each pointer, but it is sound if it reports a superset of these.
The design of the HPT analysis is a tradeoff between time (the number of iterations it takes to find the fixed point)
and accuracy.
A trivial solution is to report (in 1 step) that every pointer may point to every tag;
a perfect solution would solve the halting problem and thus would take infinite time in some situations.

We found that the number of iterations our implementation takes, is dependent of two factors:
the depth of the call graph (usually bounded by a dozen or so in practice),
and the length of static data structures in the program.
The latter surprised us, but is understandable if one considers the program
%%[[wrap=code
main = putStrLn (show (last [id,id,id,id,succ] 1))
%%]
where it takes 5 iterations to find out that 1 is a possible parameter of |succ|.

As for accuracy, our HPT algorithm works well for first-order functions.
In the presence of many higher-order functions, the results suffer from `pollution':
the use of a higher-order function in one context also influences its result in another context.
We counter this undesired behaviour in a number of ways:
\begin{itemize}
\item instead of using a generic |apply| function, the {\em BuildAppBindings} transformation
      makes a fresh copy for each use by an |App| tag. This prevents mutual pollution of |apply| results,
      and also increases the probability that the |apply| function can be inlined later;
\item we specialize overloaded functions for every dictionary that it is used with,
      to avoid the |Apply| needed on the unknown function taken from the dictionary;
\item we fall back on explicitly calling |evaluate| (instead of inlining it) in situations where the
      number of possible tags is unreasonable large.
\end{itemize}



\paragraph{Instance declarations}

The basic idea of implementing instances is simple:
an instance is a tuple (known as a `dictionary') containing all member functions,
which is passed as an additional parameter to overloaded functions.
Things are complicated, however, by the presence of default implementations in classes:
the dictionary for an instance declaration is a merge of the default implementations
and the implementations in the instance declaration.
Even worse, the class declaration may reside in another module than the instance declaration,
and still be mutally dependent with it.
Think of the |Eq| class, having mutually circular definitions of |eq| and |ne|, leaving
it to the instance declaration to implement either one of them (or both).

A clever scheme was designed by Fax\'en to generate the dictionary from a generator function
that is parameterized by the dictionary containing the default implementations,
while the default dictionary is generated from a generator function
parameterized by the instance dictionary \cite{faxen02semantics-haskell}.
Lazy evaluation and black holes make this all work, and we employ this scheme in UHC too.
It would be a waste, however, now that we are in a full program analysis situation,
not to try to do as much work as possible at compile time.

Firstly, we have to merge the default and instance dictionaries.
In the Grin world, we have to deal with what the Core2Grin transformation
makes of the Fax\'en scheme. That is:
\begin{itemize}
\item A 1-ary generator function |gfd| that, given a default dictionary, will generate the dictionary;
\item A 0-ary function |fd| that binds a variable to a black hole, calls |gfd|, and returns the result
\item A global variable |d| which is bound to a node with tag |Fun fd|.
\end{itemize}
We want to change this in a situation where |d| is bound directly to the dictionary node.
This involves reverse engineering the definitions of |d|, |fd| and |gfd| to find the
actual member function names buried deep in the definition of |gfd|.
Although possible, this is very volatile as it depends on the details of the Core2Grin translation.
Instead, we take a different approach: the definition of |fd| is annotated with the names of the member functions
at the time when it is still explicitly available, that is during the EH2Core translation.
Similary, class definitions are annotated with the names of the default functions.
Now the {\em Grin.MergeInstance} transformation can easily collect the required dictionary fields,
provided that the {\em Core.LiftDictFields} transformation ensures they are available as top-level functions.
The |fd| and |gfd| functions are obsolete afterwards, and can be discarded by a later reachability analysis.

Secondly, we hunt the program for 
dictionaries $d$ (as constructed above) and
selection functions $s_k$ (easily recognizable as a function that pattern-matches its parameter to a dictionary structure and returns its $k$th field $x_k$).
In such situations |Call s_k d| can be replaced by |Eval x_k|.
A deferred member selection, involving a node with tag |Fun s_k| and field |d|, is dealt with similarly:
both are done by the {\em MemberSelect} transformation.

Thirdly, as $x_k$ is a dictionary field, it is a known node |n|.
If |n| has a |Fun f| tag, then |Eval x_k| can be replaced by |Call f|,
and otherwise it can be replaced by |Unit n|.
This is done by the {\em EvalStored} transformation.
The new |Unit| that is exposed by this transformation can be combined with the |App| expression
that idiomatically follows the member selection, which is what {\em ApplyUnited} does.

All of this only works when members are selected from a constant dictionary.
Overloaded functions however operate on dictionaries that are passed as parameter,
and member selection from a variable dictionary is not caught by {\em MemberSelect}.
The constant dictionary appears where the overloaded function is called,
and can be brought to the position where it is needed by
specializing functions when they are called with constant arguments.
This is done in the {\em SpecConst} transformation.
Besides useful in the chain of transformations that together remove the dictionaries,
it is useful for removal of other constants, like a 1-ary successor function as a 
specialization of |plus 1|.
(If constant specialization is also done for string constants, we get
many specializations of |putStrLn|).

The whole pack of transformations is applied repeatedly, as applying them
exposes new opportunities for sub-dictionaries.
Four iterations suffice to deal with the common cases (involving |Eq|, |Ord|, |Integral|, |Read| etc.)
from the prelude.

The only situation where dictionaries cannot be eliminated completely, is where an infinite family
of dictionaries is necessary, such as arises from the |Eq a => Eq [a]| instance declaration
in the prelude. In this situation we automatically fall back to the Fax\'en scheme.


\paragraph{Foreign functions}


    

\subsection{Silly Transformations}


\begin{enumerate}
\item {\em InlineExpr}
Avoids copying variables to other variables, 
if in all uses the original one could be used just as well
(i.e., are not modified in between).

\item {\em ElimUnused}
Eliminates assignments to variables that are never used.

\item {\em EmbedVars}
Silly has a notion of function arguments and local variables.
After this transformation, these kind of variables are not used anymore,
but replaced by explicit strack offsets.
So, this transformation does the mapping of variables to stack positions
(and, if available, registers).
In a tail call, the parameters of the function that is called overwrites
the parameters and local variables of the function that does the call.
The assignments are scheduled in such a way
that no values are overwritten that are still needed in assignments to follow.


\item {\em GroupAllocs}
This transformation combines separate, adjacent calls to |malloc| into one,
enabling to do heap overflow check only once for all the memory that
is allocated in a particular function.

\end{enumerate}


\section{Conclusion}


\subsection{Code size}

UHC is the standard materialization of a more general code base (the UHC framework, formerly known as EHC),
from which increasingly powerful `variants' of the compiler can be drawn,
where independent experimental `aspects' can be switched on or off.
The whole source code base consists of a fairly exact 100.000 lines of code.
Just over half of it is Attribute Grammar code, which of course has lots
of embedded Haskell code in it. 
One third of the code base is plain Haskell (mostly for utility functions, the compiler driver, and the type inferencer),
and one sixth is C (for the runtime system and a garbage collector).

In figure~\ref{fig-uhcarch-codesize} the breakdown of code size over
various subsystems in the pipeline is shown.
All numbers are in kilo-lines-of-code, but because of the total of 100.000 lines
they can also be interpreted as percentages.
Column `UHC only' shows the size of the code that is selected by Shuffle
for the standard compiler, i.e.\ the most powerful variant without experimental aspects.
On average, 60\% of the total code base is used in UHC.
The rest is either code for low variants which is overwritten in higher variants,
code for experimental aspects that are switched off in UHC,
chunk header overhead, or
comments that were placed outside chunks.

The fraction of code used for UHC is relatively low in 
the type inferencer (as there are many experimental aspects here),
in the experimental backends like Java, Cil and LLVM (as most of them are switched off), and
in the garbage collector (as it is not yet used: UHC by default uses a third-party generic garbage collector).


\begin{figure}
\begin{center}
\begin{tabular}{l||rrrr||rr}
subsystem    & \multicolumn{4}{c||}{All variants and aspects}    & \multicolumn{2}{c}{UHC only} \\
             &    AG      &    HS      &     C      &    total   &    total   &  fract.  \\\hline
utility/general&   1.7    &    18.3    &            &    20.0    &    14.0    &    70\%  \\
Haskell      &     6.7    &     3.3    &            &     9.9    &     6.9    &    70\%  \\
EH           &    11.2    &     0.6    &            &    11.8    &     6.7    &    57\%  \\
EH typing    &     8.0    &     7.5    &            &    15.5    &     7.0    &    45\%  \\
Core         &     7.1    &     1.0    &            &     8.0    &     4.7    &    58\%  \\
ByteCode     &     2.1    &            &            &     2.1    &     1.7    &    82\%  \\
Grin         &    11.3    &     1.6    &            &    12.9    &     8.5    &    66\%  \\
Silly        &     2.8    &            &            &     2.8    &     2.6    &    93\%  \\
exp.backends &     2.5    &     0.4    &            &     2.9    &     0.8    &    26\%  \\
runtime system&           &            &     8.1    &     8.1    &     6.2    &    77\%  \\
garb.collector&           &            &     6.0    &     6.0    &     0.7    &    11\%  \\\hline
total        &    53.4    &    32.5    &    14.1    &   100.0    &    59.8    &    60\%  \\
\end{tabular}
\end{center}
\caption{Code size (in 1000 lines of code) of source files containing Attribute Grammar code (AG), Haskell code (HS) and C code (C),
for various subsystems. Column `all variants' is the total code base for all variants and aspects, column `UHC' is the selection of the standard compiler,
where `fract.' shows the fraction of the full code base that is selected for UHC.}
\label{fig-uhcarch-codesize}
\end{figure}



\subsection{Methodological observations}


\paragraph{Internal organisation}

UHC and its internal framework uses an aspectwise organisation,
in which as much as possible is described by higher level domain specific languages from which we generate lower level implementations.
UHC as a framework offers a set of compilers, thus allowing picking and choosing a starting point for play and experimentation.
This makes UHC a good starting point for research but debugging is also facilitated by it.
A problem can more easily pinpointed to originate in a particular step of the whole sequence of language increments;
the framework then allows to debug the compiler in this limited context, with less interaction by other features.

The stepwise organisation, where langauge features are built on top of each other, offers a degree of isolation.
Much better would be to completely independently describe language features.
However, this is hard to accomplish because language features always interact and require redefinition of parts of their independent implementation when combined.
To do this for arbitrary combinations would be much more complicated then to do it for a sequence of increments.

\paragraph{AG Design Patterns}

We tend to use various AG idioms frequently. For example, information often is gathered over a tree via a synthesized attribute,
subsequently to be passed back as an inherited attribute.
This leads to a 2-pass tree traversal.

Some idiomatic use is directly supported by the AG system.
For example, transformations are expressed as attribute grammars with a single, specially annotated, attribute declaration for a copy of the tree begin walked over.
Only where the transformation differs from the original needs to be specified.

The AG notation allows us to avoid writing much boilerplate code, similar
to \cref{visser05stratego-www,visser01stratego-system05,laemmel03boilerplate}{other tree traversal approaches}.
The use of attributes sometimes also resembles reader, writer, and state monads.
In practice, the real strength of the AG system lies in combining the separately defined tree traversals into one.
For example, the EH type analysis repeatedly builds environments for kinds, types, datatypes, etc.
Combined with the above idiomatic use this easily leads to many passes over the EH tree;
something we'd rather not write by hand using monads (and monad transformers) or other mechanisms more suitable for single-pass tree traversals!

However, not all idiomatic use is supported by AG.
For example, the need to pattern match on subtrees arises when case analysis on abstract syntax trees must be done.
Currently this must be programmed by hand, and we would like to have automated support for it (see e.g. \cref{visser05stratego-www,visser01stratego-system05}{Stratego}).

\paragraph{The use of domain specific languages (DSL)}

We use various special purpose languages for subproblems:
AG for tree traversals, Shuffle for incremental, aspectwise, and better explainable development, Ruler for type systems.
Although this means a steeper learning curve for those new to the implementation,
in practice the used DSL's and their supporting tools effectively solve a identifiable design problem.
There lies a balance here, but for us the used DSL's make development life easier.

\paragraph{Annotations}

We tend to extend languages with annotations.
This is either to prevent keeping separate lookup tables
(e.g. for arity of constructors),
or to propagate information that is required in a later stage (e.g. type information of FFI's),
or to track the origin of constructs (e.g. class declaration).


\subsection{Related work}

Clearly other Haskell compilers exist, most notably \cref{www04ghc}{GHC},
which is hard if not impossible to match in its reliability and feature richness:
UHC itself uses GHC as its main development tool.

The strong point of UHC is its internal organisation.
UHC, in particular its internal aspectwise organised framework,
is designed to be (relatively) easy to use as a platform for research and education.
In Utrecht students regularly use the UHC framework to experiment with.
The use of AG and other tools also distinguishes UHC from other Haskell compilers,
most of them written in Haskell and lower level languages.

Recently,
\cref{www09jhc}{JHC} and \cref{www09lhc}{LHC} (derived from JHC) also take the full program analysis approach taken by \cref{boquist96grin-optim,boquist99phd-optim-lazy}{Boquist}
as their starting point.
LHC in its most recent incarnation is available as a backend to GHC, and in that sense is not a standalone Haskell compiler.

Already longer available alongside GHC are \cref{www03hugs}{Hugs} which was influential on Haskell as a language,
\cref{www09nhc98}{NHC98}, and \cref{www09yhc}{YHC} derived from NHC98, all mature Haskell98 compilers
with extensions.
\cref{heeren05www-helium}{Helium} (also from Utrecht) does not implement full Haskell98 but focusses on good error reporting,
thereby being suitable for learning Haskell.

\subsection{Future work}

We have recently released a first version of UHC.
In the near future we intend to add support for better installation, in particular the use of Cabal, and add missing language features and libraries.
On a longer time scale we continue working on full program analysis, the optimisations allowed by it, add classical analyses (e.g. strictness),
improve the runtime system (own garbage collector).
We welcome those who want to contribute in these or other areas of interest.


%%]




