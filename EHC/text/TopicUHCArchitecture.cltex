%%[abstract
In \thispaper\ we describe the architecture of the Utrecht Haskell Compiler (UHC).
%%]


%%[introduction

\newcommand{\todo}[1]{{\em Todo: [#1]}}


This is yet another Haskell compiler, like the
\cref{www04ghc,marlow98new-ghc-run,peytonjones96hs-transf,peytonjones02hs-inline}{Glasgow Haskell Compiler (GHC)}.
\todo{add real introduction}
%%]






%%[body


\section{Techniques and Tools}

\subsection{Tree-oriented programming}


Using higher order functions on lists, like |map|, |filter| and |foldr|,
is a good way to abstract from common patterns in
functional programs.

The idea that underlies the definition of |foldr|, i.e.\ to capture the pattern
of an inductive definition by having a function parameter for each constructor of
the data structure, can also be used for other data types, and even for
multiple mutually recursive data types.
A function that can be expressed in this way was called a {\em catamorphism}
by Bird, and the collective extra parameters to |foldr|-like functions 
an {\em algebra} \cite{bird84circ-traverse,birdmoor96algebra}. 
Thus, |((+),0)| is an algebra for lists, and |((++),[])| is another.
In fact, every algebra defines a {\em semantics} of the data structure.
When applying |foldr|-like functions to the algebra consisting of the original constructor functions,
such as |((:),[])| for lists, we have the identity function.
Such an algebra is said to define the ``initial'' semantics.
Outside circles of functional programmers and category theorists, an
algebra is simply known as a ``tree walk''.

In compiler construction, algebras are very useful in defining
a semantics of a syntactic structure or, bluntly said, to define tree walks over the parse tree.
The fact that this is not widely done, is due to the following problems:

\begin{enumerate}
\item Unlike lists, for which |foldr| is standard, in a compiler we deal with
      custom data structures for abstract syntax of a language, 
      which each need a custom |fold|
      function. Morover, whenever we change the abstract syntax,
      we need to change the |fold| function and every algebra.
\item Generated code can be described as a semantics of the language, but often
      we need additional semantices: listings, messages,
      and internal structures (symbol tables etc.).
      This can be done by having the semantic functions in algebras return
      tuples, but this makes the program hard to maintain.
\item Data structures for abstract syntax tend to have many alternatives,
      so algebras end up to be clumsy tuples containing dozens of functions.
\item In practice, information not only flows bottom-up in the parse tree,
      but also top-down. E.g., symbol tables with global definitions need
      to be distributed to the leafs of the parse tree to be able to evaluate them.
      This can be done by using higher-order domains
      for the algebras, but the resulting code becomes even harder to understand.
\item A major portion of the algebra is involved with moving information around.
      The essense of a semantics usually forms only a small part of the algebra
      and is obscured by lots of boilerplate.
\end{enumerate}
Some seek the solution to these problems in the use of monads: the reader monad
to pass information down into the tree, the writer monad to move information
upwards, and the state monad and its derivatives to accumulate information during
the tree walk.\cite{jones99thih}.

Despite the attractiveness of staying inside Haskell we think this approach is
doomed to fail when the algrabras to be decribed are getting more and more
complicated.

%Many compiler writers thus end up writing ad hoc recursive functions
%instead of defining the semantics by a algebra,
%or even resort to non-functional techniques.
%Others succeed in giving a concise definition of a semantics,
%often using proof rules of some kind, but thereby loose the executability.
%For the implementation they still need conventional techniques,
%and the issue arises whether the program soundly implements
%the specified semantics.

To save the nice idea of using an algebra for defining a semantics,
we use a preprocessor for Haskell \cite{swierstra99comb-lang-Short} that overcomes the abovementioned problems.
It is not a separate language; we can still use Haskell for writing
auxiliary functions, and use all abstraction techniques and libraries available.
The preprocessor just allows a few additional constructs, which can be translated
into a custom |fold| function and algebras, or an equivalent more efficient implementation.


We describe the main features of the preprocessor here, and explain why they overcome
the five problems mentioned above.
The abstract syntax of the language is defined in a |DATA| declaration,
which is like a Haskell |Data| declaration with named fields,
however without the braces and commas.
Constructor function names need not to be unique between types.
As an example, consider a fragment of a typical imperative language:
\begin{code}
DATA Stat
  =  Assign  dest   :: String  ^^  ^^  ^^  ^^  src   :: Expr
  |  While   cond   :: Expr    ^^  ^^  ^^  ^^  body  :: Stat
  |  Group   elems  :: [Stat]
DATA Expr
  =  Const   num   :: Int
  |  Var     name  :: String
  |  Add     left  :: Expr     ^^  ^^  ^^  ^^  right  :: Expr
  |  Call    name  :: String   ^^  ^^  ^^  ^^  args   :: [Expr]
\end{code}

The preprocessor generates corresponding |Data| declarations
(making the constructors unique by prepending the type name, like |Expr_Const|),
and generates a custom |fold| function. This overcomes problem 1.

For any desired value we wish to compute over a tree, we can declare a ``synthesized attribute''.
Possibly more than one data type can have the same attribute.
For example, we can declare that both statements and expressions need to 
synthesize bytecode as well as listings, and that expressions
can be evaluated to integer values:
\begin{code}
ATTR Expr Stat  SYN bytecode  :: [Instr]  ^^ ^^ ^^ SYN listing   :: String
ATTR Expr       SYN value     :: Int
\end{code}
The preprocessor generates semantic functions that return
tuples of synthesized attributes, but we can simply refer to attributes by name.
This overcomes problem 2.
Moreover, if at a later stage we add extra attributes, we do not have to refactor a lot of code.

The value of each attribute needs to be defined for 
every constructor of every data type which has the attribute.
Such definitions
are known as ``semantic rules'', and start with keyword |SEM|.
\begin{code}
SEM Expr  | Const  lhs.value = @num
          | Add    lhs.value = @left.value + @right.value
\end{code}
This states that the synthesized (left hand side) |value| attribute
of a |Const|ant expression is just the contents of the |num| field,
and that of an |Add|-expression can be computed
by adding the |value| attributes of its subtrees.
The |@|-symbol in this context should be read as ``attribute'',
not to be confused with Haskell ``as-patterns''.
At the left of the |=|-symbol, the attribute to be defined is mentioned;
at the right, the defining Haskell expression is given.
The preprocessor collects and orders all definitions in a single algebra,
replacing attribute references by suitable selections from the results 
of the tree walk on the children. 
This overcomes problem 3.

To be able to pass information downward during a tree walk,
we can define ``inherited'' attributes
(the terminology goes back to Knuth \cite{knuth68ag}).
As an example, it can serve to pass down an environment,
i.e.\ a lookup table that associates variables to values,
which is needed to evaluate expressions:
\begin{code}
TYPE Env = [(String,Int)]
ATTR Expr INH env::Env
SEM Expr | Var  lhs.value =  fromJust $ 
                             lookup @lhs.env @name
\end{code}
The preprocessor translates inherited attributes into
extra parameters for the semantic functions in the algebra.
This overcomes problem 4.

In many situations, |SEM| rules only specify that attributes
a tree node inherites 
should be passed unchanged to its children, as in a |Reader| monad.
To scrap the boilerplate expressing this, 
the preprocessor has
a convention that, 
unless stated otherwise, attributes with the same name
are automatically copied.
A similar automated copying is done for synthesized attributes
passed up the tree, as in a |Writer| monad.
When more than one child offers a synthesized attribute with the required name,
we can specify to |USE| an operator to combine several candidates:
\begin{code}
ATTR Expr Stat SYN listing USE (++) []
\end{code}
which specifies that by default, the synthesized
attribute |listing| is the concatenation of the |listing|s of
all children that produce a sub-listing, or the empty list if no child produces one.
This overcomes problem 5.



\subsection{Rule-oriented programming}

\todo{Ruler as a preprocessor}

\subsection{Aspect-oriented programming}

\todo{AG affords aspects-orientation, and shuffle even more}


\section{Languages}

The compiler translates a Haskell program to executable code
by applying many small transformations.
In the process, the program is represented using five different
data structures, or languages.
Some transformations map one of these languages to the next,
some are transformations within one language.
Together, the five languages span the spectrum between
a full feature, lazy functional language, 
and a limited, low level imperative language.

Here is a short characterization of these languages.
The remainder of this section gives a more detailed description.
\begin{enumerate}
\item Haskell (HS): a general-purpose, higher-order, typed, lazy functional language.
\item Essential Haskell (EH): a higher-order, typed, lazy functional language close to lambda-calculus, without syntactic sugar.
\item Core: an untyped, lazy functional language close to lambda-calculus
            (similar to, but not the same as the Core language used in GHC).
\item Grin: `Graph reduction intermediate notation', the instruction set of a virtual machine
            of a small functional language with strict semantics, with features that enable implementation of lazyness.
\item Silly: `Simple imperative little language', an abstraction of features found in every imperative language
            (if-statements, assignments, explicit memory allocation) augmented with primitives for manipulating a stack,
            easily translatable to e.g.\ C (not all features of C are provided, only those that are needed for our purpose).
\end{enumerate}





\subsection{The Haskell Language}

The Haskell language (HS) closely follows Haskell's concrete syntax.
It consists of numerous datatypes, some of which have many constructors.
A |Module| consists of a name, exports, and declarations.
Declarations can be varied: function bindings, pattern bindings, type signatures, 
data types, new types, type synonyms, class, instance\dots
Function bindings involve a right hand side which is either an expression or a list of guarded expressions.
An expression, in turn, has no less than 29 alternatives.
All in all, the description of the context-free grammar consists of about 1000 lines of code.

We maintain sufficient information in the abstract syntax tree 
to reconstruct the original input; this includes parentheses, indentation, etc. (but no comments).



\subsection{The Essential Haskell Language}

In constrast to the HS language, the Essential Haskell language (EH)
brings back the language to its essence, removing as much syntactic sugar as is possible.
An EH module consists of a single expression only, which is
body of the |main| function, with local let-bindings for the other top-level values.





\subsection{The Core Language}

A Core module, apart from its name,
consists of nothing more than an expression,
which can be thought of as the body of |main|:
%%[[wrap=code
DATA  CModule
   =  Mod  nm:Name   expr:CExpr
%%]]
An expression resembles an expression in lambda calculus.
We have constants, variables, and lambda abstractions and applications of one argument:
\typeout{todo: encoding of constructors in CExpr}
%%[[wrap=code
DATA  CExpr
   =  Int     int:Int
   |  Char    char:Char
   |  String  str:String
   |  Var     name:Name
   |  Lam     arg:Name    body:CExpr
   |  App     func:CExpr  arg:Cexpr
%%]]
Furthermore, there is case distinction and local binding:
%%[[wrap=code
   |  Case    expr:CExpr   alts:[CAlt]    dflt:CExpr
   |  Let     categ:Categ  binds:[CBind]  body:CExpr
%%]]
The |categ| of a |Let| describes whether the binding is recursive or not.
These two constructs use the auxiliary notions of alternative and binding:
%%[[wrap=code
DATA  CAlt
   =  Alt     pat:CPat   expr:CExpr
DATA  CBind   
   =  Bind    name:Name  expr:CExpr
   |  FFI     name:Name  imp:String   ty:Ty
%%]]
\todo{description of CPar in Core}




\subsection{The Grin Language}

A Grin module consists of its name,
global variabels with their initializations, and
bindings of function names with parameters to their bodies.
%%[[wrap=code
DATA  GrModule
   =  Mod   nm:Name  globals:[GrGlobal]  binds:[GrBind]
DATA  GrGlobal
   =  Glob  nm:Name  val:GrVal
DATA  GrBind
   =  Bind  nm:Name  args:[Name]  body:GrExpr
%%]]
Values manipulated in the Grin language are varied:
we have nodes (think: heap records) consisting of a tag and a list of fields,
standalone tags, basic ints and strings, pointers to nodes, and `empty'.
Some of these are directly representable in the languages (nodes, tags, ints and strings)
%%[[wrap=code
DATA  GrVal
   =  LitInt  int:Int
   |  LitStr  str:String
   |  Tag     tag:GrTag
   |  Node    tag:GrTag   flds:[GrVal]
%%]]
Pointers to nodes are also values, but they have no direct denotation.
On the other hand, variables ranging over values are not a value themselves,
bur for syntactical convenience we do add
the notion of a `variable' to the |GrVal| data type:
%%[[wrap=code
   |  Var     name:Name
%%]]
The tag of a node describes its role.
It can be a constructor of a datatype (|Con|),
a function of which the call is deferred because of lazy evaluation (|Fun|),
a function that is partially parameterized but still needs more arguments (|PApp|), or
a deferred application |apply| of an unknown function (appearing as the first argument of the node) to a list arguments (|App|).
%%[[wrap=code
DATA  GrTag
   =  Con   name:Name
   |  Fun   name:Name
   |  PApp  needs:Int  name:Name
   |  App   applyfn:Name
%%]]
The body of a function denotes the calculation of a value,
which is represented in a program by an `expression'.
Expressions can be combined in a monadic style.
Thus we have |Unit| for describing a computation immediately returning a value,
and |Seq| for binding a computation to a variable (or rather a lambda pattern), to be used subsequently in another computation:
%%[[wrap=code
DATA  GrExpr
   =  Unit   val:GrVal
   |  Seq    expr:GrExpr  pat:GrPatLam  body:GrExpr
%%]]
There are some primitive computations (that is, constants in the monad)
one for storing a node value (returning a pointer value), and two
for fetching a node previously stored, and for fetching one field thereof:
%%[[wrap=code
   |  Store       val:GrVal
   |  FetchNode   name:Name
   |  FetchField  name:Name  offset:Int
%%]]
Other primitive computations call Grin and foreign functions, respectively.
The name mentioned is that of a known function (i.e., there are no function variables) and the argument list should fully saturate it:
%%[[wrap=code
   |  Call        name:Name    args:[GrVal]
   |  FFI         name:String  args:[GrVal]
%%]]
Two special primitive computations are provided for evaluating node that may contain a |Fun| tag,
and for applying a node that must contain a |PApp| tag (a partially parameterized function) to further arguments:
%%[[wrap=code
   |  Eval        name:Name
   |  App         name:Name    args:[GrVal]
%%]]
Next, there is a computation for selecting a matching alternative, given the name of the variabele containing a node pointer:
%%[[wrap=code
   |  Case        val:GrVal    alts:[GrAlt]
%%]]
Finally, we need a primitive computation to express the need of `updating' a variable after it is evaluated.
Boquist proposed an |Update| expression for the purpose which has a side effect only and an `empty' result value \cite{boquist99phd-optim-lazy}.
We observed that the need for updates is always next to either a |FetchNode| or a |Unit|, and found it more practical
and more efficient to introduce two update primitives:
%%[[wrap=code
   |  FetchUpdate  src:Name  dst:Name
   |  UpdateUnit   name:Name  val:GrVal
%%]]
Auxiliary data structures are that for describing a single alternative in a |Case| expression:
%%[[wrap=code
DATA  GrAlt
   |  Alt   pat:GrPatAlt   expr:GrExpr
%%]
And for two kinds of patterns, occuring in a |Seq| expression and in an |Alt| alternative, respectively.
A simplified version of these is the following, but we will need some more constructores for patterns later.
%%[[wrap=code
DATA  GrPatLam
   =  Var   name:Name
DATA  GrPatAlt
   =  Node  tag:GrTag   args:[Name]
%%]






\subsection{The Silly Language}

\todo{Description of Silly}



\section{Transformations}

\subsection{HS Transformation}

\subsection{EHC Transformation}

\subsection{Core Transformations}

Three major gaps have to be bridged in the transformation from 
Core to Grin.
Firstly, where Core has a lazy semantics, in Grin deferring of
function calls and their later evaluation is explicitly encoded.
Secondly, in Core we can have local function definitions,
whereas in Grin all function definitions are at top level.
Grin does have a mechanism for local, explicitly sequenced variable bindings.
Thirdly, wheras Core functions always have one argument,
in Grin functions can have multiple parameters, but they
take them all at the same time. 
Therefore a mechanism for partial parametrization is necessary.

Before the actual translation from Core to Grin takes place,
a series of Core to Core transformations is applied, which
perform `lambda lifting', that is move
lambda-expressions to the top level. Where bodies of lambda expressions
refer to non-global variables, they are made into explicit parameters.
Apart from lambda lifting, also a few optimizations are applied in the following list of all transformations:
\begin{enumerate}
\item {\em EtaReduction}
\item {\em RenameUnique}
    Renames variables such that all variables are globally unique.
\item {\em LetUnrec}
\item {\em InlineLetAlias}
    Inlines let bindings for variables and constants.
\item {\em ElimTrivApp}
    Eliminates application of the |id| function.
\item {\em ConstProp}
    Performs addition of int constants at compile time.
\item {\em FullLazy}
    Lambda abstractions are moved outward as far as possible.
\item {\em LamGlobalAsArg}
    Pass global variables of let-bound lambda-expressions as explicit parameters,
    as a preparation for lambda-lifting.
\item {\em CAFGlobalAsArg}
    Similar for let-bound constant applicative forms (CAFs).
\item {\em FloatToGlobal}
    Performs 'lambda lifting': move bindings of lambda-expressions and CAFs to the global level.
\item {\em LiftDictFields}
    Makes sure that all dictionary fields exist as a top-level binding.
\item {\em FindNullaries}
    Finds nullary (parameterless) functions and duplicates them;
    the two copies are differently annotated,
    such that one of the two will end up as an updateable global variable.
\end{enumerate}
After the transformations, translation to Grin is performed,
where the following issues are adressed:
\begin{itemize}
\item for |Let|-expressions:
      global expressions are collected and made into Grin function bindings;
      local non-recursive expressions are sequenced by Grin |Seq|-expressions;
      for local recursive let-bindings a |Seq|uence is created
      which starts out to bind a new variable to a `black hole' node, then processes the body, and finally generates a |FetchUpdate|-expression for the introduced variable.
\item for |Case|-expressions:
      an explicit |Eval|-expression for the scrutinee is generated, in |Seq|uence with
      a Grin |Case|-expression.
\item for |App|-expressions:
      it is determined what it is that is applied:
      \begin{itemize}
      \item if it is a constructor, then a node with |Con| tag is returned;
      \item if it is a lambda of known arity which has exactly the right number of arguments, then
            either a |Call|-expression is generated (in strict contexts)
            or a node with |Fun| tag is stored with a |Store|-expression (in lazy contexts);
      \item if it is a lambda of known arity that is undersaturated (has not enough arguments), then
            a node with |PApp| tag is returned (in strict contexts) or |Store|d (in lazy contexts)
      \item if it is a lambda of known arity that is oversaturated (has too many arguments), then
            (in stict contexts) first a |Call|-expression to the function is generated that applies the function
            to some of the arguments, and the result is bound to a variable that is sub|Seq|uently |App|lied
            to the remaining arguments; or
            (in non-strict contexts) a node with |Fun| tag is |Store|d, and bound to a variable
            that is used in another node which has an |App| tag.
      \item if it is a variable that represents a function of unknown arity, then
            (in strict contexts) the variable is explicitly |Eval|uated, and its result used in an |App|expression to the arguments; or
            (in non-strict contexts) as a last resort, both function variable and arguments are stored in a node with |App| tag.
      \end{itemize}
\item for global bindings:
      lambda abstractions are `peeled off' the body, to become the arguments of a Grin function binding.
\item for foreign function bindings:
      functions with |IO| reslut type are treated specially.
\end{itemize}

We have now reached the point in the compilation pipeline where we perform our full-program analysis.
The Core module of the program under compilation is merged with the Core modules of all used libraries.
The resulting big Core module is then translated to Grin.



\subsection{Grin Transformations}

In the Grin world, we take the opportunity to perform many optimizing transformations.
Other transformations are designed to move from graph manipulation concepts
(complete nodes that can be `fetched', `evaluated' and pattern matched for)
to a lower level where single word values are moved and inspected in
the imperative target language.

We first list all transformations in the order they are performed,
and then discuss some issues that are tackled with the combined effort
of multiple transformations.

\begin{enumerate}
\item {\em DropUnreachableBindings}
    Drops all functions not reachable from |main|,
    either through direct calls, 
    or through nodes that store a deferred or partially parameterized function.
    The transformation performs a provisional numbering of all functions, and creates a graph of dependencies.
    A standard reachablility algorithm determines which functions are reachable from |main|;
    the others are dropped.
\item {\em MergeInstance}
    Introduces an explicit dictionary for each instance declaration,
    by merging the default definitions of functions taken from class declarations.
    This is possible because we have the full program available now (see discussion below).
\item {\em MemberSelect}
    Looks for the selection of a function from a dictionary and its subsequent
    application to parameters. Replaces that by a direct call.
\item {\em DropUnreachableBindings}
    Drops the now obsolete implicit constructions of dictionaries.
\item {\em Cleanup}
    Replaces some node tags by equivalent ones:
    |PApp 0|, a partial application needing 0 more parameters, is changed into |Fun|, a simple deferred function;
    deferred applications of constructor functions are changed to immediate application of the constructor function.
\item {\em SimpleNullary}
    Optimises nullary functions that immediately return a value or call another function,
    by inlining them in nodes that encode their deferred application.
\item {\em ConstInt}
    Replaces deferred applications of |integer2int| to constant integers by a constant int.
    This situation occurs for every numeric literal in the source program,
    because of the way literals are overloaded in Haskell.
\item {\em BuildAppBindings}
    Introduces bindings for |apply| functions with as many parameters as are needed in the program.
\item {\em GlobalConstants}
    Introduces global variables for each constant foud in the program,
    instead of allocation the constants locally.
\item {\em Inline}
    Inlines functions that are used only once at their call site.
\item {\em SingleCase}
    Replaces case expressions that have a single alternative by the body of that alternative.
\item {\em EvalStored}
    Do not do |Eval| on pointers that bind the result of a previous |Store|.
    Instead, do a |Call| if the stored node is a deferred call (with a |Fun| tag), 
    or do a |Unit| of the stored node for other nodes.
\item {\em ApplyUnited}
    Do not perform |Apply| on variables that bind the result of a previous |Unit| of a node with a |PApp| tag.
    Instead, do a |Call| of the function if it is now saturated, or build a new |PApp| node if it is undersaturated.
\item {\em SpecConst}
    Specialize functions that are called with a constant argument.
    The transformation is useful for creating a specialized `increment' function instead of |plus 1|,
    but its main merit lies in making specialized versions of overloaded functions, 
    that is functions that take a dictionary argument.
    If the dictionary is a constant, specialization exposes new opporunities for the {\em MemberSelect} transformation,
    which is why {\em SpecConst} is iterated in conjunction with {\em EvalStored}, {\em ApplyUnited} and {\em MemberSelect}.
\item {\em DropUnreachableBindings}
    Drops unspecialized functions that may have become obsolete.
\item {\em NumberIdents}
    Attaches an unique number to each variable and function name.
\item {\em HeapPointsTo}
    Does a `heap points to analysis' (HPT), which is an abstract interpretation of the program
    in order to determine the possible tags of the nodes that each variable can refer to.
\item {\em InlineEA}
    Replaces all occurences of |Eval| and |Apply| to equivalent constructs.
    Each |Eval x| is replaced by |FetchNode x|, followed by a |Case| distinction
    on all possible tag values of the node referred to by |x|,
    which was revealed by the HPT analysis.
    If the number of cases is prohibitively large, we resort to a |Call| to a generic |evaluate| function,
    that is generated for the purpose and that distinguishes all possible node tags.
    Each |App f x| construct, that is used to apply an unknown function |f| to argument |x|, is replaced
    by a |Case| distinction on all possible |PApp| tag values of the node referred to by |f|.
\item {\em ImpossibleCase}
    Removes alternatives from |Case| constructs that, according to the HPT analysis, can never occur.
\item {\em LateInline}
    Inlines functions that are used only once at their call site.
    New opportunities for this transformation are present because the {\em InlineEA} transformation introduces new |Call| constructs.
\item {\em SingleCase}
    Replaces case expressions that have a single alternative by the body of that alternative.
    New opportunities for this transformation are present because the {\em InlineEA} transformation introduces new |Case| constructs.
\item {\em DropUnusedExpr}
    Removes bindings to variables if the variable is never used,
    but only when the expression has no side effect.
    Therefore, an analysis is done to determine which expressions may have side effects.
    |Update| and |FFI| expressions are assumed to have side effect, 
    and |Case| and |Seq| expressions if one of their childres does.
    The tricky one is |Call|, which has a side effect if its body does,
    which is circular if the function is recursive.
    Thus we take a 2-pass approach: a `coarse' approximation that assumes that every |Call| has a side effect, 
    and a `fine' approximation that takes into account the coarse approximation for the body.
    Variables that are never used but which are retained because of the possible side effects of their bodies are replaced by wildcards.
\item {\em MergeCase}
    Merges two adjacent |Case| constructs into a single one in some situations.
\item {\em LowerGrin}
    Translates to a lower level version of Grin, in which variables never represent a node.
    Instead, variables are introduced for the separate fields, of which the number became known through HPT analysis.
    Also, after this transformation |Case| constructs scrutinise on tags rather than full nodes.
\item {\em CopyPropagation}
    Shortcuts repeated copying of variables.
\item {\em SplitFetch}
    Translates to an even lower level version of Grin, in which the node referred to by a pointer is not fetched as a whole,
    but field by field. That is, the |FetchNode| expression is replaced by a series of |FetchField| expressions.
    The first of these fetches the tag, the others are specialized in the alternatives of the |Case| expression
    that always follows a |FetchNode| expression, such that no more fields are fetched than required by this particular tag.
\item {\em DropUnusedExpr}
    Removes variable bindings introduced by {\em LowerGrin} if they happen not to be used.    
\item {\em CopyPropagation}
    Again shortcuts repeated copying of variables.
\end{enumerate}    


\paragraph{Simplification}
The Grin language features constructs for manipulating heap nodes,
including ones that encode deferred function calls, that are explicitly
triggered by an |Eval| expression.
As part of the simplification, this high level construct should be decomposed in smaller steps.
Two strategies can be taken to implement evaluation:
\begin{itemize}
\item {\em tagged}:  nodes are tagged by small numbers,
                     evaluation is performed by calling a special |evaluate| function that scrutinizes the tag,
                     and for each possible |Fun| tag calls the corresponding function and updates the thunk;
\item {\em tagless}: nodes are tagged by pointers to code that does the call and update operations,
                     thus evaluation is tantamount to just jumping to the code pointed to by the tag.
\end{itemize}
The tagged approach has overhead in calling |evaluate|,
but the tagless approach has the disadvantage that the indirect jump involved may stall the lookahead buffer of pipelined processors.
Boquist proposed to inline the |evaluate| function at every occurence of |Eval|,
where for every instance the |Case| expression involved only contains those cases which can actually occur.
It is this approach that we take in UHC.

This way, they high level concept of |Eval| is replaced by lower level concepts of |FetchNode|, |Case|, |Call| and |Update|.
In turn, each |FetchNode| expression is replaced by a series of |FetchField| expressions in a later transformation,
and the |Case| that scrutinzes a node is replaced by one that scrutinizes the tag only.

    
\paragraph{Abstract interpretation}
The desire to inline a specialized version of |evaluate| at every |Eval| instance
brings the need for an analysis that for each pointer variable determines the possible tags of the node.
An abstract interpretation of the program, known as `heap points to (HPT) analysis' tries to approximate this knowledge.
As a preparation, the program is scanned to collect constraints on variables.
Some constaints immediately provides the information needed (e.g., the variable that binds the result of a |Store| expression
is obviously a pointer to a node with the tag of the node that was stored),
but other constraints are indirect (e.g., the variable that binds the result of a |Call| expression
will have the same value as the called function returns).
The analysis is essentially a full-program analysis, as actual parameters of functions
impose constraints on the parameters.

The constraint set is solved in a fixpoint iteration, which processes the indirect constraints
based on information gathered thus far. In order to have fast access to the mapping that records
the abstract value for each variable, we uniquely number all variables, and use mutable arrays to store the mapping.

Special attention deserves the processing of the constraint that expresses that |x| binds the 
result of |Eval p|.
If |p| is already known to point to nodes with a |Con| tag (i.e., values) then this is also a possible value for |x|.
If |p| is known to point to nodes with a |Fun f| tag (i.e., deferred functions), then the possible results for |f| are also possible values for |x|.
And if |p| is known to point to nodes with an |App apply| tag (i.e., generic applications of unknown functions by |apply|),
then the possible results for |apply| are also possible values for |x|.


\paragraph{HPT performance}

The HPT analysis must at least find all possible tags for each pointer, but it is sound if it reports a superset of these.
The design of the HPT analysis is a tradeoff between time (the number of iterations it takes to find the fixed point)
and accuracy.
A trivial solution is to report (in 1 step) that every pointer may point to every tag;
a perfect solution would solve the halting problem and thus would take infinite time in some situations.

We found that the number of iterations our implementation takes, is dependent of two factors:
the depth of the call graph (usually bounded by a dozen or so in practice),
and the length of static data structures in the program.
The latter surprised us, but is understandable if one considers the program
%%[[wrap=code
main = putStrLn (show (last [id,id,id,id,succ] 1))
%%]
where it takes 5 iterations to find out that 1 is a possible parameter of |succ|.

As for accuracy, our HPT algorithm works well for first-order functions.
In the presence of many higher-order functions, the results suffer from `pollution':
the use of a higher-order function in one context also influences its result in another context.
We counter this undesired behaviour in a number of ways:
\begin{itemize}
\item instead of using a generic |apply| function, the {\em BuildAppBindings} transformation
      makes a fresh copy for each use by an |App| tag. This prevents mutual pollution of |apply| results,
      and also increases the probability that the |apply| function can be inlined later;
\item we specialize overloaded functions for every dictionary that it is used with,
      to avoid the |Apply| needed on the unknown function taken from the dictionary;
\item we fall back on explicitly calling |evaluate| (instead of inlining it) in situations where the
      number of possible tags is unreasonable large.
\end{itemize}



\paragraph{Instance declarations}

The basic idea of implementing instances is simple:
an instance is a tuple (known as a `dictionary') containing all member functions,
which is passed as an additional parameter to overloaded functions.
Things are complicated, however, by the presence of default implementations in classes:
the dictionary for an instance declaration is a merge of the default implementations
and the implementations in the instance declaration.
Even worse, the class declaration may reside in another module than the instance declaration,
and still be mutally dependent with it.
Think of the |Eq| class, having mutually circular definitions of |eq| and |ne|, leaving
it to the instance declaration to implement either one of them (or both).

A clever scheme was designed by Fax\'en to generate the dictionary from a generator function
that is parameterized by the dictionary containing the default implementations,
while the default dictionary is generated from a generator function
parameterized by the instance dictionary \cite{faxen02semantics-haskell}.
Lazy evaluation and black holes make this all work, and we employ this scheme in UHC too.
It would be a waste, however, now that we are in a full program analysis situation,
not to try to do as much work as possible at compile time.

Firstly, we have to merge the default and instance dictionaries.
In the Grin world, we have to deal with what the Core2Grin transformation
makes of the Fax\'en scheme. That is:
\begin{itemize}
\item A 1-ary generator function |gfd| that, given a default dictionary, will generate the dictionary;
\item A 0-ary function |fd| that binds a variable to a black hole, calls |gfd|, and returns the result
\item A global variable |d| which is bound to a node with tag |Fun fd|.
\end{itemize}
We want to change this in a situation where |d| is bound directly to the dictionary node.
This involves reverse engineering the definitions of |d|, |fd| and |gfd| to find the
actual member function names buried deep in the definition of |gfd|.
Although possible, this is very volatile as it depends on the details of the Core2Grin translation.
Instead, we take a different approach: the definition of |fd| is annotated with the names of the member functions
at the time when it is still explicitly available, that is during the EH2Core translation.
Similary, class definitions are annotated with the names of the default functions.
Now the {\em Grin.MergeInstance} transformation can easily collect the required dictionary fields,
provided that the {\em Core.LiftDictFields} transformation ensures they are available as top-level functions.
The |fd| and |gfd| functions are obsolete afterwards, and can be discarded by a later reachability analysis.

Secondly, we hunt the program for 
dictionaries $d$ (as constructed above) and
selection functions $s_k$ (easily recognizable as a function that pattern-matches its parameter to a dictionary structure and returns its $k$th field $x_k$).
In such situations |Call s_k d| can be replaced by |Eval x_k|.
A deferred member selection, involving a node with tag |Fun s_k| and field |d|, is dealt with similarly:
both are done by the {\em MemberSelect} transformation.

Thirdly, as $x_k$ is a dictionary field, it is a known node |n|.
If |n| has a |Fun f| tag, then |Eval x_k| can be replaced by |Call f|,
and otherwise it can be replaced by |Unit n|.
This is done by the {\em EvalStored} transformation.
The new |Unit| that is exposed by this transformation can be combined with the |App| expression
that idiomatically follows the member selection, which is what {\em ApplyUnited} does.

All of this only works when members are selected from a constant dictionary.
Overloaded functions however operate on dictionaries that are passed as parameter,
and member selection from a variable dictionary is not caught by {\em MemberSelect}.
The constant dictionary appears where the overloaded function is called,
and can be brought to the position where it is needed by
specializing functions when they are called with constant arguments.
This is done in the {\em SpecConst} transformation.
Besides useful in the chain of transformations that together remove the dictionaries,
it is useful for removal of other constants, like a 1-ary successor function as a 
specialization of |plus 1|.
(If constant specialization is also done for string constants, we get
many specializations of |putStrLn|).

The whole pack of transformations is applied repeatedly, as applying them
exposes new opportunities for sub-dictionaries.
Four iterations suffice to deal with the common cases (involving |Eq|, |Ord|, |Integral|, |Read| etc.)
from the prelude.

The only situation where dictionaries cannot be eliminated completely, is where an infinite family
of dictionaries is necessary, such as arises from the |Eq a => Eq [a]| instance declaration
in the prelude. In this situation we automatically fall back to the Fax\'en scheme.


\paragraph{Foreign functions}


    

\subsection{Silly Transformations}


\begin{enumerate}
\item {\em InlineExpr}
Avoids copying variables to other variables, 
if in all uses the original one could be used just as well
(i.e., are not modified in between).

\item {\em ElimUnused}
Eliminates assignments to variables that are never used.

\item {\em EmbedVars}
Silly has a notion of function arguments and local variables.
After this transformation, these kind of variables are not used anymore,
but replaced by explicit strack offsets.
So, this transformation does the mapping of variables to stack positions
(and, if available, registers).
At the end of each function, the function result values overwrites the parameter area
of the stack; the assignments are scheduled in such a way
that is does not overwrite parameters that are still needed in assignments to follow.

\item {\em GroupAllocs}
This transformation combines separate, adjacent calls to |malloc| into one,
enabling to do heap overflow check only once for all the memory that
is allocated in a particular function.

\end{enumerate}




\section{Conclusion}

\subsection{Methodological observations}


\paragraph{Stepwise transformation}

\todo{We roberen zoveel mogelijk te genereren, zo specifiek mogelijke data
staructuren te gebruiken en zoveel mogelijk stapje voor stapje te doen. Dit in
contrast met de GHC waar er een grote datastructuur is die alle informatie bevat
en die langzamerhand getransformeerd wordt. daarbij wordt er af en toe een "the
impossible happened" "my brain just exploded" boodschap gegenereerd als er een
onverwachte situatie tegen wordt gekomen: "Ik had niet gedacht dat deze
constructor in deze fase nog voor kwam". Dat proberen we met aparte types zoveel
mogelijk te voorkomen.}


\paragraph{AG Design Patters}

\todo{Often multi-pass, first collect environment, which is distributed a la repmin.}

\todo{Sometimes there is a need for structure pattern matching.}

\todo{en dan zeggen dat je hier in de evaluatie op terug komt, en dan
laten zien hoeveel passes er nodig zijn en hoeveel attributen er rond hangen. En
dan het suggestieve zinnetje: do we really want to build a stack of moe than ten
monad transformers...}


\paragraph{Annotations}

\todo{We tend to extend languages with annotations.
Either to prevent keeping separate lookup tables (e.g. for arity of constructors),
or to keep information that is necessary afterwards (e.g. type information of FFI's),
or to track the origin of constructs (e.g. class declaration).}




\subsection{Related work}

\todo{GHC; YHC; Hugs; Boquist and JHC/LHC}


\subsection{Future work}

\todo{More libraries, cabal support etc.
Full Haskell98 (n+k patterns, type subtleties).
More optimisations, especially full program.
Own garbage collector to be independent of B\"ohm.
Strictness analysis.}


%%]




