\documentclass[preprint,9pt]{sigplanconf}

\usepackage{graphicx} %% needs: fancyvrb



%include lhs2tex.fmt

\def\spacecorrection{\;}
\def\isspacecorrection{\spacecorrection}
\def\allowforspacecorrection#1{%
  \gdef\temp{#1}%
  \ifx\isspacecorrection\temp
    \let\next=\empty
  \else
    \let\next=\temp
  \fi
  \next}



\newcounter{enumctr}
\newenvironment{enumate}{%
\begin{list}{\arabic{enumctr}}{
\usecounter{enumctr}
\parsep  = 0pt
\parskip = 0pt
\topsep  = 0pt
\itemsep = 0pt
}}{\end{list}}
\newenvironment{itize}%
{\begin{list}%
  {$\bullet$%
  }%
  {\parsep  = 0pt%
   \parskip = 0pt%
   \topsep  = 0pt%
   \itemsep = 0pt%
  }%
}%
{\end{list}%
}


%format ^*      = "^{*}"
%format epsilon = "\epsilon"
%format bottom = "\bot"
%format ...    = "\mbox{\dots}"
%format not    = "\mathit{not}"


% Transformations

%format mergeInstance   =  "\mbox{{\sf MergeInstance}}"
%format selectMember    =  "\mbox{{\sf SelectMember}}"
%format specConst       =  "\mbox{{\sf SpecConst}}"
%format evalKnown       =  "\mbox{{\sf EvalKnown}}"
%format applyKnown      =  "\mbox{{\sf ApplyKnown}}"
%format dropUnused      =  "\mbox{{\sf DropUnused}}"
%format dropUnreachable =  "\mbox{{\sf DropUnreachable}}"


% Core keywords

%format let    = "\mathbf{let}"
%format in     = "\mathbf{in}"
%format letrec = "\mathbf{let}^{\mathbf{R}}"
%format letstrict = "\mathbf{let}^{\mathbf{S}}"

% Grin annotations

%format dictinst   = "\mathbf{dictinst}"
%format dictclass  = "\mathbf{dictclass}"
%format specialized  = "\mathbf{specialized}"
%format overloaded  = "\mathbf{overloaded}"

% Grin metanotions

%format BVar   = "\mathit{\underline{Var}}"
%format BName  = "\mathit{\underline{Name}}"
%format return = "\mathbf{return}"


% Grin keywords

%format evalf = "\mathit{eval}"
%format eval  = "\mathbf{eval}"
%format apply = "\mathbf{apply}"
%format store = "\mathbf{store}"
%format unit  = "\mathbf{unit}"
%format call  = "\mathbf{call}"
%format case  = "\mathbf{case}"
%format of    = "\mathbf{of}"

% Grin tags

%format /     = "\mbox{/}"
%format P1    = "\mathbf{P}_1"
%format P2    = "\mathbf{P}_2"
%format P3    = "\mathbf{P}_3"
%format F     = "\mathbf{F}"
%format A     = "\mathbf{A}"
%format C     = "\mathbf{C}"
%format P     = "\mathbf{P}"
%format Pm    = "\mathbf{P}_{m}"
%format Pnm   = "\mathbf{P}_{n-m}"
%format PN    = "\mathbf{P}_{N}"

%format a1    = "\mathit{a}_{1}"
%format an    = "\mathit{a}_{n}"
%format app9  = "\mathit{app}_{n}"
%format b0    = "\mathit{b}_{0}"
%format bk    = "\mathit{b}_{k}"
%format e1    = "\mathit{e}_{1}"
%format e2    = "\mathit{e}_{2}"


%format .    = "."
%format ^    = " "
%format ^^    = "\;"
%format ^@    = "@"

%format @ = "\spacecorrection @"
%format [          = "[\mskip1.5mu\allowforspacecorrection "
%format (          = "(\allowforspacecorrection "
%subst fromto b e t     = "\fromto{" b "}{" e "}{{}\allowforspacecorrection " t "{}}'n"





\usepackage{amsmath}

% \usepackage{natbib}
% \bibpunct();A{},
% \let\cite=\citep
% \bibliographystyle{plainnat}

\bibliographystyle{plain}

\begin{document}

\conferenceinfo{GPCE '10}{October 11, Freiburg.} 
\copyrightyear{2010}
\copyrightdata{[to be supplied]} 

%\titlebanner{Working copy v.1}        % These are ignored unless
%\preprintfooter{Working copy v.1}   % 'preprint' option specified.

\setlength{\parindent}{0pt}
\setlength{\parskip}{3pt}


\title{Compiling by transformation:\\efficient implementation of overloading in Haskell}

 \authorinfo{Jeroen Fokker\and S.~Doaitse Swierstra}
            {Utrecht University}
            {\{jeroen,doaitse\}@@cs.uu.nl}

\maketitle

\begin{abstract}
The Utrecht Haskell Compiler (UHC) is designed as 
the composition of many small transformations.
We illustrate the transformational approach by showing 
how overloading is implemented and optimized in UHC.
Overloaded functions take additional `dictionary' arguments, 
which are automatically inserted during code generation,
based on the inferred types.

For each instance declaration, a dictionary is generated 
containing the functions defined in the instance.
The dictionary also contains the default definitions 
from the corresponding class declaration,
thus requiring a mechanism for combining them.

When modules are compiled separately, this combining is done dynamically,
during program startup or at the first use of the dictionary.
When performing whole-program analysis, however, 
information from the class and instance declarations
can be combined statically using symbolic computation.
Further transformations, notably specialization of functions for constant arguments,
can completely eliminate run-time overhead normally associated with dictionary passing.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

%\terms
%term1, term2

%\keywords
%keyword1, keyword2

\section{Introduction}

Compiling a program is the oldest and most widely used example of generative programming.
They are so common, that programmers are hardly conscious any more that compiling, 
say, a C pogram alleviates them of the tedious task of writing machine code.
With compilers for higher level languages, it is more manfest that a compiler
generates code, which in turn can be regarded as source code for another compiler.
For example, Haskell compilers often target the C language, and domain specific
languages in turn may target Haskell.

Compilers are complex programs.
The translating process involves many tasks, such as parsing, type checking,
modelling memory structure, code generation, and optimization.
These tasks ideally are dealt with in separate components, and by selecting and
arranging these components one can tune a compiler for different requirements,
such as various levels of optimization, or targeting various back ends.

We decided to build a compiler for Haskell 
(the Utrecht Haskell Compiler, or UHC)
that radically takes a component based approach: 
it translates a program by transforming it
in many small steps. Each transformation is relatively simple, and
can be understood and tested separately.
Along the route from source (Haskell) to target (C) language, 
the program is transformed through some intermediate languages:
some of them existing, some of them defined for the purpose.
Therefore some transformations (5 of them) are actually translations
between languages,
and many others (about 60) are source-to-source translations in
one of the intermediate languages.

In this paper we present a case study that illustrates the approach:
the implementation of function overloading in Haskell.
This case touches many aspects: 
modelling a high-level concept (overloading) by lower level datastructures (dictionaries),
dealing with two compiler modes (separate compiling versus whole-program optimization),
and the composition of 6 transformations that together accomplish a task.

Moreover, the Haskell feature under scrutiny (type classes and their instances) happens to
be a mechanism that is often used by Haskell programmers for achieving modularity,
and thus is interesting in its own right from a component engineering perspective.

To make the paper self-contained, we start in section~\ref{sec.overloading} with a short description
of overloading in Haskell.
In section~\ref{sec.uhc} we sketch the structure of UHC,
and introduce the intermediate language central to this case study.
Then implementation of overloading is described for two compiler modes:
in separate compilation mode (section~\ref{sec.dynamic}) we see generative programming at work,
wheras in whole-program analysis mode (section~\ref{sec.static}) many components
cooperate to optimize the code.
We conclude with remarks on the methodology and pointers to related work.





\section{Overloading in Haskell}\label{sec.overloading}
In this section we briefly summarize how function overloading is modeled in Haskell.
Overloaded functions can be applied to values of various types.
For example, the addition function can be applied to |Int| values and also to |Float| values (but not to |Bool| values).
The function which tests for equality can be applied to values of many types, like |Int|, |Float|, |Char|, |Bool|.
Furthermore, it can even be applied to values of list type |[a]|, provided that it is also defined for the element type |a|.
Note however that equality testing is not possible for values of a function type.

A closely related concept is {\em polymorphism}.
Polymorphic functions can be applied to values of various types.
For example, concatenation can be applied to lists of type |[Int]| and to lists of type |[Char]|,
and in fact, to lists of type |[a]| for any type |a|.

The difference is that polymorphic functions have a uniform definition, regardless of the type of the arguments,
whereas an overloaded function usually has a different definition for each type of argument for which it is defined.
The type of arguments of polymorphic functions can be {\em any} instantiation of the type variables in its type,
whereas the type of arguments of overloaded functions can only be those types for which a version of the function has been defined.
Therefore, function overloading is sometimes referred to as {\em ad hoc polymorphism}.

\subsection{Class and instance declarations}

The group of types to which an overloaded function can be applied is known as its {\em class}.
A {\em class declaration} introduces a name for such a group of types, together the signatures of functions.
These signatures can involve type variables, which may be instantiated to types belonging to the group.
For example, when introducing the class |Eq| of types that support equality and non-equality functions
we write the class declaration
\begin{code}
class Eq a where
  eq  ::  a -> a -> Bool
  ne  ::  a -> a -> Bool
\end{code}
This |Eq| class is actually part of the Haskell standard library. 
There, operators $==$ and $/=$ are defined rather than functions |eq| and |ne|,
but in this paper we avoid operators, as the notational conventions for writing them obscures the explanation.

Individual types can be defined to be an instance of a class by an {\em instance declaration}.
For example, |Int| can be made instance of |Eq| by providing definitions for the functions specified in the class declaration:
\begin{code}
instance Eq Int where
  eq x y  =  eqPrimInt x y
  ne x y  =  not (eqPrimInt x y)
\end{code}
The definitions rely on the existence of a function |eqPrimInt| that gives a primitive implementation for equality on integers.
The way |eqPrimInt| is defined is not relevant for the present discussion.

As another example, we give an instance declaration for |Eq Bool|.
It shows that we can define the function |eq| from scratch if we wish, without relying on other functions.
The function |ne| can be defined in a similar way (and that would probably be more efficient),
but just to show the possibility to have |ne| rely on the existence of the other overloaded function |eq|.
\begin{code}
instance Eq Bool where
  eq  False  False  = True
  eq  True   True   = True
  eq  _      _      = False
  ne  x      y      = not (eq x y)
\end{code}
The terminology of the notions `class' and `instance' is borrowed from the object-oriented paradigm.
There are similarities, in that a class declaration specifies functions that are implemented by instance declaration.
But note that there are differences as well: in the object-oriented paradigm, class member functions take an 
object of the class as an implicit argument, 
whereas in the functional paradigm all parameters are declared explicitly; thus we can have two arguments that are
constrained tothe type of this instance declaration.

Not all overloaded functions need to be defined in a class.
Every function that uses an overloaded function, will automatically be overloaded itself.
For example, the function |elem| that checks element membership of a list uses the overloaded equality function |eq| on the list elements:
\begin{code}
elem e []      = False
elem e (x:xs)  = eq e x || elem e xs
\end{code}
Therefore, although the |elem| function has a uniform definition and thus seems to be a polymorphic function,
it can actually only be used on lists of which the elements are in the |Eq| class.
This fact is expressed in the signature of the |elem| function by:
\begin{code}
elem :: Eq a => a -> [a] -> Bool
\end{code}
Note the double-shafted arrow which is to be read as `constrains', as opposed to the single-shafted arrow 
which is to be read as `function from/to'.
We may read this signature as: 
for each type |a| which is an instance of |Eq|, 
|elem| has the type |a -> [a] -> Bool|.

\subsection{Default definitions}
One could question the need for declaring |ne| to be part of the |Eq| class.
After all, non-equality is (in any sound implementation) the negation of equality.
So it would have been possible to only specify |eq| in the class,
and to define |ne| with a uniform definition, in a similar fashion to |elem|:
\begin{code}
class Eq a where
  eq  ::  a -> a -> Bool
ne  ::  Eq a => a -> a -> Bool
ne x y  =  not (eq x y)
\end{code}
Using this approach, instance programmers are freed from the burden to give an explicit, but straightforward, definition of |ne|.
But the downside is that it is now impossible for instance programmers
to define their own version of |ne|. 
The programmer of |Eq Bool| might regret not being able to define
a more efficient version of |ne| that uses pattern matching directly, 
instead of relying on the uniform definition that calls |eq| and |not|.

To overcome this dilemma, Haskell allows a class definition to be augmented with {\em default definitions}
for some or all of the member functions.
Now, instance programmers have the choice either to rely on a default definition of |ne|
as it appears in the class, or to give a more efficient version for a particular type.

A default definition by nature is a uniform definition, as it cannot assume
the type variables in the signature to be instantiated to a particular type.
But the definition can refer to the other functions from the class.
In fact, the Haskell standard library implementation of |Eq| has a default definition 
for both |ne| and |eq|, defined in terms of each other:
\begin{code}
class Eq a where
  eq  ::  a -> a -> Bool
  ne  ::  a -> a -> Bool
  ne x y   =  not (eq x y)
  eq x y   =  not (ne x y)
\end{code}
An instance programmer now has the choice to define either |eq| or |ne| (and rely on the default definition for the other),
or define both (thus ignoring/erasing the default definitions).
Defining neither of the two is allowed as well, but would result in inheriting the circular definitions without
breaking the circle by redefining at least one of the two.


\subsection{Super classes}

Using the terminology of object-oriented paradigm even further, Haskell has the notion of a `superclass' as well.
It is exemplified by the class |Ord| of types that have an ordering, by providing comparison operators like $<$, $<=$, $>$, and $\geq$.
Default definitions specify these in terms of each other.
Instance programmers can choose to implement only one of the four (and rely on the default definitions for the others),
or more if they wish.
But the default definitions not only call each other, but also the |eq| function from class |Eq|.
This is possible because |Ord| is specified to be a subclass of |Eq|;
that is, a type is only allowed to be an instance of |Ord| if it is an instance of |Eq| as well.

A possible class definition of |Ord| is:
\begin{code}
class (Eq a) => Ord a where
  lt  :: a -> a -> Bool
  le  :: a -> a -> Bool
  gt  :: a -> a -> Bool
  ge  :: a -> a -> Bool
  lt   x y  =  not  (ge x y)
  gt   x y  =  not  (le x y)
  le   x y  =  lt  x y || eq x y
  ge   x y  =  gt  x y || eq x y
\end{code}
The superclass is mentioned in the class header, featuring another meaning of the double-shafted arrow, 
and is parenthesized as there can be specified more than one superclass.
The actual definition in the standard library also specifies functions |compare|, |min|, and |max|, 
and an even more intricate web of mutually recursive default definitions.

\subsection{Context for instances}

Finally, Haskell provides a mechanism to declare an instance in the context of another instance.
An example is the definition of |Eq| for lists, where the equality for
lists of elements is expressed using the equality function for the elements.
This only makes sense in a context where the element type is assumed to be instance of |Eq| as well.
\begin{code}
instance  Eq a => Eq [a] where
  eq []      []      =  True
  eq []      (y:ys)  =  False
  eq (x:xs)  (y:ys)  =  eq x y && eq xs ys
\end{code}
The defining expression line contains two calls to |eq|.
The second is a recursive call on the tails of the list.
The first however is not a recursive call, but a call to |eq| for a different instance type:
the list element type, guaranteed to be an instance of |Eq| by the context mentioned in the header.
Type analysis by the compiler makes sure that the right version of |eq| is called.


\begin{figure}[tbfh]
\includegraphics[scale=0.43]{figs/uhc-pipeline.pdf}
\caption{Intermediate languages and transformations in the UHC pipeline, in each of the three operation modes:
whole-program analysis (left), bytecode interpreter (middle), and Java (right).}
\label{fig-uhcarch-pipeline}
\end{figure}


\section{UHC compiler structure}\label{sec.uhc}


\subsection{Transformational programming}

The main structure of the Utrecht Haskell Compiler is shown in Figure~\ref{fig-uhcarch-pipeline}.
Haskell source text is translated to an executable program by stepwise transformation.
Some transformations translate the program to a lower level language,
many others are transformations within one language,
establishing an invariant or performing an optimization.

All transformations, both within a language and between languages, are expressed
as an algebra giving a semantics to the language.
The algebras are described with the aid of an attribute grammar,
which makes it possible to write multi-pass tree-traversals
without even knowing the exact number of passes.
Although the compiler driver is set up to pass data structures between transformations,
for all intermediate languages we have a concrete syntax with a parser
and a pretty printer. This facilitates debugging the compiler, by inspecting
code between transformations.
Here is a short characterization of the intermediate languages:
\begin{itize}
\item Haskell (HS): a general-purpose, higher-order, polymorphically typed, lazy functional language.
\item Essential Haskell (EH): a higher-order, polymorphically typed, lazy functional language close to lambda-calculus, without syntactic sugar.
\item Core: an untyped, lazy functional language close to lambda-calculus,
            augmented with let-bindings and case distinction with simple pattern matching.
\item Grin: `Graph reduction intermediate notation', 
            the instruction set of a virtual machine of a small functional language with strict semantics, 
            with features that enable implementation of laziness \cite{boquist99phd-optim-lazy}.
\item Silly: `Simple imperative little language', an abstraction of features found in every imperative language
            (if-statements, assignments, explicit memory allocation) augmented with primitives for manipulating a stack,
            easily translatable to e.g.\ C (not all features of C are provided, only those that are needed for our purpose).
\item BC: A bytecode language for a low-level machine intended to interpret Grin which is not whole-program analyzed nor transformed.
          We do not discuss this language in this paper.
\end{itize}
The compiler targets different back ends, based on a choice of the user.
In all cases, the compiler starts compiling on a per module basis,
desugaring the Haskell source text to Essential Haskell, type checking it and translating it to Core.
Then there is a choice from three modes of operation:
\begin{itize}
\item In {\em whole-program analysis mode},
      the Core modules of the program and required libraries are assembled together
      and processed further as a whole.
      At the Grin level, elaborate inter-module optimization takes place.
      Ultimately, all functions are translated to low level C,
      which can be compiled by a standard compiler.
      As alternative back ends, we are experimenting with other target languages,
      among which are the Common Intermediate Language (CIL) from the Common language infrastructure used by .NET,
      and the Low-Level Virtual Machine (LLVM) compiler infrastructure.
\item In {\em bytecode interpreter mode},
      the Core modules are translated to Grin separately.
      Each Grin module is translated into instructions for a custom bytecode machine.
      The bytecode is emitted in the form of C arrays,
      which are interpreted by a handwritten bytecode interpreter in C.
\item In {\em Java mode},
      the Core modules are translated to Java bytecode, to be interpreted by the Java virtual machine (JVM).
      Each function is translated to a separate class with an |evalf| function, and
      each closure is represented by an object combining a function with its parameters.
      Together with a driver function in Java which steers the interpretation,
      these can be stored in a Java archive (jar) and be interpreted by a standard Java interpreter.
\end{itize}
The bytecode interpreter mode is intended for use during program development:
it compiles fast, but because of the interpretation overhead execution is not very fast.
The whole-program analysis mode is intended to be used for the final program:
it takes more time to compile, but generates code that is more efficient.



\begin{figure}[tbfh]

\begin{center}
\fbox{
\parbox{8cm}{
\begin{code}
Program  ::=   Bind^*
Bind     ::=   BName BVar^* = Annot { Expr }
         |     BVar <- store Node
Expr     ::=   unit Var
         |     unit Node
         |     Expr ; \ BVar -> Expr
         |     store Node
         |     call  Name Var^*
         |     apply Var Var^*
         |     eval Var
         |     case Var of Alt^*
Alt      ::=   Pat -> Expr
Pat      ::=   ( Tag BVar^* )
Node     ::=   ( Tag Var^* )
         |     ( Tag Lit )
Tag      ::=   C/Constr
         |     PN/Name
         |     F/Name
         |     A
N        ::=   1 | 2 | ...
Z        ::=   N | 0 | -1
Annot    ::=   epsilon
         |     dictclass (Name^*)
         |     dictinst  (Constr Name (Name^*))
         |     overloaded ( (Z^*)^* )
         |     specialized (Name Var^*)
\end{code}
}
}
\end{center}

\caption{Syntax of the Grin language. An |^*| denotes 0 or more occurrences.
|Var|, |Name|, and |Constr| are identifiers referring to values, functions, or constructors.
Underlining denotes a defining position.
|Lit| is an integer or character literal.
}
\label{fig-grin-syntax}
\end{figure}



\subsection{The Grin intermediate language}

Grin is a small language designed to be an intermediate language
between Core (a higher-order lazy functional language) and Silly (an imperative language).
Grin is a functional language (function bodies are expressions which are evaluated when a function is called) 
but it is first-order (all functions are defined at top level and cannot be passed as arguments)
and strict (arguments are evaluated when functions are called).
Nevertheless, Grin has an imperative spirit, as the evaluation of expressions can have side effects on the heap.
The sequencing of these side effects is made explcit in the Grin code, using a monadic notation: the only possible expression forms
are a handful of basic operations on the heap, and a monadic bind and unit operation.

The mapping from Core to Grin makes the evaluation order explicit.
Thus, it has to simulate the lazy semantics of Core by means of primitives that are provided in Grin for that purpose.
The data that is manipulated by Grin is structured into {\em nodes}, which are tagged lists of single-word elements.
Nodes can be assigned to local variables directly, but local variables can also hold pointers to nodes that are stored on the heap.

Refer to figure~\ref{fig-grin-syntax} for the syntax of the Grin language.
It shows that a program consists of bindings, 
each of which is either a function definition 
or the initialization of global variable with a pointer to a node stored on the heap.
Function definitions are optionally annotated with an |Annot|, the meaning of which is described in section~\ref{sec.static}.

The body of a function is an expression of the built-in monad:
either one of six primitive forms, a monadic binding of one expression to a variable in another expression,
or a selection from alternatives based on the value of a variable.
Sequencing is written in monadic style as |e1 ; \x -> e2|, with semantics
`execute |e1|, bind the result to |x| and then execute |e2| in the new context.
Although the lambda symbol suggests that we can define local functions, this is 
by no means general lambda-calculus, as the lambda can only be used in this particular position.
The notation involving the semicolon, lambda and arrow is just a triadic expression form,
which in a different concrete syntax would look more like an imperative assignment statement |x := e1; e2|.

We will discuss the semantics of the various expression forms shortly, 
but first we focus on nodes and tags.
A node can be thought of as a variable-sized block of memory,
consisting of a tag indicating its purpose, and a payload of zero or more pointers.
A special form of node consists of a tag and a primitive literal.

Tags come in four flavors: |C|, |P|, |F|, and |A|.
Nodes with |C| or |P| tag are final: when a pointer to them is evaluated, they remain the same.
Nodes with |F| or |A| tags encode a deferred computation.
They are also known as {\em thunks}, used in the implementation of lazy evaluation.
When a pointer to such a node is evaluated, the computation takes place, and the node is updated with a final node.

The meaning of the four tag flavors is as follows:
\begin{itize}
\item |C| corresponds to a constructor in Haskell. 
      For example, 
      the empty list has tag |C/Nil| and can be represented as a node with zero-length payload |(C/Nil)|.
      A non-empty list can be constructed as |(C/Colon x xs)|.
\item |P| stands for a partially parametrised function.
      In the lambda calculus, a function application with to few arguments to be reduceable is in {\em head normal form},
      that is, cannot be further evaluated. 
      The |P| tag encodes which function was partially parameterized, and (written as a numeric suffix)
      the number of parameters it still lacks.
      The payload of a node with |P|-tag holds the argunents it has already received.
      For example, the node |(P1/plus one)|, where |one| could be globally bound by |one <- store (C/Int 1)|,
      denotes a partial parameterization of function |plus|, still lacking its second argument.
\item |F| is used to denote a deferred call to a function. 
      For example, the node |(F/divide one zero)| is an encoding of the fact that function |divide| will be called,
      but only when this node is forced to evaluation.
\item |A| is used to denote the deferred application of a partially parameterized function to further arguments.
      Nodes with this tag must have a payload of at least one pointer, which is supposed to evaluate to a node with |P| tag.
      For example, the node |(A succ one)|, where variable |succ| is bound to node |(P1/plus one)|,
      will be updated to |(C/Int 2)|, but only when forced to evaluation.
\end{itize}
We now turn to the semantics of the various expression forms.
\begin{itize}
\item |unit n|, where |n| is an explicit node, just returns that node, without having side effects on the heap.
\item |unit x| just returns the value of the variable |x|, which can be a node or a pointer value.
\item |e1 ; \x -> e2| executes |e1|, binds the result to variable |x| and subsequently executes |e2|. It returns whatever |e2| returns.
\item |store n| stores the node value |n| on the heap, and returns a pointer to it
\item |call f a1...an| calls function |f| with the given arguments, and returns the node that is its result
\item |apply x b1...bk| expects variable |x| to evaluate to a node with value |(Pm/f a1...an)|, and calls |f a1...an b1...bm| if $k>=m$.
      When $k>m$, the result of the call is recursively applied to the remaining arguments.
      When $k<m$, the function still lacks arguments, and an appropriate new |P|-node is returned.
\item |eval x| expects a pointer in variable |x| and fetches the corresponding node from the heap.
      If the node is in head normal form, that is, has a |C| or |P| tag, that node is just returned.
      If the node has an |F/f| tag, function |f| is called.
      The node is updated with the result, and that result is also returned.
      If the node is |(A x a1...an)|, the effect is the same as an |apply|.
\item |case x of ... p->e ...| expects a node in variable |x| and executes expression |e| coupled with pattern |p| that matches that node.
\end{itize}

Variables in a Grin program can in principle hold either a pointer or a node.
But all syntactic structures involving variables have specific requirements on the value of that variable.
In this sense Grin is a (implicitly) typed language: it can be statically determined which variables hold pointers, and which hold nodes.
The type requirements are as follows: the first |Var| in an |apply| and the one in a |case| should hold a node,
but the other |Var|s occurring in a |eval|, |call|, |apply|, and node payload should hold a pointer.
The |Var| in a |unit| can hold either a pointer or a node.

Likewise, the return value of each statement form is fixed:
|call|, |apply|, |eval|, and |unit n| return a node, whereas |store| returns a pointer.
The remaining expression forms |unit|, sequencing, and |case|, return whatever their sub-constructs return.

The code generator (Core to Grin transformation) takes care that the generated Grin program is type correct in this respect.
It can generate a |store| expression when a node needs to be available as a pointer,
and it can generate an |eval| expression when the node pointed to by a pointer is needed.
For a final node, |eval| just fetches the node pointed to.
For a non-final (thunk) node, |eval| first forces the thunk to a final node,
but that is a good thing to do when a node is needed.
     






\section{Dynamic handling of overloading}\label{sec.dynamic}
\subsection{Dictionaries}

The standard technique for implementing overloading in Haskell,
as described by Augustsson \cite{augustsson93impl-hask-overl},
makes use of {\em dictionaries}.
A dictionary basically contains the implementations for a particular instance
of the functions specified in a class.
Calling an overloaded function then amounts to selecting one of the functions from the dictionary,
and subsequently applying it to its arguments.

Function names are not storable values in Grin,
but we can store a node that represents a partially parameterized function.
As an example, consider the instance declaration in Haskell that
defines both member functions of |Eq| for the |Int| type:
\begin{code}
instance Eq Int where
  eq x y  =  eqPrimInt x y
  ne x y  =  not (eqPrimInt x y)
\end{code}
A Grin representation of the dictionary corresponding to this declaration is a node,
using the dictionary name as a constructor, and having
two pointers corresponding to the two function definitions as its payload:
The two pointers point to nodes denoting partial parameterizations
of functions: they have `already' received 0 arguments,
and are still lacking 2 arguments:
\begin{code}
eqIntP     <- store (P2/eqPrimInt)
neIntP     <- store (P2/neInt)
dictEqInt  <- store (C/Eq eqIntP neIntP)
\end{code}
As the definition of |eq| for |Int| is just a call to |eqPrimInt|, 
we can use |(P2/eqPrimInt)| for the first pointer.
But the definition of |ne| for |Int| is not just a renaming of an existing function.
It therefore gives rise to a function binding in Grin:
\begin{code}
neInt x y =
{  store (F/eqPrimInt x y) ; \r ->
   call not r
}
\end{code}
The second member of the dictionary points to a |P2|-node for this function.

The class declaration in Haskell, which only contains signatures for the member functions,
nevertheless gives rise to generated Grin code: one function binding for each member function.
These functions take a single dictionary argument, and select the appropriate field from that dictionary.
The selector functions for the |Eq| class are:
\begin{code}
eq d =
{  eval d ; \t ->
   case t of
    (C/Eq p q) -> {eval p}
}
ne d =
{  eval d ; \t ->
   case t of
    (C/Eq p q) -> {eval q}
}
\end{code}
Finally, we describe the code that is generated for the call in Haskell of a class member function, as in the example
\begin{code}
test1 = eq 3 4
\end{code}
Grin code for this example does the call to the member function in two steps.
First the selector function is called on the dictionary corresponding to the type (|Int| in the example) of the arguments.
This is supposed to return a |P2|-tagged node,
which is subsequently applied to the arguments by |apply|.
\begin{code}
test1 =
{  store (C/Int 3)     ; \x ->
   store (C/Int 4)     ; \y ->
   call eq dictEqInt   ; \p ->
   apply p x y
}
\end{code}
Overloaded function that are not declared in the class, such as the Haskell function
\begin{code}
elem :: Eq a => a -> [a] -> Bool
\end{code}
are implemented as a Grin function that takes an additional dictionary argument.



\subsection{Default definitions}

Now suppose that we are compiling a class with a default definition, for example
\begin{code}
class Eq a where
  eq  ::  a -> a -> Bool
  ne  ::  a -> a -> Bool
  ne x y   =  not (eq x y)
\end{code}
Clearly, the compiler needs to generate code for the default function.
It can't be named |ne|, as that name is already used for the selector function,
so let's name it |neDef|.
In the body of |neDef| we need to call |eq|. This process, as in the |test1| example above,
takes two steps: first select the function from the dictionary, then apply it to its arguments.

The dictionary where the member functions can be found needs to be passed as an additional argument to |neDef|.
Essentially, default functions are implemented similarly to overloaded functions outside a class, such as |elem|.
So, although the Haskell default definition for |ne| has 2 parameters,
its Grin implementation has 3:
\begin{code}
neDef d x y =
{  store (F/eq d)   ; \f ->
   store (A f x y)  ; \r ->
   call not r
}
\end{code}
Note that the 2-stage calling process is disguised here in its lazy form.
Instead of a |call| and an |apply| as in the |test1| example, 
we store a deferred call to the selector by means of an |F|-node, 
and a deferred apply by means of an |A|-node.
Whether or not these thunk-nodes are ever evaluated is decided inside the |not| function.
Actually, |not| indeed does evaluate its argument, but without a strictness analysis
we can't predict that when compiling |neDef|.

Inside a dictionary for class |Eq|, we need two |P2|-nodes, as |eq| and |ne| in Haskell have 2 arguments.
We would get something like:
\begin{code}
eqIntP     <- store (P2/eqPrimInt)
neIntP     <- store (P2/neDef dictEqInt)
dictEqInt  <- store (C/Eq eqIntP neIntP)
\end{code}
The dictionary is to be populated with pointers to |P|-nodes, 
that refer to functions which are a mixture from the instance declaration (|eqIntP| in the example)
and default definitions from the class declaration (|neIntP|).
Note that |neIntP| and |dictEqInt| are referring to each other, which is allowed in Grin.

In a setting where compilation is done for separate modules, it is not possible
to generate the dictionary at compile time.
Class and instance declarations can reside in separate modules,
and thus the code generated for the class declaration is not available for inspection
by the compiler at the moment that the |dictEqInt| binding is generated.

Instead of having the compiler emit the binding
\begin{code}
dictEqInt  <- store (C/Eq eqIntP neIntP)
\end{code}
directly, the compiler generates code that can construct the dictionary at run time,
at the first occasion that it is used.


\subsection{Dynamic generation of dictionaries -- first attempt}

Instead of the explicit construction of the dictionary by the compiler,
the compiler generates a nullary function that can construct it on demand.
The dictionary variable is bound to a thunk for that function, thus
triggering the construction when the dictionary is first used.
\begin{code}
makeEqInt =
{  -- construct P2-nodes for both dictionary fields
}
dictEqInt <- store (F/makeEqInt)
\end{code}
By the nature of the evaluation/updating mechanism, from the moment 
that |dictEqInt| has been evaluated for the first time, it will bound to the final |C/Eq|-node.

For construction the dictionary, we need information from the class declaration.
The only way of communication with another mdule is call functions or evaluate global bindings.
We arrange that the class module, for this purpose, defines a global binding to a dictionary that contains fields for the default functions,
and |undefined| elsewhere.
\begin{code}
neDefP     <-  store (P3/neDef)
dictEqDef  <-  store (C/Eq undefined neDefP)
\end{code}
Note that this `default dictionary' stores |P3|-nodes, not |P2|-nodes as we need in the instance dictionaries.

Now the task of constructing the dictionary for the instance is performed by fetching the default dictionary,
and applying the |P3|-node found there to the dictionary it needs.
That is: the dictionary that we are about to build; lazy evaluation allows this seemingly circular definition.
Applying a |P3|-node to one further argument results in a |P2|-node, which
is stored in the dictionary, along with a |P2|-node for the other function:
\begin{code}
makeEqInt =
{  eval dictEqDef ; \d'
   case d' of
      (C/Eq _ q) ->
      {  eval q                ; \q'
         apply q' dictEqInt    ; \q''
         store (P2/eqPrimInt)  ; \p
         unit (C/Eq p q'')
      }
}
\end{code}


\subsection{Dynamic generation of dictionaries}

Although the idea in the previous subsection works in simple situations, such as the |Eq| example,
it does not work in general.
The flaw is that default definitions not always take the additional dictionary parameter as suggested in the first attempt.
Situations where that assumption doesn't hold are:
\begin{itize}
\item the default definition does not use the other members.
      The Core code generator only introduces additional parameters when they are really necessary,
      so here we get a default definition without dictionary parameter.
\item the default definition does use other fields, but only from superclasses (see section~\ref{sec.superclass}).
      The Core code generator optimizes for this situation by having the function take the dictionary for the superclass directly.
\end{itize}
In both situations the expression |apply q' dictEqInt| in the first attempt applies |q'| wrongly.

To solve this problem, we take an approach inspired by Fax\'en \cite{faxen2002}.
His endeavour was to give a formal definition of the semantics of Haskell,
but actually the construction used there works quite well in a practical setting.

The idea is that the responsibility for applying the default dictionary 
to the (thunk for the) final result is shifted from the instance module to the class module.
In this way the class module, that knows about the expectations of the default definitions,
can decide to apply the default definition to the final result, not to do that if it is not needed,
or rather use the superclass when needed.

For this purpose, the creation of the default dictionary is now made dynamic as well,
and parameterized by the instance that it is needed in:
\begin{code}
makeEqDef d =
{  store (P2/neDef d)     ; \q ->
   unit (C/Eq bottom q)
}
\end{code}
In this example, in the first line it was decided to apply the definition to the final dictionary after all,
but here the compiler has freedom to act differently if the desires of |neDef| would have been different.

In this new arrangement, we revise the definition of |makeEqInt| such that, 
instead of fetching |eqDef|, 
it calls our new dynamic generator |makeEqDef|:
\begin{code}
makeEqInt =
{  call genEqDef dictEqInt ; \d'
   case d' of
      (C/Eq _ q) ->
      {  store (P2/eqPrimInt)  ; \p
         unit (C/Eq p q)
      }
}
\end{code}

% We define a function |genEqInt| that is supposed to generate the dictionary for |Eq Int|,
% but which expects as parameter its own solution.
% A parameterless function |fixEqInt| calls the generator,
% where it passes the global dictionary variable as an argument.
% That global variable is initialized with a deferred version of |fixEqInt|.
% This is just lazy enough to actually construct the fixed point of the generator. 
% Here is the code:
% \begin{code}
% genEqInt d =
% {  -- generate a dictionary, given a pointer to its solution
% }
% fixEqInt =
% {  call genEqInt dictEqInt
% }
% dictEqInt <- store (F/fixEqInt)
% \end{code}
% As for the dictionary generation function:
% it makes use of a generation function |genEq| for a dictionary containing only the default definitions,
% which is parameterized by the final solution as well.
% \begin{code}
% genEq d =
% {  store (P2/neDef d)     ; \q ->
%    unit (C/Eq bottom q)
% }
%\end{code}

% Core code for dictionary instance
% \begin{code}
% dictEqInt =
%   letrec  {  fixEqInt =
%                let  gen =
%                       letstrict  d = genEq fixEqInt
%                       in         case d of
%                                    (Eq f g) -> (Eq f eqInt)
%                in   gen
%           ;  eqInt =
%                eqPrimInt
%           }
%   in fixEqInt
% \end{code}


\section{Static handling of overloading}\label{sec.static}


When it is possible to inspect and transform the program as a whole,
we can fully eliminate the run-time overhead of manipulating dictionaries.
The idea was first described by Jones \cite{jones1995a}.
We take a radical transformational approach,
describing the necessary steps as separate program transformation steps.
Thus we adhere to our philosophy of preferring a large number of easy transformations,
over a small number of complicated ones.

The most important transformations are:
\begin{itize}
\item |mergeInstance|: 
      for each instance declarations, merge the definitions found there with the default definitions in the class declaration.
\item |selectMember|:
      statically rather than dynamically select members from a dictionary
\item |specConst|:
      specialize functions that are called with a constant argument.
      This is a general technique that can also transform an expression like |plus x 1| to |succ x|,
      where |succ| is a specialized version of |plus| with constant argument |1|.
      Here we use it to specialize overloaded functions that are called with a constant dictionary.
\end{itize}
The opportunities for applying these transformations are prepared by some more transformations:
\begin{itize}
\item |evalKnown|: simplify uses of |eval x| in a situation where the value of |x| happens to be statically known
\item |applyKnown|: simplify uses of |apply p x| in a situation where the value of |p| happens to be statically known
\item |dropUnused|: remove bindings to (local and global) variables that are never used
\item |dropUnreachable|: remove bindings to global variables and functions that are not reachable from |main|
\end{itize}





\subsection{The |mergeInstance| transformation}

In a whole-program analysis setting, it is possible to merge functions defined in an instance
with the default definitions from the class statically.
However, the (EH to Core) code generator generates Core code without having a particular back end in mind.
So we are confronted with the code as described in the previous section for dynamic dictionary construction,
and the first task is to turn that in a static construction.

So the goal is to replace
\begin{code}
dictEqInt <- store (F/makeEqInt)
\end{code}
back to
\begin{code}
dictEqInt  <- store (C/Eq eqIntP neIntP)
\end{code}
and generate appropriate member fields:
\begin{code}
eqIntP     <- store (P2/eqPrimInt)
neIntP     <- store (P2/neDef dictEqInt)
\end{code}

This is hard for two reasons:
it is hard to see that |dictEqInt| is indeed a dictionary:
it would involve deconstructing the thunk, 
% statically doing the evaluation of |fixEqInt| which leads us to |makeEqInt|,
inspecting the code of |makeEqInt|
and see that it has the form that we can recognize as the generated code for instance declarations.
Once we found that, we should carefully deconstruct |makeEqInt| for finding the information needed in the dictionary,
including the reference to |makeEqDef| which is to be deconstructed as well.

Although this approach is possible in principle, it feels like reversely engineering
the outcome of all the transformations that were responsible of generating this Grin definitions.
Apart from being tricky, the procedure is bound to break whenever we would make changes in the Core to Grin transformation pipeline.

Instead, we take a different approach, by making manifest the intention of some of the generated function definitions,
by means of annotations.
The price is that we need to extend both the Core and the Grin language to facilitate such annotations.
\begin{itize}
\item the definition of |makeEqInt| is annotated with a marker |dictinst|: `this constructs a dictionary corresponding to a instance declaration';
\item the definition of |makeEqDef| is annotated with a marker |dictclass|: `this is a dictionary generator corresponding to a the default definitions in a class declaration';
\item the definition of |neDef| is annotated with a marker |overloaded|: `this is an overloaded default definition with specific additional argument desiderata'.
\end{itize}
Apart from the marker we embed in the annotation all information relevant for the dictionaries:
\begin{itize}
\item for |dictinst|,  we need the name of the tag of the dictionary, the name of the dictionary constructor for the default definitions, and all the names of the members defined.
\item for |dictclass|, we need the names of all default definitions.
\item for |overloaded|, we need a description of the dictionary it needs (and whether it needs one at all).
\end{itize}
In our example, the relevant annotations are:
\begin{code}
makeEqInt =
  dictinst(Eq genEq (eqPrimInt _))
{ ... }
makeEqDef d =
  dictclass (_ neDef)
{ ... }
neDef d x y =
  overloaded ( () )
{ ... }
\end{code}
There are empty positions in the name lists when functions are not defined in the instance or class declarations.
The annotation of the overloaded function indicates that this function simply needs the dictionary for the whole class.

These annotations can be easily inserted when generating Core, because all this information is needed anyway
for generating the Core definitions.
The annotations are propagated unchanged through all Core and Grin transformations, so that we have them available
when we need them: in the |mergeInstance| transformation.



\subsection{The |selectMember| transformation}\label{subsec.selectMember}

Dictionaries are passed as additional arguments to overloaded functions.
In their body, they can be passed to other overloaded functions, 
but in the end dictionaries are only used for one purpose:
selecting a member function from them.

An example is a |test1| which we introduced earlier:
\begin{code}
test1 =
{  store (C/Int 3)     ; \x ->
   store (C/Int 4)     ; \y ->
   call eq dictEqInt   ; \p ->
   apply p x y
\end{code}
The third line selects a field from the dictionary by calling the |eq| selector;
the resulting function is subsequently applied to its arguments.

Now that the previous transformation has made all dictionaries statically available,
we can now proceed by selecting the member fields statically.
This is what the |selectMember| transformation does:
it scans the Grin program for expressions of the form |call s d|
where |s| is a selector function and |d| is a dictionary.

In the example, the selector is |eq|, which is a Grin function defined by
\begin{code}
eq d =
{  eval d ; \t ->
   case t of
    (C/Eq f _) -> {eval f}
}
\end{code}
It is recognized as a selector, because its definition is a two-line function where the second line evaluates one of the members of a dictionary.
We actually hunt the program for selectors, that is functions that have this very structure.
(Again this is a form of reverse engineering;
another approach would be to explicitly annotate selector functions as such at the moment they are generated,
in a similar fashion as we use |dictinst| and |dictclass| annotations to avoid hunting for complex patterns).

In the example, the dictionary is |dictEqInt|, which is a global variable that at this time
(after the |mergeInstance| transformation) is defined as
\begin{code}
dictEqInt  <- store  (C/Eq eqEqInt neEqInt)
\end{code}
Now that the selector and the dictionary are identified, the call can be performed statically:
the expression |call eq dictEqInt| is replaced by |eval eqEqInt|.

Our example ends up as transformed to:
\begin{code}
test1 =
{  store (C/Int 3)  ; \x ->
   store (C/Int 4)  ; \y ->
   eval eqEqInt     ; \p ->
   apply p x y
\end{code}


\subsection{The |evalKnown| transformation}

At this point in the pipeline of transformations it is useful to perform 
two transformations that simplify the Grin program based on variables of which the value may be known in a particular context.
There are two such transformations: |evalKnown| and |applyKnown|.

An |eval| expression occurs in Grin code when it is needed to force a variable to head normal form
and fetch its value from the heap.
A typical occurrence is in the body of a function, where the unknown value of the argument needs to be forced and fetched
in order to be scrutinized:
\begin{code}
f x =
{  eval x   ; \v ->
   case v of ...
\end{code}
Sometimes, a Grin program evaluates a variable of which the value {\em is} known.
We can encounter expressions like:
\begin{code}
   store (C/Int 5)  ; \n ->
   eval n
\end{code}
This is equivalent to a shorter expression:
\begin{code}
   unit (C/Int 5)
\end{code}
which is more efficient since it avoids storing a node on the heap, and executing the |eval| operation to fetch it back.

So is the Grin code generator to blame for emitting inefficient code like the example above?
Not really, because the value of the variable could have become known only later, as a result of transformation of Grin code.
For example, if the |inline| transformation decides to inline the call to |f| in
\begin{code}
   store (C/Int 5)  ; \n ->
   call f n
\end{code}
we end up with the inefficient |store|-|eval| combination.

To compensate for this, we have a transformation |evalKnown| that hunts for |eval x|
where |x| has a known value, either because it is a global variable or it is the target of an earlier |store|.
This transformation catches situations like the example above.
Since this transformation is carried out anyway, the Grin code generator
can afford itself to generate |store|-|eval| pairs on occasion.
This makes the code generator simpler: it can be defined compositionally, without having to 
bother to avoid |store|-|eval| pairs.

The |evalKnown| transformation symbolically collects all (local and global)
|store|s, and for each |eval x| checks whether the variable |x| has a known value.
There are three cases:
\begin{itize}
\item |x| is a global or local variable used to store a value |v| in head normal form, that is a node with a |C| or |P| tag.
      Then |eval x| can be replaced by |unit v|.
\item |x| is a local variable used to store a thunk with an |F| tag, that is a node |(F/f a1...an)|.
      When |x| is used only once,
      then |eval x| can be replaced by |call f a1...an|
\item |x| is a local variable used to store a thunk with an |A| tag, that is a node |(A f a1...an)|.
      When |x| is used only once,
      then |eval x| can be replaced by |call app9 f a1...an|,
      where |app9| is a compiler-generated function
\begin{code}
app9 p a1...an =
{  eval p  ; \p' ->
   apply p' a1...an
}
\end{code}
\end{itize}

The reason that we bring up this whole story, is that the previous |selectMember| transformation
generates opportunities for |evalKnown|. 
Remember that it has replaced |call eq dictEqInt| by |eval eqEqInt|.
This is an opportunity for the |evalKnown| transformation, since |eqEqInt| is a global variable bound to |(P2/eqPrimInt)|.
Thus |eval eqEqInt| is transformed to |unit (P2/eqPrimInt)|.

Our example thus now is transformed to:
\begin{code}
test1 =
{  store (C/Int 3)      ; \x ->
   store (C/Int 4)      ; \y ->
   unit (P2/eqPrimInt)  ; \p ->
   apply p x y
}
\end{code}


\subsection{The |applyKnown| transformation}

The |apply| operation expects a value that represents a partially applied function, and applies it to further arguments.
Normally this operation is generated by the code generator for values that are not statically known,
for example when emitting code for polymorphic functions such as |map|.

But similar to the previous subsection, where |eval| is occasionally used on variables with a statically known value,
situations can occur where |apply| is used on values that are statically known.
In fact, this happens in the example outcome of the previous transformation:
we have an |apply| operating on a value that obviously is |(P2/eqPrimInt)|,
a partial application of |eqPrimInt| lacking 2 parameters.

Since in this example the lacking 2 parameters are provided as part of the |apply| operation,
the call is thereby saturated and equivalent to |call eqPrimInt x y|.

This is exactly what the |evalKnown| transformation performs: it symbolically collects all
|unit|s, and for each |apply x b1...bk| checks whether the variable |x| holds a known node |(Pm/f a1...an)|.
There are three cases:
\begin{itize}
\item |n<m| (undersaturated call): replace the |apply| operation by |unit (Pnm/f a1...an b1...bk)|
\item |n=m| (saturated call): replace the |apply| operation by\\ |call f a1...an b1...bk|
\item |n>m| (oversaturated call): do nothing. (This situation does not occur in the context discussed in this paper. If it does occur in other situations, doing nothing is always safe.)
\end{itize}

Our example ends up as:
\begin{code}
test1 =
{  store (C/Int 3)      ; \x ->
   store (C/Int 4)      ; \y ->
   unit (P2/eqPrimInt)  ; \p ->
   call eqPrimInt x y
}
\end{code}
Note that the binding of the |P2|-node to |p| is now obsolete. 
However, it is not removed by the |applyKnown| transformation, 
as it will be caught anyway by a |dropUnused| transformation performed further downstream.
This way, we keep the individual transformations straightforward, 
while they together still do all that is needed.



\subsection{The |specConst| transformation}

The combined effort of all transformations so far has succeeded to annihilate 
all dictionary overhead involved in the Haskell expression |eq 3 4|.
But now let's see what happens for the Haskell expression |ne 5 6|.

Because the instance declaration |Eq Int| relies on the default definition for |ne|,
the resulting Grin code is different now. 
The field selection and subsequent |apply| is shortcut successfully,
but the default definition |neDef| that is now called is overloaded,
and thus needs an additional dictionary parameter itself.
\begin{code}
test2 =
{  store (C/Int 5)      ; \x ->
   store (C/Int 6)      ; \y ->
   call neDef dictEqInt x y
}
\end{code}
So we still have overhead for run-time dictionary passing here.

To overcome this, we rely on a technique that is actually more general:
make specialized `clones' of a function that is called with a constant argument.
Function |neDef| has three parameters, but it is called here with constant arguments:
a dictionary, that (since the |selectMember| transformation is performed) is defined as a global constant:
\begin{code}
dictEqInt  <- store  (C/Eq eqEqInt neEqInt)
\end{code}
In this particular example the other two arguments are constants as well,
so the function will end up to be specialized for all three arguments,
but in a typical situation only the dictionary is constant, and we get a specialized copy still expecting two parameters.

For the purpose of explaining, we assume here that the function is only specialized for its dictionary argument
(imagine an option to be active that forbids specializing for integer arguments).
The definition of |neDef| is taken:
\begin{code}
neDef d x y =
{  store (F/eq d)   ; \f ->
   store (A f x y)  ; \r ->
   call not r
}
\end{code}
and a clone of it is generated, which is specialized for the first argument:
\begin{code}
neDef' x y =
  specialized (neDef (dictEqInt _ _))
{  store (F/eq dictEqInt)  ; \f ->
   store (A f x y)         ; \r ->
   call not r
}
\end{code}
Note that the clone is annotated in such a way that it is manifest
what was the original function, and for which arguments it was specialized.
This way, when the transformation is run again later, we can avoid making another clone for the same argument.

The call is adapted accordingly:
\begin{code}
test2 =
{  store (C/Int 5)      ; \x ->
   store (C/Int 6)      ; \y ->
   call neDef' x y
}
\end{code}


\subsection{\dots and repeat}


After the specialization, new opportunities are exposed for transformations that we discussed earlier.
Firstly, the operation\\
|store (F/eq dictEqInt)| is an opportunity for the |selectMember| transformation.
In subsection~\ref{subsec.selectMember} we described that 
for all selectors |s| that select field |i| from a dictionary,
and all constant dictionaries |d| having |f| as it's |i|th field,
it replaces |call s d| by |eval f|.

We now widen the task of |selectMember| to also handle selections from constant dictionaries 
that are disguised as thunk. That is: 
replace |store (F/s d)| by |unit f|.

This way, we lose the lazy behavior of the field selection.
But there is no need for laziness in this situation:
field selection cannot fail, and it is performed fast -- in fact, 
now that we perform it statically, it takes no time at all.
Nobody would oppose not postponing a call that takes zero time and cannot fail.

Another d\'eja vu: the |unit eqEqInt| that emerges from the previous transformation
can be combined with the thunkified |apply| in |store (A f x y)|.
This is done by the |applyKnown|, whose task is also widened to handle situations
where an |apply| is disguised as a thunk with |A| tag.

Next, we may have new opportunities for another |specConst| transformation, etcetera.
All in all, the following sequence of transformations should be performed repeatedly:
|selectMember|, |evalKnown|, |applyKnown|, and |specConst|.

New opportunities will appear as often as overloaded functions keep calling each other,
requiring as many iterations as the static nesting depth of overloaded functions.
If we want to be sure that all dictionary-passing is removed, we should
iterate the four transformations until none of the four does replacements.
In practice, a fixed number of iterations could satisfy as well.

In real-life example programs involving the complicated classes from
the numeric, IO, and read/show libraries, we observed the transformation of a program to converge
to a fixed point after about 5 iterations.




\section{Super classes and contexts}\label{sec.superclass}

Even more parameters

\section{Implementation}

With AG's

\section{Conclusion}

Transformational approach enables us to concentrate on simple transformations,
that can be composed to perform complex tasks.

Related work:
w.r.t. tackling overloading: GHC stores info in hi files, Fax\'en  gives formal description.
Jones does static evaluation in a core-like language; we do it as late as possible in a lower level language.

w.r.t. transformational approach: Boquist used transformations in a Haskell compiler, 
but without tackling overloading

w.r.t. transformational approach in general: Stratego, ASF/SDF and many others

w.r.t. AG-approach: Jastadd.



\begin{thebibliography}{}

\bibitem[Augustsson 1993]{augustsson93impl-hask-overl}
Lennart Augustsson.
Implementing Haskell overloading.
In: {\em Functional Programming and Computer Architecture} FPCA 1993, pp~65--73.


\bibitem[Bernardy et al 2009]{bernardy2009}
Jean-Philippe Bernardy et al.
A comparison of C++ concepts and Haskell type classes.
In: {ACM workshop on generic programming} GP 2009, pp.~37--48.


\bibitem[Boquist and Johnsson 1996]{boquist1996}
Urban Boquist and Thomas Johnsson.
The GRIN project: A highly optimising back end for lazy functional languages.
In: {\em Workshop on Implementation of Functional Languages} IFL 1996.
Springer LNCS 1268. 

\bibitem[Boquist 1999]{boquist99phd-optim-lazy}
Urban Boquist.
{\em Code optimisation techniques for lazy functional languages}.
PhD Thesis Chalmers University, G\"oteborg March 1999.

\bibitem[Dijkstra 2005]{dijkstra2005}
Atze Dijkstra.
{\em Stepping through Haskell}.
PhD Thesis Utrecht University, November 2005.

\bibitem[Fax\'en 2002]{faxen2002}
Karl-Filip Fax\'en.
A static semantics for Haskell.
{\em J.~Functional Programming} {\bf 12} (2002), pp.~295--357.


\bibitem[Jones 1995a]{jones1995a}
Mark. P.\ Jones.
Dictionary-free overloading by partial evaluation.
{\em Lisp and symbolic computation} {\bf 8} (1995), pp.~229--248.


\bibitem[Jones 1995b]{jones1995b}
Mark. P.\ Jones.
Functional programming with overloading and higher-order polymorphism.
In: {\em Advanced Functional Programming} AFP 1995, pp.~97--136.
Springer LNCS 925.

\end{thebibliography}

\end{document}
