%%[abstract
In \thispaper\ we describe the structure of the Essential Haskell Compiler (EHC)
and how we manage its complexity,
despite its growth beyond the essentials towards a full Haskell compiler.
Our approach partitions both language and its implementation into smaller, manageable steps,
and uses compiler domain specific tools to generate parts of the compiler from higher level descriptions.
%%]

%%[abstractExpReport
Haskell compilers are complex programs.
Testimony to this observation is the Glasgow Haskell compiler (GHC),
which simultaneously incorporates many novel features, is used as a reliable workhorse for many a
functional programmer, and offers a research platform for language designers.
As a result, modifying GHC requires a steep learning curve for getting acquainted 
with GHC's internals.
In this experience report we describe the structure of the Essential Haskell Compiler (EHC)
and how we manage complexity,
despite its growth beyond the essentials towards a full Haskell compiler.
Our approach partitions both language and its implementation into smaller, manageable steps,
and uses compiler domain specific tools to generate parts of the compiler from higher level descriptions.
As major parts of EHC have been written in Haskell both its implementation and use are reported about.
%%]

%%[introduction
Haskell is a perfect example of a programming language which offers many features improving programming efficiency by
offering a sophisticated type system.
As such it is an answer for the programmer looking for a programming language which does as much as possible of the programmer's job,
while at the same time guaranteeing program properties like ``well-typed programs don't crash''.
However, the consequence is that a programming language implementation is burdened by these responsibilities,
and becomes complex as a result.
Haskell thus also is a perfect example of a programming language for which its compilers are complex.
Testimony to this observation is the Glasgow Haskell compiler (GHC),
which simultaneously incorporates many novel features, is used as a reliable workhorse for many a
functional programmer, and offers a research platform for language designers.
As a result, modifying GHC requires a steep learning curve for getting acquainted 
with GHC's internals.

In \thispaper\ we show how we deal with this compiler complexity
in the context of the Essential Haskell (EH) Compiler (EHC)
\cite{dijkstra04ehc-web,dijkstra05phd},
and in particular the following issues:
\begin{Itemize}
\item \textbf{Implementation complexity} (\secRef{sec-ehcstruct-complexity-implementation})
  The amount of work a compiler has to do is a source of complexity.
  We use a dataflow through many relatively small transformations, between various internal representations.
\item \textbf{Description complexity} (\secRef{sec-ehcstruct-complexity-description})
  The specification of parts of the implementation itself can become complex because low-level details
  are visible.
  We use domain specific languages which factor out those low-level details which can be dealt with automatically.
\item \textbf{Design complexity} (\secRef{sec-ehcstruct-complexity-design})
  Language features usually are experimented with in isolation.
  We describe their implementation in isolation, as a sequence of language variants, building on top
  of each other.
\item \textbf{Maintenance complexity} (\secRef{sec-ehcstruct-complexity-maintenance})
  Actual compiler source, its documentation and specification tend to become inconsistent over time.
  We deal with inconsistencies by avoiding their main cause: duplication.
  Whenever two artefacts have to be consistent, we generate these from a common description.
\end{Itemize}
%%]

%%[complexityDesign
To cope with the many features of Haskell,
EHC is designed as a sequence of compilers,
each of which adds new features.
This enables us to experiment with
non-standard features.
\figRef{fig-ehcstruct-alllangvariant} shows the language variants we currently recognize,
with the standard and experimental features they introduce.
The subdivision reflects a didactical choice
of increasingly complex features; it is not
the development history of the project.
Every compiler in the series can actually be built out of the repository.

For each compiler in the series, various artefacts
are created:
a definition of the semantics, an implementation,
example programs, documentation, etcetera.
\figRef{fig-ehcstruct-langs-shadowed} shows some instances.
The language in the first column introduces the starting point
for subsequent languages.
It features the simply typed |lambda|-calculus, where all defined values require an accompanying type signature,
demonstrated by the example.
For the description of the corresponding compiler we require the static semantics of all language constructs,
their implementation and documentation.
The semantics is defined in terms of type rules and the implementation in terms of attribute grammars.

\begin{CenterFigure}{t}{EH language variants (work in progress marked by an asterisk)}{fig-ehcstruct-alllangvariant}
%%@SlidesIntro.ehVariantsTableLong
\end{CenterFigure}

{
%%@Poster.exportedMacros
%%@Poster.exportedTikZMacros

\newcommand{\Node}[2]{node[fill=white,text=black,anchor=south west,shape=rectangle,rounded corners,draw] {#2}}
\newcommand{\PNode}[2]{\Node{#1}{\parbox{#1cm}{#2}}}
\newcommand{\PCNode}[2]{\PNode{#1}{{\begin{center}#2\end{center}}}}

\begin{figure*}[t]
\begin{tikzpicture}

%\draw[step=1cm,blue,very thin] (0,0) grid (13,13);

%% Titles
\draw (0,13) node[anchor=north west] {\textbf{\(\downarrow\) Simply typed |lambda| calculus}};
\draw (2,13.5) node[anchor=north west] {\textbf{\(\downarrow\) Polymorphic type inference}};
\draw (4,14) node[anchor=north west] {\textbf{\(\downarrow\) Higher ranked types)}};

\draw (1,3.25) node[anchor=east] {\textbf{Implementation:}};
\draw (1,8.25) node[anchor=east] {\textbf{Semantics:}};
\draw (1,11.25) node[anchor=east] {\textbf{Example:}};

%% Examples
\draw (4,10) \PNode{6.5}{%
%%@Poster.langSeriesEx3
};

\draw (2,9.5) \PNode{5.5}{%
%%@Poster.langSeriesEx2
};

\draw (0,9) \PNode{3.5}{%
%%@Poster.langSeriesEx1
};

%% Semantics
\draw (4,6.5) \PNode{6.5}{%
%%@Poster.langSeriesSem3
};

\draw (2,6) \PNode{5.5}{%
%%@Poster.langSeriesSem2
};

\draw (0,5.5) \PNode{3.5}{%
%%@Poster.langSeriesSem1
};

%% Implementation
\draw (4,1) \PNode{8}{%
{\small
%%@Poster.langSeriesImpl3
}
};

\draw (2,0.5) \PNode{8.5}{%
{\small
%%@Poster.langSeriesImpl2
}
};

\draw (0,0) \PNode{7.5}{%
{\small
%%@Poster.langSeriesImpl1
}
};

\end{tikzpicture}
\caption{Languages design steps}
\label{fig-ehcstruct-langs-shadowed}
\end{figure*}

}

The next two columns in \figRef{fig-ehcstruct-langs-shadowed} show the incorporation of polymorphic type inference
and higher ranked types, respectively.

The tool architecture was designed in such a way that the description
for each language variant |n| consists of the delta with respect to language |(n-1)|.
Usually this delta is a pure addition, but there is interaction between subsequent variants when:

\begin{Itemize}
\item Language features interact.
\item The overall implementation and individual increments interact: an increment is described
  in the context of the implementation of preceding variants,
  whereas such a context must anticipate later changes.
\end{Itemize}

Conventional compiler building tools are neither aware of partitioning into increments nor their interaction.
We use a separate tool, called |Shuffle|, to take care of such issues.
We describe |Shuffle| in the next section.

%%]
EH thus consists of a sequence of languages, each with its own compiler.
Each language variant is described, and implemented, as an increment on top of the previous variant.
Each increment describes one language feature, or a group of related language features.
Subsequent increments ideally are independent,
but in practice interact because:

%%[complexityMaintenance
For any large programming project the greatest challenge is not to make the first version,
but to be able to make subsequent versions.
One must be prepared for change, and in order to facilitate change, the object of change should
be isolated and encapsulated.
Although many programming languages support encapsulation,
this is not enough for the construction of a compiler,
because one language feature influences not only different parts of a compiler
(parser, structure of abstract syntax tree, type system, code generation, runtime system)
but also other artefacts such as specification, documentation and test suites.
Encapsulation of a language feature in a compiler therefore is difficult, if not impossible, to achieve.

We mitigate the above problems by using |Shuffle|,
a separate preprocessor.
In all source files, we annotate to which
language variants the text is relevant.
|Shuffle| preprocesses all source files by selecting
and reordering those fragments (called |chunk|s)
that are needed for a particular language variant.
Source files can be (chunked) Haskell code,
(chunked) \LaTeX\ text, but also code in other
languages we use (see \figRef{fig-ehcstruct-toolchain}).

|Shuffle| behaves similar to literate programming \cite{knuth92litprog} tools
in that it generates program source code.
The key difference is that with the literate programming style program source code is generated out of a file containing
program text plus documentation,
whereas |Shuffle| combines chunks for different variants from different files into either program source code or documentation.

|Shuffle| offers a different functionality than version management: version management offers historical versions,
whereas |Shuffle| offers the simultaneous handling of different variants from one source.

For example, for language variant |2| and |3| (on top of |2|) a different Haskell wrapper function |mkTyVar| for
the construction of the compiler internal representation of a type variable is required.
In variant |2|, |mkTyVar| is equivalent to the constructor |Ty_Var|:

%%@EHTy.2.mkTyVar wrap=code

However, version |3| introduces polymorphism as a language variant,
which requires additional information for a type variable, which defaults to |TyVarCateg_Plain|
(we do not further explain this):

%%@EHTy.3.mkTyVar wrap=code

These two Haskell fragments are generated from the following |Shuffle| description:

%%[[wrap=tt
%%%[2.mkTyVar
%%@EHTy.2.mkTyVar
%%%]

%%%[3.mkTyVar -2.mkTyVar
%%@EHTy.3.mkTyVar
%%%]
%%]]

The notation @%%[2.mkTyVar@ begins a chunk for variant |2| with name |mkTyVar|, ended by @%%]@.
The chunk for @3.mkTyVar@ explicitly specifies to override @2.mkTyVar@ for variant |3|.
Although the type signature can be factored out, we refrain from doing so for small definitions.

In summary, |Shuffle|:
\begin{Itemize}
\item uses notation @%%[ ... %%]@ to delimit and name text chunks;
\item names chunks by a variant number and (optional) additional naming;
\item allows overriding of chunks based on their name;
\item combines chunks upto an externally specified variant, using an also externally specified variant ordering.
\end{Itemize}

%%]

%%[complexityDescription
Haskell is well suited as an implementation language for compilers, 
among others because of the ease of manipulating tree structures.
Still, if one needs to write many tree walks, especially with multiple passes
over complicated syntax trees, the necessary mutually recursive functions
tend to become hard to understand, and contain large pieces of boilerplate code.
In the implementation of EHC we therefore use a chain of preprocessing tools,
depicted in \figRef{fig-ehcstruct-toolchain}.

\begin{figure*}
\hspace*{-5mm}\raisebox{-10mm}[70mm][0mm]{\FigScaledPDF{0.45}{toolchain2}}
\caption{chain of tools used to build EHC}
\label{fig-ehcstruct-toolchain}
\end{figure*}

We use the following preprocessing tools:
\begin{Itemize}
\item \textbf{UUAGC} (Utrecht University Attribute Grammar Compiler),
which enables us to specify abstract syntax trees and tree walks over them using an attribute grammar (AG) formalism
\cite{swierstra99comb-lang,dijkstra04thag,baars04ag-www,dijkstra05phd}.
\item \textbf{Ruler},
a translator for an even more specialized language,
which enables a high-level specification of type inferencing,
generating both AG code and \LaTeX\ documentation
\cite{dijkstra06ruler}.
\item \textbf{Shuffle}, 
which deals with the compiler organisation and logistics of many different language features,
and provides a form of literate programming.
\end{Itemize}

In the rest of this section we elaborate on the rationale of UUAGC (\secRef{sec-ehcstruct-explainUUAGC}) and Ruler (\secRef{sec-ehcstruct-explainRuler}).
We illustrate their use with example code, which implements part of a Hindley-Milner type checker.
In the section on UUAGC this is idealized toy code, but the section on Ruler shows 
actual code taken from EHC for the same example case.
In \secRef{sec-ehcstruct-complexity-design} and \ref{sec-ehcstruct-complexity-maintenance} we continue with the rationale and use of Shuffle.
%%]

%%[explainUUAGC

Functional languages are famous for their ability to 
parameterize functions not only with numbers and data structures,
but also with functions and operators.
The standard textbook example involves the functions |sum| and |product|,
which can be defined separately by tedious inductive definitions:
%%[[wrap=code
sum      []      = 0
sum      (x:xs)  = x + sum xs
product  []      = 1
product  (x:xs)  = x * product xs
%%]]
This pattern can be generalized in a function |foldr|
that takes as additional parameters the base value and the operator to apply
in the inductive case:
%%[[wrap=code
foldr op e []      = e
foldr op e (x:xs)  = x `op` foldr op e xs
%%]]
Once we have this generalized function, we can partially parameterize it
to obtain simpler definitions for |sum| and |product|, and it is also useful
in various other problem domains:
%%[[wrap=code
sum        =  foldr (+)  0
product    =  foldr (*)  1
concat     =  foldr (++) []
sort       =  foldr insert []
transpose  =  foldr (zipWith (:)) (repeat [])
%%]]
The idea that underlies the definition of |foldr|, i.e.\ capturing the pattern
of an inductive definition by adding a function parameter for each constructor of
the data structure, can also be used for other data types, and even for
multiple mutually recursive data types.
Functions that can be expressed in this way were called {\em catamorphisms}
by Bird, and the collective extra parameters to |foldr|-like functions 
an {\em algebra} \cite{bird84circ-traverse,birdmoor96algebra}. 
Thus, |((+),0)| is an algebra for lists, and |((++),[])| is another.
In fact, every algebra defines a {\em semantics} of the data structure.
% When applying |foldr|-like functions to the algebra consisting of the original constructor functions,
% such as |((:),[])| for lists, we have the identity function.
% Such an algebra is said to define the `initial' semantics.

Outside circles of functional programmers and category theorists, an
algebra is simply known as a `tree walk'.
In compiler construction, algebras could be very useful to define
a semantics of a language, or bluntly said to define tree walks over the parse tree.
The fact that this is not widely done, is due to the following problems:

\begin{enumerate}
\item Unlike lists, which have a standard function |foldr|, in a compiler we deal with
      (many) custom data structures to describe the abstract syntax of a language, 
      so we have to invest in writing a custom |fold|
      function first. Moreover, whenever we change the abstract syntax,
      we need to change the |fold| function, and every algebra.
\item Generated code can be described as a semantics of the language, but often
      we need additional semantices: pretty-printed listings, warning messages,
      and various derived structures for internal use (symbol tables etc.).
      This can be done by having the semantic functions in the algebra return
      tuples, but this makes them hard to handle.
\item Data structures for abstract syntax tend to have many alternatives,
      so algebras end up to be clumsy tuples containing dozens of functions.
\item In practice, information not only flows bottom-up in the parse tree,
      but also top-down. E.g., symbol tables with global definitions need
      to be distributed to the leafs of the parse tree to be able to evaluate them.
      This can be done by making the semantic functions in the algebra
      higher order functions, but this pushes handling algebras beyond human control.
\item Much of the work is just passing values up and down the tree.
      The essence of a semantics is sparsely present in the algebra,
      and obscured by lots of boilerplate.
\end{enumerate}

Many compiler writers thus end up writing ad hoc recursive functions
instead of defining the semantics by a algebra,
or even resort to non-functional techniques.
Others succeed in giving a concise definition of a semantics,
often using proof rules of some kind, but thereby loose executability.
For the implementation they still need conventional techniques,
and the issue arises whether the program soundly implements
the specified semantics.

To save the nice idea of using an algebra for defining a semantics,
we use a preprocessor for Haskell that overcomes the abovementioned problems \cite{swierstra99comb-lang}.
It is not a separate language; we can still use Haskell for writing
auxiliary functions, and use all abstraction techniques and libraries available.
The preprocessor just allows a few additional constructs, which are translated
into a custom |fold| function and algebras.



We describe the main features of the preprocessor here, and explain why they overcome
the five problems mentioned above.
To start with, the abstract syntax of the language is defined in a |DATA| declaration,
which is like a Haskell |data| declaration with named fields.
The difference is that we don't have to write braces and commas,
and that constructor function names need not to be unique.
As an example, we show some fragments of the EH language, used in EHC to represent typed lambda calculus:
%%[[wrap=code
DATA Expr
  =  Var   ^^  ^^  name :: Name
  |  Let   ^^  ^^  decl :: Decl   ^^  ^^  ^^    body :: Expr
  |  App   ^^  ^^  func :: Expr   ^^  ^^  ^^    arg :: Expr
  |  Lam   ^^  ^^  arg  :: Pat    ^^  ^^  ^^    body :: Expr
DATA Decl
  =  Val   ^^  ^^  pat :: Pat     ^^  ^^  ^^    expr :: Expr
DATA Pat
  =  Var   ^^  ^^  name :: Name
  |  App   ^^  ^^  func :: Expr   ^^  ^^  ^^    arg :: Expr
%%]]
The preprocessor generates corresponding Haskell |data| declarations
(making the constructors unique by prepending the type name, like |Expr_Var|),
and more importantly, generates a custom |fold| function. This overcomes problem 1.

For any desired value we wish to compute over a tree, we can declare a `synthesized attribute'.
Attributes can be defined for one or more data types.
For example, we can define that for all three datatypes we wish to 
synthesize a pretty-printed listing, and that expressions
in addition synthesize a type and a variable substitution map:
%%[[wrap=code
ATTR Expr Decl Pat  SYN  listing   :: String
ATTR Expr           SYN  typ       :: Type
                         varmap    :: [(Name,Type)]
%%]]
The preprocessor will ensure that the semantic functions will return appropriate
tuples, but in our program we can simply refer to the attributes by name.
This overcomes problem 2.

The value of each attribute needs to be defined for 
every constructor of every data type which has the attribute.
As this defines the semantics of the language, these definitions
are known as `semantic rules', and start with keyword |SEM|.
An example is:
%%[[wrap=code
SEM Expr | Let
  @lhs.listing =  "let " ++ @decl.listing ++ " in " ++ @body.listing
%%]]
This states that the synthesized |listing| attribute
of a |Let| expression can be constructed
by combining the |listing| attributes of its |decl| and |body| children
and some fixed strings.
The $\!\!$|@| symbol in this context should be read as `attribute',
not to be confused with Haskell `as-patterns'.
The keyword |lhs| refers to the parent of the children $\!\!$|@decl| and $\!\!$|@body|,
i.e.\ the nameless |Expr| at the \textbf{l}eft \textbf{h}and \textbf{s}ide of
the grammar rule.
At the left of the |=| symbol, the attribute to be defined is mentioned;
at the right, any Haskell expression can be given.
The $\!\!$|@| symbol may be omitted in the destination attribute,
as is done in the next example. 
This example shows that the right hand side of a semantics definition
can use any Haskell expression form (a |case| expression in the example)
and use auxiliary functions (like |substit| in the example),
with embedded occurrences of child attributes.
Also, it shows how to use the value of terminal symbols ($\!\!$|@name| in the example),
and how to group multiple semantic rules under a single |SEM| header:
%%[[wrap=code
SEM Expr
  | Var  lhs.listing  =  @name
  | Lam  lhs.typ      =  Type_Arrow (substit @body.varmap @arg.typ) @body.typ
  | App  lhs.typ      =  case @func.typ of
                           (Type_Arrow p b) -> substit @arg.varmap b
%%]]
The preprocessor collects and orders all definitions into a single algebra,
replacing the attribute references by suitable selections from the results 
of the recursive tree walk on the children. 
This overcomes problem 3.

To be able to pass information downward during a tree walk,
we can define `inherited' attributes
(the terminology goes back to \cite{knuth68ag}).
As an example, it can serve to pass an environment,
i.e.\ a lookup table that associates variables to types,
which can be consulted when we need to determine the type of a variable:
%%[[wrap=code
ATTR Expr INH env :: [(Name,Type)]
SEM Expr 
  | Var  lhs.typ  =  fromJust (lookup @lhs.env @name)
%%]]
The value to use for the inherited attributes can be defined
in semantic rules higher up the tree.
In the example, |Let| expressions extend the environment
that they inherited themselves with new declarations,
to define the environment to be used in the body:
%%[[wrap=code
SEM Expr
  | Let  body.env  =  @decl.newenv ++ @lhs.env
%%]]
The preprocessor translates inherited attributes into
extra parameters for the semantic functions in the algebra.
This overcomes problem 4.

In practice, there are many situations where inherited attributes
are passed unchanged as inherited attributes for the children.
For example, the environment is passed unchanged at |App| and |Lam| expressions:
This can be quite tedious to do:
%%[[wrap=code
SEM Expr
  |  App  func.env  = @lhs.env
          arg.env   = @lhs.env
  |  Lam  body.env  = @lhs.env
%%]]
To make this easier, the preprocessor has a convention that, 
unless stated otherwise, attributes with the same name
are automatically copied. So, the attribute |env| that
an |App| expression inherited from its parent, is automatically copied
to the children which also inherit an |env|, and the tedious rules
above can be omitted.
A similar automated copying is done for synthesized attributes,
so if they need to be passed unchanged up the tree, this needs
not to be explicitly coded.

% When more than one child offers a candidate to be copied,
% normally the last one is taken.
% But if we wish a combination of the copy candidates
% to be used, we can specify so in the attribute declaration in a special |USE| clause.
% An example is:
% %%[[wrap=code
% ATTR Expr Decl Pat
%  SYN listing USE (++) []
% %%]]
% which specifies that by default, the synthesized
% attribute |listing| is the concatenation of the |listing|s of
% all children that have one, or the empty list if no child has one.
% This defines a useful default rule, which can be overridden
% when extra symbols need to be interspersed, as for example in
% the definition of |listing| for |Let| expressions given earlier.

It is allowed to declare both an inherited and a synthesized attribute
with the same name. In combination with the copying mechanisms,
this enables us to silently thread a value through the entire
tree, updating it when necessary. 
Such a pair of attirbutes can be declared as if it were a single `threaded' attribute.
A useful application is to thread an integer value as a source
for fresh variable names, incrementing it whenever a fresh name is 
needed during the tree walk.
This captures a pattern for which often |Reader| and
|Writer| monads are introduced \cite{jones99thih}.

The preprocessor automatically generates semantic rules
in the standard situations described, and this overcomes problem 5.
%%]

%%[explainRuler
With the AG system we can describe the part of a compiler related to treewalks concisely and efficiently.
However, this does not give us any means of looking at such an implementation in a more formal setting.
Currently a formal description of Haskell, suitable for both the generation of a implementation and use in formal proofs,
does not exist.
For EH we |Ruler|,
which allows us to have both an implementation and a type rule presentation with the guarantee that
these are mutually consistent.

With |Ruler| we describe type rules in such a way that both
\LaTeX\ rendering and AG implementation are generated from such a common type rule description.
We will demonstrate the use of |Ruler| by showing |Ruler| code for the Hindley-Milner type inferencing of
function application |App| (see previous section for this and other names for expression terms).
We omit a thorough explanation of the meaning of these fragments,
as our purpose here is to demonstrate how we can describe these fragments with
one common |Ruler| source text.
Neither do we intend to be complete in our description;
we point out those parts corresponding to
the distinguishing features of the |Ruler| system.

From a single source, to be discussed below, |Ruler| can both generate a \LaTeX\ rendering for human use
in technical writing:

\[
%%@Poster.langSeriesSem2Rule
\]

and its corresponding AG implementation, for further processing by UUAGC:

%%@Poster.langSeriesImpl2

The given rule algorithmically describes the typing of a function application in a standard lambda calculus
with the Hindley-Milner type system.
The rule involves four judgements, three premises and one conclusion.
All judgements but the judgement involving the freshness of a type variable have the same structure as these all
relate various properties of expressions: the conclusion about the function application |e1 e2|, the premises about
the function |e1| and argument |e2|.

|Ruler| exploits this commonality by means of the \IxAsDef{scheme} of a judgement,
which can be thought of as the type of a judgement:

%{
%include ruler.fmt

%%[[wrap=code
scheme expr =
    holes  [ node e: Expr, inh valGam: ValGam, inh knTy: Ty
           , thread tyVarMp: VarMp, syn ty: Ty ]
    judgeuse tex valGam ; tyVarMp.inh ; knTy :-.."e" e : ty ~> tyVarMp.syn
    judgespec valGam ; tyVarMp.inh ; knTy :- e : ty ~> tyVarMp.syn
%%]]

The scheme declaration for expressions |expr| defines
a common framework for the judgements of each |expr| term, such
as |App| and |Lam| (lambda expression):

\begin{Itemize}
\item |holes|: names, types and modifiers of placeholders (or \IxAsDef{hole}s) for the various properties,
      such as |e| and |valGam|.
\item |judgeuse tex| (unparsing): \LaTeX\ pretty printing in terms of holes and other symbols,
      such as |:-| and |~>|. 
\item |judgespec| (parsing): concrete syntax for specifying a complete judgement. 
\end{Itemize}

Modifiers |node|, |inh|, |syn|, and |thread| are mainly required for the generation of an AG implementation,
as more information is needed to turn a rule into an algorithm.
The |thread| modifier introduces two holes suffixed with |.inh| and |.syn|,
to be used by UUAGC for a threaded attribute.
For a \LaTeX\ rendering the remaining modifiers do not matter.

Additional formatting is required to map identifiers to \LaTeX\ symbols, for example: 

%%[[wrap=code
valGam          :->     Gamma
ty              :->     sigma
knTy            :->     sigmak
tyVarMp.inh     :->     VarMpk
%%]]

We omit further discussion of lexical issues.

The rule for function application |App| now is defined by judgements introduced with the keyword |judge|:

%%[[wrap=code
rule e.app =
      judge tvarvFresh
      judge expr =  tyVarMp.inh ; tyVarMp ; (tvarv -> knTy)
                    :- eFun : (ty.a -> ty) ~> tyVarMp.fun
      judge expr =  tyVarMp.fun ; valGam ; ty.a
                    :- eArg : ty.a ~> tyVarMp.arg
      -
      judge expr =  tyVarMp.inh ; valGam ; knTy
                    :- (eFun ^^ eArg) : (tyVarMp.arg ty) ~> tyVarMp.arg
%%]]

For each judgement its scheme is specified (|expr| in the example).
The |judgespec| of the corresponding scheme is used to check the concrete syntax and to bind the holes of the judgement
to the concrete values specified by the judgement.
From this rule definition a \LaTeX\ rendering is straightforwardly generated.

For the generation of a corresponding AG implementation we need information as specified by hole modifiers.
An AG implementation is tree based, that is the structure of the tree drives the choice of which rule to apply.
One of the holes needs to correspond to a node of such a tree; the modifier |node| specifies which.
Other holes correspond to attributes.
Attributes however have a top-to-bottom inherited direction indicated by |inh|,
a bottom-to-top synthesized direction indicated by |syn|, or both, indicated by |thread|.

The judgement with scheme |tvarvFresh| is an example of a judgement which does not fit into a tree structure as required
by AG: it does not refer to a |node| hole.
For such schemes, called \IxAsDef{relation}s,
an explicit AG implementation must be provided; we omit further discussion of relations.

Finally, |Ruler| also provides support for incremental language specification,
which we discuss in \secRef{sec-ehcstruct-complexity-design}.

%}

%%]

%%[complexityImplementation

EHC is organised as a sequence of transformations between internal representations
of the program under compilation. 
In order to keep the compiler understandable, we keep the transformations simple,
and consequently, there are many transformations.
This approach is similar to the one taken in GHC 
\cite{peytonjones92ghc-overview,peytonjones94trafo-ghc,peytonjones96hs-transf}.
All our transformations are expressed as a full tree walk over the data structure, 
using a tool for easily defining tree walks (see \secRef{sec-ehcstruct-explainUUAGC}).
At each step where the nature of the representation changes drastically
we introduce a separate data structure (or language, if you will).
\figRef{fig-ehcstruct-dataflow} shows these languages,
and the transformations between them:

\begin{figure*}
\hspace*{-25mm}\raisebox{-60mm}[30mm][0mm]{\FigScaledPDF{0.60}{ehc-dataflow2}}
\caption{stages in the EHC pipeline}
\label{fig-ehcstruct-dataflow}
\end{figure*}

\begin{Itemize}
\item \textbf{HS} 
  (Haskell) is a representation of the concrete program text as parsed.
  It is used for
  desugaring, name and dependency analysis, and the regrouping of definitions accordingly.
\item \textbf{EH}
  (Essential Haskell) is a simplified and desugared representation.
  It is used for
  type analysis and code expansion of class system related constructs.
\item \textbf{Core}
  is a representation in untyped |lambda|-calculus.
\item \textbf{Grin}
  (Graph reduction intermediate notation)
  is a representation where local definitions have been made sequential
  and the need for evaluation has been made explicit \cite{boquist96grin-optim,boquist99phd-optim-lazy}.
\item \textbf{Silly}
  (Simple imperative little language)
  is a simple abstraction of an imperative language with an explicit stack and heap,
  and functions with local variables which can be called and tail-called.
\item \textbf{C}
  is used here as a universal back-end, hiding the details of the underlying machine.
  Primitive functions are implemented here.
\item \textbf{LLVM}
  (Low level virtual machine)
  is an imperative language which, other than C, is {\em intended} to be a universal
  back-end \cite{lattner07www-llvm}.
  We have it under consideration as an alternative route to executable code.
\end{Itemize}

As can be seen from the figure, the compilation pipeline branches after the Grin stage,
offering different modes of compilation:
\begin{Itemize}
\item Grin code can be interpreted directly by a simple (and thus slow) interpreter.
\item Grin code can be translated to C directly. 
      In this mode, the program is represented by a custom bytecodeformat, stored in arrays,
      and executed by a bytecode interpreter written in C.
\item Grin code can be translated to C via transformations which perform global program analysis,
      and generate optimized Silly code, which can be further processed by either the
      C or LLVM route.
\end{Itemize}

The transformations between the languages mentioned above bring the program stepwise
to a lower level of representation, until it can be executed directly. 
Much of the simplification work is done however by the transformations that are
indicated by a loop in \figRef{fig-ehcstruct-dataflow}, i.e.\ for which the
source and target language are the same.
We give a short description of the more important of these transformations.
Some of them are necessary simplifications, others are optimisations that can
optionally be left out.

Transformations on the Core language include:
\begin{Itemize}
\item Cleanup transformations:
      Eta-reduction, 
      Eliminating trivial applications, 
      Inline let alias, 
      Remove unnecessary letrec mutual recursion
\item Constant propagation
\item Rename identifiers to unique names
\item Lambda lifting, composed of smaller transformations:
      Full lazyness of subexpressions, 
      Lambda/CAF globals passed as argument, 
      Float lambda expressions to global level
\end{Itemize}

Transformations on the Grin language include:
\begin{Itemize}
\item Transformations on separate modules:
      Alias elimination, 
      Unused name elimination,
      Eval elimination,
      Unboxing,
      Local inlining
\item Transformations based on a global abstract interpretation
      that determines possible constructors of actual parameters:
      Inline eval operation,
      Remove dead case alternatives and unused functions,
      Global inlining
\item Transformations that remove higher-level constructs, 
      such as splitting complete nodes into their constituent fields.
\end{Itemize}

Transformations on the Silly language include:
\begin{Itemize}
\item Shortcut: avoid unnecessary copying of local variables
\item Embed: map local variables to stack positions
\end{Itemize}

%%]

%%[experiences
\Paragraph{Development and debugging}

The partitioning into variants is helpful for both development and debugging.
It is always clear to which variant code contributes,
and if a problem arises one can go a variant back in order to isolate the problem.
Experimentation also benefits because one can pick a suitable variant to build upon,
without being hindered by subsequent variants.

However, on the downside, there are builtin system wide assumptions, for example about how type checking is done.
We are currently investigating this issue in the context of |Ruler|.

\Paragraph{Improvements}

Although our approach to cope with complexity indeed leads to the advocated benefits,
there is room for improvement:

\begin{Itemize}
\item \textbf{Ruler and type rules.}
  With |Ruler| we generate both AG and \LaTeX.
  |Ruler| notation, AG, and \LaTeX\ have a similar structure.
  Consequently |Ruler| does not hide as much of
  the implementation as we would like.
  We are investigating a more declarative notation for |Ruler|.
\item \textbf{Loss of information while transforming.}
  With a transformational approach to different intermediate representations,
  the relation of later stages to earlier available information becomes unclear.
  For example, by desugaring to a simpler (Essential) Haskell representation,
  sourcecode gets reshuffled and their original source location has to be propagated correctly
  as part of the AST.
  Such information flow patterns are not yet automated.
\item \textbf{High level description and efficiency.}
  Using a high level description usually also provides opportunities to optimise at a low level.
  For attribute grammars a large body of optimisations are available,
  of which some are finding their way into our AG system.
\end{Itemize}

\Paragraph{Status and plans}

We are working towards a release as a Haskell compiler: the last variant of the whole sequence.
Compilation of a Prelude succeeds, and we run programs with a GRIN based bytecode interpreter.
We intend to work on AG optimisations, use LLVM \cite{lattner07www-llvm} as a backend,
and GRIN global transformations.

%%]

%%[oldLangSteps
\begin{figure*}[t]
\newcommand{\Nw}{}
\def\Line{\hline}
\def\Impl{Impl.}
\def\Sem{Sem.}
\def\Doc{Doc.}
\begin{tabular}{l||l@@{\Nw|->|\Nw}l@@{\Nw|->|\Nw}l@@{\Nw|->|\Nw}l}
%%@Poster.langSeries
\end{tabular}
\caption{Languages design steps}
\label{fig-ehcstruct-langs}
\end{figure*}

%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

%%[XX
%%]

