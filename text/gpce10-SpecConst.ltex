\documentclass[preprint,9pt]{sigplanconf}

\usepackage{graphicx} %% needs: fancyvrb



%include lhs2tex.fmt

\def\spacecorrection{\;}
\def\isspacecorrection{\spacecorrection}
\def\allowforspacecorrection#1{%
  \gdef\temp{#1}%
  \ifx\isspacecorrection\temp
    \let\next=\empty
  \else
    \let\next=\temp
  \fi
  \next}



\newcounter{enumctr}
\newenvironment{enumate}{%
\begin{list}{\arabic{enumctr}}{
\usecounter{enumctr}
\parsep  = 0pt
\parskip = 0pt
\topsep  = 0pt
\itemsep = 0pt
}}{\end{list}}
\newenvironment{itize}%
{\begin{list}%
  {$\bullet$%
  }%
  {\parsep  = 0pt%
   \parskip = 0pt%
   \topsep  = 0pt%
   \itemsep = 0pt%
  }%
}%
{\end{list}%
}


%format ^*      = "^{*}"
%format epsilon = "\epsilon"
%format bottom = "\bot"
%format ...    = "\mbox{\dots}"
%format not    = "\mathit{not}"


% Core keywords

%format let    = "\mathbf{let}"
%format in     = "\mathbf{in}"
%format letrec = "\mathbf{let}^{\mathbf{R}}"
%format letstrict = "\mathbf{let}^{\mathbf{S}}"

% Grin annotations

%format dictinst   = "\mathbf{dictinst}"
%format dictclass  = "\mathbf{dictclass}"
%format specialized  = "\mathbf{specialized}"

% Grin metanotions

%format BVar  = "\mathit{\underline{Var}}"


% Grin keywords

%format eval  = "\mathbf{eval}"
%format apply = "\mathbf{apply}"
%format store = "\mathbf{store}"
%format unit  = "\mathbf{unit}"
%format call  = "\mathbf{call}"
%format case  = "\mathbf{case}"
%format of    = "\mathbf{of}"

% Grin tags

%format /     = "\mbox{/}"
%format P2    = "\mathbf{P}_2"
%format F     = "\mathbf{F}"
%format A     = "\mathbf{A}"
%format C     = "\mathbf{C}"
%format Pm    = "\mathbf{P}_{m}"
%format Pnm   = "\mathbf{P}_{n-m}"
%format PN    = "\mathbf{P}_{N}"

%format a1    = "\mathit{a}_{1}"
%format an    = "\mathit{a}_{n}"
%format app9  = "\mathit{app}_{n}"
%format b0    = "\mathit{b}_{0}"
%format bk    = "\mathit{b}_{k}"


%format .    = "."
%format ^    = " "
%format ^^    = "\;"
%format ^@    = "@"

%format @ = "\spacecorrection @"
%format [          = "[\mskip1.5mu\allowforspacecorrection "
%format (          = "(\allowforspacecorrection "
%subst fromto b e t     = "\fromto{" b "}{" e "}{{}\allowforspacecorrection " t "{}}'n"





\usepackage{amsmath}

\usepackage{natbib}
\bibpunct();A{},
\let\cite=\citep
\bibliographystyle{plainnat}



\begin{document}

\conferenceinfo{GPCE '10}{October 11, Freiburg.} 
\copyrightyear{2010}
\copyrightdata{[to be supplied]} 

%\titlebanner{Working copy v.1}        % These are ignored unless
%\preprintfooter{Working copy v.1}   % 'preprint' option specified.

\setlength{\parindent}{0pt}
\setlength{\parskip}{3pt}


\title{Compiling by transformation:\\efficient implementation of overloading in Haskell}

 \authorinfo{Jeroen Fokker\and S.~Doaitse Swierstra}
            {Utrecht University}
            {\{jeroen,doaitse\}@@cs.uu.nl}

\maketitle

\begin{abstract}
The Utrecht Haskell Compiler (UHC) is designed as 
the composition of many small transformations.
We illustrate the transformational approach by showing 
how overloading is implemented and optimized in UHC.
Overloaded functions take additional `dictionary' arguments, 
which are automatically inserted during code generation,
based on inferred types.

For each instance declaration, a dictionary is generated 
containing the functions defined in the instance.
The dictionary also should contain the default definitions 
from the corresponding class declaration,
thus requiring a mechanism for combining them.

When compiling modules separately, the combination is done dynamically.
When doing whole program analysis, class and instance can be combined
statically by symbolic computation.
Further transformations, notably specialization of functions for constant arguments,
can completely eliminate run-time overhead for dictionary passing.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

%\terms
%term1, term2

%\keywords
%keyword1, keyword2

\section{Introduction}


\section{Overloading in Haskell}
In this section we briefly summarize how function overloading is modeled in Haskell.
Overloaded functions can be applied to values of various types.
For example, the addition function can be applied to |Int| values and also to |Float| values (but not to |Bool| values).
The equality testing function can be applied to values of many types, like |Int|, |Float|, |Char|, |Bool|.
Furthermore, it can be applied to values of list type |[a]|, provided that it is also applicable to the element type |a|.
But equality testing is not possible on values of function type.

A related concept is {\em polymorphism}.
Polymorphic functions can be applied to values of various types.
For example, concatenation can be applied to lists of type |[Int]| and to lists of type |[Char]|,
and in fact, to lists of type |[a]| for any type |a|.

The difference is that polymorphic functions have a uniform definition, regardless of the type of the arguments,
whereas an overloaded function can have a different definition for each type of argument to which it is applicable.
The type of arguments of polymorphic functions can be {\em any} instantiation of the type variables in its type,
whereas the type of arguments of overloaded functions can only be some particular instantiation of the type variables in its type.
Therefore, function overloading is sometimes refered to as {\em ad hoc polymorphism}.

\subsection{Class and instance declarations}

The group of types to which an overloaded function can be applied is known as a {\em class}.
A {\em class declaration} introduces a name for such a group of types, plus the signatures of functions
of which the variables in the type are constrained to be instantiated to types in that group.
For example, we can introduce the class |Eq| of types that support the equality and the non-equality functions
by writing the class declaration
\begin{code}
class Eq a where
  eq  ::  a -> a -> Bool
  ne  ::  a -> a -> Bool
\end{code}
The |Eq| class is actually part of the Haskell standard library. 
There, operators $==$ and $/=$ are defined rather than functions |eq| and |ne|,
but in this paper we avoid operators, as the notational conventions for writing them obscures the explanation.

Particular types can be defined to be an instance of the class by an {\em instance declaration}.
For example, |Int| can be made instance of |Eq| by giving definitions for the functions specified in the class declaration:
\begin{code}
instance Eq Int where
  eq x y  =  eqPrimInt x y
  ne x y  =  not (eqPrimInt x y)
\end{code}
The definitions rely on the existence of a function |eqPrimInt| that gives a primitive implementation for equality on integers.
The way |eqPrimInt| is defined is not relevant for the present discussion.

As another example, we give an instance declaration for |Eq Bool|.
It shows that we can define the function |eq| from scratch if we wish, without relying on other functions.
The function |ne| can be defined in a similar fashion (and that would probably be more efficient),
but just to show the possiblility to have |ne| rely on the existence of the other overloaded function |eq|.
\begin{code}
instance Eq Bool where
  eq  False  False  = True
  eq  True   True   = True
  eq  _      _      = False
  ne  x      y      = not (eq x y)
\end{code}
The terminology of the notions `class' and `instance' is borrowed from the object-oriented paradigm.
There are similarities, in that a class declaration specifies functions that are implemented in the instance declaration.
But note that there are differences as well: in the object-oriented paradigm, class member functions take an 
object of the class as implicit argument, 
whereas in the functional paradigm all arguments are declared explicitly, and thus we can have two arguments that are
constrained to the instance type.

Not all overloaded functions need to be defined in a class.
Every function that uses an overloaded function, will automatically be overloaded itself.
For example, the function |elem| that checks element membership of a list uses the overloaded equality function |eq| on the list elements:
\begin{code}
elem e []      = False
elem e (x:xs)  = eq e x || elem e xs
\end{code}
Therefore, although the |elem| function has a uniform definition and thus seems to be a polymorphic function,
it can actually only be used on lists of which the elements are in the |Eq| class.
This fact is expressed in the signature of the |elem| function by:
\begin{code}
elem :: Eq a => a -> [a] -> Bool
\end{code}
Note the double-shafted arrow which is to be read as `constrains', as opposed to the single-shafted arrow 
which is to be read as `function from/to'.


\subsection{Default definitions}
One could question the need for declaring |ne| to be part of the |Eq| class.
After all, non-equality is (in any sound implementation) the negation of equality.
So it would have been possible to only specify |eq| in the class,
and to define |ne| with a uniform definition, in a similar fashion to |elem|:
\begin{code}
class Eq a where
  eq  ::  a -> a -> Bool
ne  ::  Eq a => a -> a -> Bool
ne x y  =  not (eq x y)
\end{code}
That way, instance programmers are freed from the burden to give a definition of |ne|.
But the downside is that it is now impossible for instance programmers
to define their own version of |ne|. 
The programmer of |Eq Bool| might regret not being able to define
a more efficient version of |ne| that does pattern matching directly, 
instead of relying on the uniform definition that calls |eq| and |not|.

To overcome the dilemma, Haskell allows a class definition to be augmented with {\em default definitions}
for some or all of the member functions.
Now, instance programmers have the choice to either rely on a default definition of |ne|
as it appears in the class, or give a more efficient version for a particular type.

A default definition by nature must be a uniform definition, as it can not assume
the type variables in the signature to be instantiated as a particular type.
But the definition can rely on other functions from the class.
In fact, the Haskell standard library implementation of |Eq| has a default definition 
for both |ne| and |eq|, defined in terms of each other:
\begin{code}
class Eq a where
  eq  ::  a -> a -> Bool
  ne  ::  a -> a -> Bool
  ne x y   =  not (eq x y)
  eq x y   =  not (ne x y)
\end{code}
An instance programmer now has the choice to define either |eq| or |ne| (and rely on the default definition for the other),
or define both (thus ignoring/erasing the default definitions).
Defining neither of the two is allowed as well, but would result in inheriting the circular definitions without
breaking the circle by redefining at least one of the two.


\subsection{Superclasses}

Stretching the analogy to object-oriented classes further, Haskell has a notion of `superclasses' as well.
It is exemplified by the class |Ord| of types that have an ordering, by providing comparison operators like $<$ and $\geq$.
Default definitions specify them in terms of each other.
Instance programmers can choose to implement only one of the four (and rely on the default definitions for the others),
or more if they wish.
But the default definitions not only call each other, but also the |eq| function from class |Eq|.
This is possible because |Ord| is specified to be a subclass of |Eq|, 
that is, a type is only allowed to be an instance of |Ord| if it is an instance of |Eq| as well.

A possible class definition of |Ord| is:
\begin{code}
class (Eq a) => Ord a where
  lt  :: a -> a -> Bool
  le  :: a -> a -> Bool
  gt  :: a -> a -> Bool
  ge  :: a -> a -> Bool
  lt   x y  =  not  (ge x y)
  gt   x y  =  not  (le x y)
  le   x y  =  lt  x y || eq x y
  ge   x y  =  gt  x y || eq x y
\end{code}
The superclass is mentioned in the class header, featuring another meaning of the double-shafted arrow, 
in parentheses as there may be more than one superclass.
The real definition in the standard library also specifies functions |compare|, |min|, and |max|, 
and an even more intricate web of mutually recursive default definitions.

\subsection{Context for instances}

Finally, Haskell provides a mechanism to declare an instance in the context of the presence another instance.
An example is the definition of |Eq| for lists, where lists are defined to be equal if their elements are.
This only makes sense in a context where the element type is assumed to be instance of |Eq| as well.
\begin{code}
instance  Eq a => Eq [a] where
  eq []      []      =  True
  eq []      (y:ys)  =  False
  eq (x:xs)  (y:ys)  =  eq x y && eq xs ys
\end{code}
The last line shows two calls to |eq|.
The second is a recursive call on the tails of the list.
The first however is not a recursive call, but a call to |eq| for a different instance type:
the list element type, guaranteed to be an instance of |Eq| by the context mentioned in the header.
Type analysis by the compiler makes sure that the right version of |eq| is called.



\section{UHC compiler structure}


\subsection{Transformational programming}

The main structure of the compiler is shown in Figure~\ref{fig-uhcarch-pipeline}.
Haskell source text is translated to an executable program by stepwise transformation.
Some transformations translate the program to a lower level language,
many others are transformations within one language,
establishing an invariant or performing an optimization.

All transformations, both within a language and between languages, are expressed
as an algebra giving a semantics to the language.
The algebras are described with the aid of an attribute grammar,
which makes it possible to write multi-pass tree-traversals
without even knowing the exact number of passes.
Although the compiler driver is set up to pass data structures between transformations,
for all intermediate languages we have a concrete syntax with a parser
and a pretty printer. This facilitates debugging the compiler, by inspecting
code between transformations.

Here is a short characterization of the intermediate languages.
In section~\ref{sec-uhcarch-lang} we give a more detailed description.
\begin{itemize}
\item Haskell (HS): a general-purpose, higher-order, polymorphically typed, lazy functional language.
\item Essential Haskell (EH): a higher-order, polymorphically typed, lazy functional language close to lambda-calculus, without syntactic sugar.
\item Core: an untyped, lazy functional language close to lambda-calculus
            (at the time of this writing we are working on moving to a typed intermediate language,
             a combination of Henk %\cite{peytonjones97henk-ty-interm},
             GHC core,
             and recent work on calling conventions %\cite{bolingbroke09type-call-conv}).
\item Grin: `Graph reduction intermediate notation', 
            the instruction set of a virtual machine of a small functional language with strict semantics, 
            with features that enable implementation of laziness %\cite{boquist99phd-optim-lazy}.
\item Silly: `Simple imperative little language', an abstraction of features found in every imperative language
            (if-statements, assignments, explicit memory allocation) augmented with primitives for manipulating a stack,
            easily translatable to e.g.\ C (not all features of C are provided, only those that are needed for our purpose).
\item BC: A bytecode language for a low-level machine intended to interpret Grin which is not whole-program analyzed nor transformed.
          We do not discuss this language in this paper.
\end{itemize}
The compiler targets different backends, based on a choice of the user.
In all cases, the compiler starts compiling on a per module basis,
desugaring the Haskell source text to Essential Haskell, type checking it and translating it to Core.
Then there is a choice from three modes of operation:
\begin{itemize}
\item In {\em whole-program analysis mode},
      the Core modules of the program and required libraries are assembled together
      and processed further as a whole.
      At the Grin level, elaborate inter-module optimization takes place.
      Ultimately, all functions are translated to low level C,
      which can be compiled by a standard compiler.
      As alternative backends, we are experimenting with other target languages,
      among which are the Common Intermediate Language (CIL) from the Common language infrastructure used by .NET %\cite{iso-cil},
      and the Low-Level Virtual Machine (LLVM) compiler infrastructure %\cite{llvm-cgo04}.
\item In {\em bytecode interpreter mode},
      the Core modules are translated to Grin separately.
      Each Grin module is translated into instructions for a custom bytecode machine.
      The bytecode is emitted in the form of C arrays,
      which are interpreted by a handwritten bytecode interpreter in C.
\item In {\em Java mode},
      the Core modules are translated to bytecode for the Java virtual machine (JVM).
      Each function is translated to a separate class with an |eval| function, and
      each closure is represented by an object combining a function with its parameters.
      Together with a driver function in Java which steers the interpretation,
      these can be stored in a Java archive (jar) and be interpreted by a standard Java interpreter.
\end{itemize}
The bytecode interpreter mode is intended for use during program development:
it compiles fast, but because of the interpretation overhead the generated code is not very fast.
The whole-program analysis mode is intended to use for the final program:
it takes more time to compile, but generates code that is more efficient.

In Section~2 we describe the tools that play an important role in UHC:
the Attribute Grammar preprocessor, a language for expressing type rules, and the variant and aspect manager.
In Section~3 we describe the intermediate languages in the UHC pipeline in more detail, illustrated with a running example.
In Section~4 the transformations are characterized in more detail.
Finally, in Section~5 we draw conclusions about the methodology used, and mention related and future work.


\begin{figure}[tbfh]
\includegraphics[scale=0.43]{figs/uhc-pipeline.pdf}
\caption{Intermediate languages and transformations in the UHC pipeline, in each of the three operation modes:
whole-program analysis (left), bytecode interpreter (middle), and Java (right).}
\label{fig-uhcarch-pipeline}
\end{figure}





\subsection{The Grin intermediate language}



Refer to figure~\ref{fig-grin-syntax} for the syntax of the Grin language.

A program consists of bindings, 
each of which is either a function definition or a global variable.
Local functions are not possible in Grin, so all functions are defined at top level.

The body of a function is an |Expr|.
Although this sounds functional,
the body is fact is a sequence of statements with side effects on the heap,
and assigning local variables on its way.



When a function is called, the body of the function is executed,
which has side effects on the heap

\begin{figure}[tbfh]

\begin{center}
\fbox{
\parbox{8cm}{
\begin{code}
Program  ::=   Bind^*
Bind     ::=   Name BVar^* = Annot { Expr }
         |     BVar <- Node
Expr     ::=   unit Var
         |     unit Node
         |     Expr ; \ BVar -> Expr
         |     case Var of Alt^*
         |     store Node
         |     eval  Var
         |     call  Name Var^*
         |     apply Var Var^*
Alt      ::=   Pat -> Expr
Pat      ::=   ( Tag BVar^* )
Node     ::=   ( Tag Var^* )
         |     ( Tag Lit )
Tag      ::=   C/Constr
         |     PN/Name
         |     F/Name
         |     A
N        ::=   1 | 2 | ...
Annot    ::=   epsilon
         |     dictclass Name^*
         |     dictinst  (Constr, Name, Name^*)
         |     specialized (Name, Var^*)
\end{code}
}
}
\end{center}

\caption{Syntax of the Grin language. An |^*| denotes 0 or more occurences, underline denotes a defining position.
|Var|, |Name|, and |Constr| are identifiers refering to values, functions, or constructors.
|Lit| is an integer or character literal.
}
\label{fig-grin-syntax}
\end{figure}








\section{Dynamic handling of dictionaries}
\subsection{Dictionary arguments for overloaded functions}
\subsection{Dictionary generation}

Selector functions
\begin{code}
eq d =
{  eval d ; \t ->
   case t of
    (C/Eq f _) -> {eval f}
}
\end{code}
Default definitions for member functions take the form of overloaded functions
\begin{code}
neDef d x y =
{  store (F/eq d)   ; \f ->
   store (A f x y)  ; \r ->
   call not r
}
\end{code}


Ideally, we would simply have
\begin{code}
dEqInt := (C/Eq neInt eqInt)
\end{code}
But when the dictionary relies on the default definition for |ne|, we cannot simply have
\begin{code}
dEqInt := (C/Eq neDef eqInt)
\end{code}
since |neDef| is overloaded.





\subsection{Dynamically merging default definitions}


Generator for the dictionary containing default functions, parameterized by the final dictionary
\begin{code}
genEq d =
{  store (P2/neDef d)     ; \f ->
   store (C/Eq bottom f)  ; \t ->
   eval t
}
\end{code}


Core code for dictionary instance
\begin{code}
dEqInt =
  letrec  {  fixEqInt =
               let  gen =
                      letstrict  d = genEq fixEqInt
                      in         case d of
                                   (Eq f g) -> (Eq f eqInt)
               in   gen
          ;  eqInt =
               eqPrimInt
          }
  in fixEqInt
\end{code}




Asking for the dictionary triggers evaluation of the fixpoint construction
\begin{code}
dEqInt' =
{  eval fixEqInt
}
\end{code}
The fixpoint cntruction calls the generator with the fixpoint as argument
\begin{code}
fixEqInt' =
{  call genEqInt fixEqInt
}
\end{code}
The generator calls the generator of the default functions,
and subsequentially replaces the fields that are defined in the instance declaration
\begin{code}
genEqInt d =
{  call genEq d   ; \t ->
   case t of
     (C/Eq _ f) ->
     {  store (P2/eqPrimInt)  ; \g ->
        store (C/Eq g f)      ; \r ->
        eval r
     }
}
\end{code}
Because of the lazy CAF mechanism we also get two global variables
corresponding to the 0-ary functions |dEqInt| and |fixEqInt|
\begin{code}
dEqInt    :=  (F/dEqInt')
fixEqInt  :=  (F/fixEqInt')
\end{code}

\begin{code}
\end{code}






\section{Static handling of dictionaries}


When it is possible to inspect and transform the program as a whole,
we can fully eliminate the run-time overhead of manipulating dictionaries.
The steps to achieve this can be described as separate program transformation steps,
thus adhering to our philosophy of having rather a large number of easy transformations,
than a small number of complicated ones.

The most important transformations are:
\begin{itize}
\item |mergeInstance|: 
      making available a statically known dictionary for every instance declaration,
      by merging the definitions from the instance declaration and the default definitions in the class definition
\item |selectMember|:
      statically rather than dynamically select members from a dictionary
\item |specConst|:
      specialize functions that are called with a constant argument.
      This is a general technique that can also transform an expression like |plus x 1| to |succ x|,
      where |succ| is a specialized version of |plus| with constant argument |1|.
      Here we use it to specialize overloaded functions that are called with a constant dictionary.
\end{itize}
The opportunities for applying these transformations are prepared by some more transformations:
\begin{itize}
\item |evalKnown|: simplify uses of |eval x| in a situation where the value of |x| happens to be statically known
\item |applyKnown|: simplify uses of |apply f x| in a situation where the value of |f| happens to be statically known
\item |dropUnused|: remove bindings to (local and global) variables that are never used
\item |dropUnreachable|: remove bindings to global variables that are not reachable from |main|
\end{itize}





\subsection{The |mergeInstance| transformation}

Goal is to replace
\begin{code}
dEqInt    :=  (F/dEqInt')
\end{code}
by
\begin{code}
dEqInt    :=  (C/Eq eqEqInt neEqInt)
\end{code}
which explicitly stores the dictionary without having to do the fixed point construction at run time.
Inside the dictionary, we reference two global variables which are initialized for the purpose:"
\begin{code}
eqEqInt :=  (P2/eqPrimInt)
neEqInt :=  (P2/eqDef dictEqInt)
\end{code}
This is hard for two reasons:
it is hard to see that |dEqInt| is indeed a dictionary:
it would involve inspecting the function |dEqInt'| mentioned in the thunk,
statically doing the evaluation of |fixEqInt| which leads us to |fixEqInt'|,
of which the definition has the form that we can recognize as the generated code for instance declarations.
Once we found that, we should carefully deconstruct |genEqInt| for finding the information needed in the dictionary.

Although this approach is possible in principle, it feels like reversely engineering
the outcome of all the tranformations that were responsible of generating this Grin definitions.
Apart from being tricky, the procedure is bound to break whenever we would make changes in the Core to Grin transformation pipeline.

Instead, we take a different approach, by making manifest the intention of some of the generated function definitions,
by means of annotations.
The price is that we need to extend both the Core and the Grin language to facilitate such annotations.
\begin{itize}
\item the definition of |dEqInt'| is annotated with a marker |dictinst|: `this is a dictionary corresponding to a instance declaration';
\item the definition of |genEq| is annotated with a marker |dictclass|: `this is a dictionary generator corresponding to a the default definitions in a class declaration'.
\end{itize}
Apart from the marker we embed in the annotation all information relevant for the dictionaries:
\begin{itize}
\item for |dictinst|,  we need the name of the tag of the dictionary, the name of the dictionary constructor for the default definitions, and all the names of the members defined.
\item for |dictclass|, we need the names of all default definitions.
\end{itize}
In our example, we get:
\begin{code}
dEqInt' =
dictinst(Eq, genEq, [eqPrimInt, _])
{  eval fixEqInt
}
genEq d =
dictclass [_,neDef]
{  store (P2/neDef d)     ; \f ->
   store (C/Eq bottom f)  ; \t ->
   eval t
}
\end{code}
There are empty positions in the name lists when functions are not defined in the instance or class declarations.

These annotations can be easily inserted when generating Core, because all this information is needed anyway
for generating the Core definitions.
The annotations are propagated unchanged through all Core and Grin transformations, so that we have them available
when we need them: in the |mergeInstance| transformation.



\subsection{The |selectMember| transformation}\label{subsec.selectMember}

Dictionaries are passed as additional arguments to overloaded functions.
In their body, they can be passed to other overloaded functions, 
but in the end dictionaries are only used for one purpose:
selecting a member function from them.

An example is a call to the overloaded function |eq| with arguments of known type,
as in the Haskell expression |eq 3 4|.
The Grin function that is generated for this expression is
\begin{code}
test1' =
{  store (C/Int 3)  ; \x ->
   store (C/Int 4)  ; \y ->
   call eq dEqInt   ; \f ->
   apply f x y
\end{code}
The third line selects a field from the dictinary by calling the |eq| selector;
the resulting function is subsequently applied to its arguments.

Now that the previous transformation has made all dictionaries statically available,
we can now proceed by selecting the member fields statically.
This is what the |selectMember| transformation does:
it scans the Grin program for expressions of the form |call s d|
where |s| is a selector function and |d| is a dictionary.

In the example, the selector is |eq|, which is a Grin function defined by
\begin{code}
eq d =
{  eval d ; \t ->
   case t of
    (C/Eq f _) -> {eval f}
}
\end{code}
It is recognized as a selector, because its definition is a two-line function where the second line evaluates one of the members of a dictionary.
We actually hunt the program for selectors, that is functions that have this very structure.
(Another approach would be to explcitly annotate selector functions as such at the moment they are generated,
in a similar fashion as we use |dictinst| and |dictclass| annotations to avoid hunting for complex patterns).

In the example, the dictionary is |dictEqInt|, which is a global variable that at this thime
(after the |mergeInstance| transformation) is defined as
\begin{code}
dEqInt  :=  (C/Eq eqEqInt neEqInt)
\end{code}

Now that the selector and the dictionary are identified, the call can be performed statically:
the expression |call eq dEqInt| is replaced by |eval eqEqInt|.

Our example ends up as transformed to:
\begin{code}
test1' =
{  store (C/Int 3)  ; \x ->
   store (C/Int 4)  ; \y ->
   eval eqEqInt     ; \f ->
   apply f x y
\end{code}


\subsection{The |evalKnown| transformation}

At this point in the pipeline of transformations it is useful to perform 
two transformations that simplify the Grin program based on variables of which the value may be known in a particular context.
There are two such transformations: |evalKnown| and |applyKnown|.

An |eval| expression occurs in Grin code when it is needed to force a variable to head normal form
and fetch its value from the heap.
A typical occurence is in the body of a function, where the unknown value of the argument needs to be forced and fetched
in order to be scrutinized:
\begin{code}
f x =
{  eval x   ; \v ->
   case v of ...
\end{code}
Sometimes, a Grin program evaluates a variable of which the value {\em is} known.
We can encounter expressions like:
\begin{code}
   store (C/Int 5)  ; \n ->
   eval n
\end{code}
This is equivalent to a shorter expression:
\begin{code}
   unit (C/Int 5)
\end{code}
which is more efficient since it avoids storing a node on the heap, and doesn't need to execute the |eval| operation.

So is the Grin code generator to blame for emitting inefficient code like the example above?
Not really, because the value of the variable could have become known only later, as a result of transformation of Grin code.
For example, if the |inline| transformation decides to inline the call to |f| in
\begin{code}
   store (C/Int 5)  ; \n ->
   call f n
\end{code}
we end up with the inefficient |store|-|eval| combination.

To compensate for this, we have a transformation |evalKnown| that hunts for |eval x|
where |x| has a known value, either because it is a global variable or it is the target of an earlier |store|.
This transformation catches situations like the example above.
Since this transformation is carried out anyway, the Grin code generator
can afford itself to generate |store|-|eval| pairs on occasion.
This makes the code generator simpler: it can be defined compositionally, without having to 
bother to avoid |store|-|eval| pairs.

The |evalKnown| transformation symbolically collects all (local and global)
|store|s, and for each |eval x| checks whether the variable |x| has a known value.
There are three cases:
\begin{itize}
\item |x| is a global or local variable used to store a value |v| in head normal form, that is a node with a |C| or |P| tag.
      Then |eval x| can be replaced by |unit v|.
\item |x| is a local variable used to store a thunk with an |F| tag, that is a node |(F/f a1...an)|.
      When |x| is used only once,
      then |eval x| can be replaced by |call f a1...an|
\item |x| is a local variable used to store a thunk with an |A| tag, that is a node |(A f a1...an)|.
      When |x| is used only once,
      then |eval x| can be replaced by |call app9 f a1...an|,
      where |app9| is a compiler-generated function
\begin{code}
app9 f a1...an =
{  eval f  ; \p ->
   apply p a1...an
}
\end{code}
\end{itize}

The reason that we bring up this whole story, is that the previous |selectMember| transformation
generates opportunities for |evalKnown|. 
Remember that it has replaced |call eq dEqInt| by |eval eqEqInt|.
This is an opportunity for the |evalKnown| transformation, since |eqEqInt| is a global variable bound to |(P2/eqPrimInt)|.
Thus |eval eqEqInt| is transformed to |unit (P2/eqPrimInt)|.

Our example thus now is transformed to:
\begin{code}
test1' =
{  store (C/Int 3)      ; \x ->
   store (C/Int 4)      ; \y ->
   unit (P2/eqPrimInt)  ; \v ->
   apply v x y
}
\end{code}


\subsection{The |applyKnown| transformation}

The |apply| operation expects a value that represents a partially applied function, and applies it to further arguments.
Normally this operation is generated by the code generator for values that are not statically known,
for example when emitting code for polymorphic functions such as |map|.

But similar to the previous subsection, where |eval| is occasionally used on variables with a statically known value,
situations can occur where |apply| is used on values that are statically known.
In fact, this happens in the example outcome of the previous transformation:
we have an |apply| operating on a value that obviously is |(P2/eqPrimInt)|,
a partial application of |eqPrimInt| lacking 2 parameters.

Since in this example the lacking 2 parameters are provided as part of the |apply| operation,
the call is thereby saturated and equivalent to |call eqPrimInt x y|.

This is exactly what the |evalKnown| transformation performs: it symbolically collects all
|unit|s, and for each |apply v a1...an| checks whether the value |v| holds a known node |(Pm/f b0...bk)|.
There are three cases:
\begin{itize}
\item |n<m| (undersaturated call): replace the |apply| operation by |unit (Pnm/f b0...bk a1...an)|
\item |n=m| (saturated call): replace the |apply| operation by\\ |call f b0...bk a1...an|
\item |n>m| (oversaturated call): do nothing. (This situation does not occur in the context discussed in this paper. If it does occur in other situations, doing nothing is always safe.)
\end{itize}

Our example ends up as:
\begin{code}
test1' =
{  store (C/Int 3)      ; \x ->
   store (C/Int 4)      ; \y ->
   unit (P2/eqPrimInt)  ; \v ->
   call eqPrimInt x y
}
\end{code}
Note that the binding of the |P2|-node to |v| is now obsolete. 
However, it is not removed by the |applyKnown| transformation, 
as it will be caught anyway by a |dropUnused| transformation performed further downstream.
This way, we keep the individual transformations straigtforward, 
while they together still do all that is needed.



\subsection{The |specConst| transformation}

The combined effort of all transformations so far has succeeded to annihilate 
all dictionary overhead involved in the Haskell expression |eq 3 4|.
But now let's see what happens for the Haskell expression |ne 5 6|.

Because the instance declaration |Eq Int| relies on the default definition for |ne|,
the resulting Grin code is different now. 
The field selection and subsequent |apply| is shortcut succesfully,
but the default definition |neDef| that is now called is overloaded,
and thus needs an additional dictionary parameter itself.
\begin{code}
test2' =
{  store (C/Int 5)      ; \x ->
   store (C/Int 6)      ; \y ->
   call neDef dEqInt x y
}
\end{code}
So we still have overhead for run-time dictionary passing here.

To overcome this, we rely on a technique that is actually more general:
make specialized `clones' of a function that is called with a constant argument.
Function |neDef| has three parameters, but it is called here with constant arguments:
a dictionary, that since the |selectMember| transformation is defined as a global constant:
\begin{code}
dEqInt  :=  (C/Eq eqEqInt neEqInt)
\end{code}
In this particular example the other two arguments are constants as well,
so the function will end up to be specialized for all three arguments,
but in a typical situation only the dictionary is constant, and we get a specialized copy still expecting two parameters.

For the purpose of explaining, we assume here that the function is only specialized for its dictionairy argument
(imagine an option to be active that forbids specializing for integer arguments).
The definition of |neDef| is taken:
\begin{code}
neDef d x y =
{  store (F/eq d)   ; \f ->
   store (A f x y)  ; \r ->
   call not r
}
\end{code}
and a clone of it is generated, which is specialized for the first argument:
\begin{code}
neDef~1 x y =
specialized (neDef, [dEqInt,_,_])
{  store (F/eq dEqInt)  ; \f ->
   store (A f x y)      ; \r ->
   call not r
}
\end{code}
Note that the clone is annotated in such a way that it is manifest
what was the original function, and for which arguments it was specialized.
This way, when the transformation is run again later, we can avoid making another clone for the same argument.

The call is adapted accordingly:
\begin{code}
test2' =
{  store (C/Int 5)      ; \x ->
   store (C/Int 6)      ; \y ->
   call neDef~1 x y
}
\end{code}


\subsection{\dots and repeat}


After the specialization, new opportunities are exposed for transformations that we discussed earlier.
Firstly, the operation |store (F/eq dEqInt)| is an opportunity for the |selectMember| transformation.
In subsection~\ref{subsec.selectMember} we described that 
for all selectorfunctions |s| that select field |i| from a dictionary,
and all constant dictionaries |d| having |f| as it's |i|th field,
it replaces |call s d| by |eval f|.

We now widen the task of |selectMember| to also handle selections from constant dictionaries 
that are disguised as thunk. That is: 
replace |store (F/s d)| by |unit f|.

This way, we loose the lazy behaviour of the field selection.
But there is no need for lazyness in this situation:
field selection cannot fail, and it is performed fast -- in fact, 
now that we perform it statically, it takes no time at all.
Nobody would oppose not postponing a call that takes zero time and cannot fail.

Another d\'eja vu: the |unit eqEqInt| that emerges from the previous transformation
can be combined with the thunkified |apply| in |store (A f x y)|.
This is done by the |applyKnown|, whose task is also widened to handle situations
where an |apply| is disguised as a thunk with |A| tag.

Next, we may have new opportunities for another |specConst| transformation, etcetera.
All in all, the following sequence of transformations should be performed repeatedly:
|selectMember|, |evalKnown|, |applyKnown|, and |specConst|.

New opportunities will appear as often as overloaded functions keep calling each other,
requiring as many iterations as the static nesting depth of overloaded functions.
If we want to be sure that all dictionary-passing is removed, we should
iterate the four transformations until none of the four does replacements.
In practice, a fixed number of iterations could satisfy as well.

In real-life example programs involving the complicated classes from
the numeric, IO, and read/show libraries, we observed the transformation of a program to converge
to a fixed point after about 5 iterations.




\section{Superclasses and contexts}



\section{Implementation}

With AG's

\section{Conclusion}





\begin{thebibliography}{}

\bibitem[ToDo]{ToDo} 
{\em nog veel meer, o.a. Fax\'en, maar in ieder geval ook deze:}

\bibitem[Boquist and Johnsson 1996]{boquist1996}
Urban Boquist and Thomas Johnsson.
The GRIN project: A highly optimising back end for lazy functional languages.
In {\em Workshop on Implementation of Functional Languages} IFL 1996.
Springer LNCS 1268. 

\bibitem[Boquist 1999]{boquist1999}
Urban Boquist.
{\em Code optimisation techniques for lazy functional languages}.
PhD Thesis Chalmers University, G\"oteborg March 1999.

\bibitem[Dijkstra 2005]{ehc}
Atze Dijkstra.
{\em Stepping through Haskell}.
PhD Thesis Utrecht University, November 2005.

\bibitem[Jones 1995]{cata}
Mark. P.\ Jones.
Functional programming with overloading and higher-order polymorphism.
In: {\em Advanced Functional Programming} AFP'95, pp.~97--136.
Springer LNCS 925.

\end{thebibliography}

\end{document}
